[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python for Data Analysis in Finance",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "chapters/01_jumpstart/intro.html",
    "href": "chapters/01_jumpstart/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "chapters/02_dataframe_basics/summary.html",
    "href": "chapters/02_dataframe_basics/summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#further-reading",
    "href": "chapters/01_jumpstart/jumpstart.html#further-reading",
    "title": "1  Python Jumpstart",
    "section": "1.19 Further Reading",
    "text": "1.19 Further Reading\nPython Data Science Handbook - Jake VanderPlas\nPython for Finance 2e - Yves Hilpisch\nPython for Data Analysis 3e - Wes McKinney"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#what-is-a-notebook",
    "href": "chapters/01_jumpstart/jumpstart.html#what-is-a-notebook",
    "title": "1  Python Jumpstart",
    "section": "1.1 What is a Notebook?",
    "text": "1.1 What is a Notebook?\nThis file - the one you are currently interacting with - is a Jupyter notebook.\nThe notebook format conveniently allows you to combine sentences, code, code outputs (including plots), and mathematical notation. Notebooks have proven to be a convenient and productive programming environment for data analysis.\nBehind the scenes of a Jupyter notebook is a kernel that is responsible for executing computations. The kernel can live locally on your machine or on a remote server."
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#code-cells",
    "href": "chapters/01_jumpstart/jumpstart.html#code-cells",
    "title": "1  Python Jumpstart",
    "section": "1.3 Code Cells",
    "text": "1.3 Code Cells\nA notebook is structured as a sequence of cells. There are three kinds of cells: 1) code cells that contain code; 2) markdown cells that contain markdown or latex; and 3) raw cells that contain raw text. We will work mainly with code cells and markdown cells.\nThe cell below is a code cell - try typing the code and then press shift + enter.\n\nfrom IPython.display import Image\nImage(\"not_ethical.png\")"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#edit-mode-vs-command-mode",
    "href": "chapters/01_jumpstart/jumpstart.html#edit-mode-vs-command-mode",
    "title": "1  Python Jumpstart",
    "section": "1.4 Edit Mode vs Command Mode",
    "text": "1.4 Edit Mode vs Command Mode\nThere are two modes in a notebook: 1) edit mode; 2) command mode.\nIn edit mode you are inside a cell and you can edit the contents of the cell.\nIn command mode, you are outside the cells and you can navigate between them."
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#keyboard-shortcuts",
    "href": "chapters/01_jumpstart/jumpstart.html#keyboard-shortcuts",
    "title": "1  Python Jumpstart",
    "section": "1.5 Keyboard Shortcuts",
    "text": "1.5 Keyboard Shortcuts\nHere are some of my favorite JupyterLab keyboard shortcuts:\nedit mode: enter\ncommand mode: esc\nnavigate up: k\nnavigate down: j\ninsert cell above: a\ninsert cell below: b\ndelete cell: d, d (press d twice)\nswitch to code cell: y\nswitch to markup cell: m\nexecute and stay on current cell: ctrl + enter\nexecute and move down a cell: shift + enter"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#drop-down-menus",
    "href": "chapters/01_jumpstart/jumpstart.html#drop-down-menus",
    "title": "1  Python Jumpstart",
    "section": "1.6 Drop Down Menus",
    "text": "1.6 Drop Down Menus\nHere are a few of the drop down menu functions in JupyterLab that I use frequently:\nKernel &gt; Restart Kernel and Clear All Outputs\nKernel &gt; Restart Kearnel and Run All Cells\nRun &gt; Run All Above Selected Cell"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#importing-packages",
    "href": "chapters/01_jumpstart/jumpstart.html#importing-packages",
    "title": "1  Python Jumpstart",
    "section": "1.7 Importing Packages",
    "text": "1.7 Importing Packages\nThe power and convenience of Python as a data analysis language comes from the ecosystem of freely available third party packages.\nHere are the packages that we will be using in this tutorial:\nnumpy - efficient vector and matrix computations\npandas - working with DataFrames\nyfinance - reading in data from Yahoo finance\npandas_datareader - also for reading data from Yahoo Finance\nThe following code imports these packages and assigns them each an alias.\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#reading-in-stock-data-into-a-dataframe",
    "href": "chapters/01_jumpstart/jumpstart.html#reading-in-stock-data-into-a-dataframe",
    "title": "1  Python Jumpstart",
    "section": "1.8 Reading-In Stock Data into a DataFrame",
    "text": "1.8 Reading-In Stock Data into a DataFrame\nLet’s begin by reading in 5 years of SPY price data from Yahoo Finance.\nSPY is an ETF that tracks the performace of the SP500 stock index.\n\ndf_spy = pdr.get_data_yahoo('SPY', start='2014-01-01', end='2019-01-01')\ndf_spy = df_spy.round(2)\ndf_spy.head()\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2014-01-02\n183.98\n184.07\n182.48\n182.92\n153.83\n119636900\n\n\n2014-01-03\n183.23\n183.60\n182.63\n182.89\n153.80\n81390600\n\n\n2014-01-06\n183.49\n183.56\n182.08\n182.36\n153.36\n108028200\n\n\n2014-01-07\n183.09\n183.79\n182.95\n183.48\n154.30\n86144200\n\n\n2014-01-08\n183.45\n183.83\n182.89\n183.52\n154.33\n96582300\n\n\n\n\n\n\n\nOur stock data now lives in the variable called df_spy, which is a pandas data structure known as a DataFrame. We can see this by using the following code:\n\ntype(df_spy)\n\npandas.core.frame.DataFrame"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#dataframe-index",
    "href": "chapters/01_jumpstart/jumpstart.html#dataframe-index",
    "title": "1  Python Jumpstart",
    "section": "1.9 DataFrame Index",
    "text": "1.9 DataFrame Index\nIn pandas, a DataFrame always has an index. For df_spy the Dates form the index.\n\ndf_spy.index\n\nDatetimeIndex(['2014-01-02', '2014-01-03', '2014-01-06', '2014-01-07',\n               '2014-01-08', '2014-01-09', '2014-01-10', '2014-01-13',\n               '2014-01-14', '2014-01-15',\n               ...\n               '2018-12-17', '2018-12-18', '2018-12-19', '2018-12-20',\n               '2018-12-21', '2018-12-24', '2018-12-26', '2018-12-27',\n               '2018-12-28', '2018-12-31'],\n              dtype='datetime64[ns]', name='Date', length=1258, freq=None)\n\n\nI don’t use indices very much, so let’s make the Date index just a regular column. Notice that we can modify DataFrames inplace.\n\ndf_spy.reset_index(inplace=True)\ndf_spy\n\n\n\n\n\n\n\n\nDate\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\n\n\n\n\n0\n2014-01-02\n183.98\n184.07\n182.48\n182.92\n153.83\n119636900\n\n\n1\n2014-01-03\n183.23\n183.60\n182.63\n182.89\n153.80\n81390600\n\n\n2\n2014-01-06\n183.49\n183.56\n182.08\n182.36\n153.36\n108028200\n\n\n3\n2014-01-07\n183.09\n183.79\n182.95\n183.48\n154.30\n86144200\n\n\n4\n2014-01-08\n183.45\n183.83\n182.89\n183.52\n154.33\n96582300\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1253\n2018-12-24\n239.04\n240.84\n234.27\n234.34\n217.60\n147311600\n\n\n1254\n2018-12-26\n235.97\n246.18\n233.76\n246.18\n228.59\n218485400\n\n\n1255\n2018-12-27\n242.57\n248.29\n238.96\n248.07\n230.35\n186267300\n\n\n1256\n2018-12-28\n249.58\n251.40\n246.45\n247.75\n230.05\n153100200\n\n\n1257\n2018-12-31\n249.56\n250.19\n247.47\n249.92\n232.07\n144299400\n\n\n\n\n1258 rows × 7 columns\n\n\n\nNotice that even though we ran the .reset_index() method of df_spy it still has an index; now its index is just a sequence of integers.\n\ndf_spy.index\n\nRangeIndex(start=0, stop=1258, step=1)"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#a-bit-of-cleaning",
    "href": "chapters/01_jumpstart/jumpstart.html#a-bit-of-cleaning",
    "title": "1  Python Jumpstart",
    "section": "1.10 A Bit of Cleaning",
    "text": "1.10 A Bit of Cleaning\nAs a matter of preference, I like my column names to be in snake case.\n\ndf_spy.columns = df_spy.columns.str.lower().str.replace(' ','_')\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n0\n2014-01-02\n183.98\n184.07\n182.48\n182.92\n153.83\n119636900\n\n\n1\n2014-01-03\n183.23\n183.60\n182.63\n182.89\n153.80\n81390600\n\n\n2\n2014-01-06\n183.49\n183.56\n182.08\n182.36\n153.36\n108028200\n\n\n3\n2014-01-07\n183.09\n183.79\n182.95\n183.48\n154.30\n86144200\n\n\n4\n2014-01-08\n183.45\n183.83\n182.89\n183.52\n154.33\n96582300\n\n\n\n\n\n\n\nLet’s also remove the columns that we won’t need. We first create a list of the column names that we want to get rid of and then we use the DataFrame.drop() method.\n\nlst_cols = ['high', 'low', 'open', 'close', 'volume',]\ndf_spy.drop(columns=lst_cols, inplace=True)\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nadj_close\n\n\n\n\n0\n2014-01-02\n153.83\n\n\n1\n2014-01-03\n153.80\n\n\n2\n2014-01-06\n153.36\n\n\n3\n2014-01-07\n154.30\n\n\n4\n2014-01-08\n154.33\n\n\n\n\n\n\n\nNotice that trailing commas do not cause errors in Python."
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#series",
    "href": "chapters/01_jumpstart/jumpstart.html#series",
    "title": "1  Python Jumpstart",
    "section": "1.11 Series",
    "text": "1.11 Series\nYou can isolate the columns of a DataFrame with square brackets as follows:\n\ndf_spy['adj_close']\n\n0       153.83\n1       153.80\n2       153.36\n3       154.30\n4       154.33\n         ...  \n1253    217.60\n1254    228.59\n1255    230.35\n1256    230.05\n1257    232.07\nName: adj_close, Length: 1258, dtype: float64\n\n\nThe columns of a DataFrame are a pandas data structure called a Series.\n\ntype(df_spy['adj_close'])\n\npandas.core.series.Series"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#numpy-and-ndarrays",
    "href": "chapters/01_jumpstart/jumpstart.html#numpy-and-ndarrays",
    "title": "1  Python Jumpstart",
    "section": "1.12 numpy and ndarrays",
    "text": "1.12 numpy and ndarrays\nPython is a general purpose programming language and was not created for scientific computing in particular. One of the foundational packages that makes Python well suited to scientific computing is numpy, which has a variety of features including a data type called ndarrays. One of the benefits of ndarrays is that they allow for efficient vector and matrix computation.\nThe values of a Series object is a numpy.ndarray. This is one sense in which pandas is built on top of numpy.\n\ndf_spy['adj_close'].values\n\narray([153.83, 153.8 , 153.36, ..., 230.35, 230.05, 232.07])\n\n\n\ntype(df_spy['adj_close'].values)\n\nnumpy.ndarray"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#series-built-in-methods",
    "href": "chapters/01_jumpstart/jumpstart.html#series-built-in-methods",
    "title": "1  Python Jumpstart",
    "section": "1.13 Series Built-In Methods",
    "text": "1.13 Series Built-In Methods\nSeries have a variety of built-in methods that provide convenient summarization and modification functionality. For example, you can .sum() all the elements of the Series.\n\ndf_spy['adj_close'].sum()\n\n251297.16\n\n\nNext, we calculate the standard deviation of all the elements of the Series using the .std() method.\n\ndf_spy['adj_close'].std()\n\n33.16746781625381\n\n\nThe .shift() built-in method will be useful for calculating returns in the next section - it has the effect of pushing down the values in a Series.\n\ndf_spy['adj_close'].shift()\n\n0          NaN\n1       153.83\n2       153.80\n3       153.36\n4       154.30\n         ...  \n1253    223.51\n1254    217.60\n1255    228.59\n1256    230.35\n1257    230.05\nName: adj_close, Length: 1258, dtype: float64"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#calculating-daily-returns",
    "href": "chapters/01_jumpstart/jumpstart.html#calculating-daily-returns",
    "title": "1  Python Jumpstart",
    "section": "1.14 Calculating Daily Returns",
    "text": "1.14 Calculating Daily Returns\nOur analysis analysis of the leverage effect will involve daily returns for all the days in df_spy. Let’s calculate those now.\nRecall that the end-of-day day \\(t\\) return of a stock is defined as: \\(r_{t} = \\frac{S_{t}}{S_{t-1}} - 1\\), where \\(S_{t}\\) is the stock price at end-of-day \\(t\\).\nHere is a vectorized approach to calculating all the daily returns in a single line of code.\n\ndf_spy['ret'] = df_spy['adj_close'] / df_spy['adj_close'].shift(1) - 1\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nadj_close\nret\n\n\n\n\n0\n2014-01-02\n153.83\nNaN\n\n\n1\n2014-01-03\n153.80\n-0.000195\n\n\n2\n2014-01-06\n153.36\n-0.002861\n\n\n3\n2014-01-07\n154.30\n0.006129\n\n\n4\n2014-01-08\n154.33\n0.000194\n\n\n\n\n\n\n\nNotice that we can create a new column of a DataFrame by using variable assignment syntax."
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#visualizing-adjusted-close-prices",
    "href": "chapters/01_jumpstart/jumpstart.html#visualizing-adjusted-close-prices",
    "title": "1  Python Jumpstart",
    "section": "1.15 Visualizing Adjusted Close Prices",
    "text": "1.15 Visualizing Adjusted Close Prices\nPython has a variety of packages that can be used for visualization. In this chapter we will focus on built-in plotting capabilities of pandas. These capabilities are built on top of the matplotlib package, which is the foundation of much of Python’s visualization ecosystem.\nDataFrames have a built-in .plot() method that makes creating simple line graphs quite easy.\n\ndf_spy.plot(x='date', y='adj_close');\n\n\n\n\nIf we wanted to make this graph more presentable we could do something like:\n\nax = df_spy.\\\n        plot(\n            x = 'date',\n            y = 'adj_close',\n            title = 'SPY: 2014-2018',\n            grid = True,\n            style = 'k',\n            alpha = 0.75,\n            figsize = (9, 4),\n        );\nax.set_xlabel('Trade Date');\nax.set_ylabel('Close Price');\n\n\n\n\nNotice that the ax variable created above is a matplotlib object.\n\ntype(ax)\n\nmatplotlib.axes._axes.Axes"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#visualizing-returns",
    "href": "chapters/01_jumpstart/jumpstart.html#visualizing-returns",
    "title": "1  Python Jumpstart",
    "section": "1.16 Visualizing Returns",
    "text": "1.16 Visualizing Returns\npandas also gives us the ability to simultaneously plot two different columns of a DataFrame in separate subplots of a single graph. Here is what that code looks like:\n\ndf_spy.plot(x='date', y=['adj_close', 'ret',], subplots=True, style='k', alpha=0.75, figsize=(9, 8), grid=True);\n\n\n\n\nThe returns graph above is a bit of a hack, it doesn’t really make sense to create a line graph of consecutive returns. However, because there are so many days jammed into the x-axis, it creates a desirable effect and it used all the time in finance to demonstrate properties of volatility.\nNotice that whenever there is a sharp drop in the adj_close price graph, that the magnitude of the nearby returns becomes large. In contrast, during periods of steady growth (e.g. all of 2017) the magnitude of the returns is small. This is precisely the leverage effect."
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#calculating-realized-volatility",
    "href": "chapters/01_jumpstart/jumpstart.html#calculating-realized-volatility",
    "title": "1  Python Jumpstart",
    "section": "1.17 Calculating Realized Volatility",
    "text": "1.17 Calculating Realized Volatility\nRealized volatility is defined as the standard deviation of the daily returns; it indicates how much variability in the stock price there has been. It is a matter of convention to annualize this quantity, so we multiply it by \\(\\sqrt{252}\\).\nThe following vectorized code calculates a rolling 2-month volatility for our SPY price data.\n\ndf_spy['ret'].rolling(42).std() * np.sqrt(252)\n\n0            NaN\n1            NaN\n2            NaN\n3            NaN\n4            NaN\n          ...   \n1253    0.226735\n1254    0.252813\n1255    0.249195\n1256    0.246019\n1257    0.247027\nName: ret, Length: 1258, dtype: float64\n\n\nLet’s add these realized volatility calculations todf_spy this with the following code.\n\ndf_spy['realized_vol'] = df_spy['ret'].rolling(42).std() * np.sqrt(252)\ndf_spy\n\n\n\n\n\n\n\n\ndate\nadj_close\nret\nrealized_vol\n\n\n\n\n0\n2014-01-02\n153.83\nNaN\nNaN\n\n\n1\n2014-01-03\n153.80\n-0.000195\nNaN\n\n\n2\n2014-01-06\n153.36\n-0.002861\nNaN\n\n\n3\n2014-01-07\n154.30\n0.006129\nNaN\n\n\n4\n2014-01-08\n154.33\n0.000194\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n1253\n2018-12-24\n217.60\n-0.026442\n0.226735\n\n\n1254\n2018-12-26\n228.59\n0.050506\n0.252813\n\n\n1255\n2018-12-27\n230.35\n0.007699\n0.249195\n\n\n1256\n2018-12-28\n230.05\n-0.001302\n0.246019\n\n\n1257\n2018-12-31\n232.07\n0.008781\n0.247027\n\n\n\n\n1258 rows × 4 columns"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#visualizing-realized-volatility",
    "href": "chapters/01_jumpstart/jumpstart.html#visualizing-realized-volatility",
    "title": "1  Python Jumpstart",
    "section": "1.18 Visualizing Realized Volatility",
    "text": "1.18 Visualizing Realized Volatility\nWe can easily add realized_vol to our graph with the following code.\n\ndf_spy.plot(x = 'date', \n            y = ['adj_close','ret','realized_vol',], \n            subplots=True, style='k', alpha=0.75, \n            figsize=(9, 12), \n            grid=True);\n\n\n\n\nThis graph is an excellent illustration of the leverage effect. When SPY suffers losses, there is a spike in realized volatility, which is to say that the magnitude of the nearby returns increases."
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#ides-for-jupter-notebooks",
    "href": "chapters/01_jumpstart/jumpstart.html#ides-for-jupter-notebooks",
    "title": "1  Python Jumpstart",
    "section": "1.2 IDEs for Jupter Notebooks",
    "text": "1.2 IDEs for Jupter Notebooks\nYou will need another piece of software called an integrated development environment (IDE) to actually work with Jupyter notebooks; here are three popular and free IDEs for working with them:\n\nJupyterLab - my personal favorite, created by the Jupyter project, which also creates the Jupyter notebook format.\nJupyter Notebook Classic - this was the predecessor to JupyterLab, also created by the Jupyter project.\nVSCode - an general purpose IDE created my Microsoft."
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#ides-for-jupyter-notebooks",
    "href": "chapters/01_jumpstart/jumpstart.html#ides-for-jupyter-notebooks",
    "title": "1  Python Jumpstart",
    "section": "1.2 IDEs for Jupyter Notebooks",
    "text": "1.2 IDEs for Jupyter Notebooks\nYou will need another piece of software called an integrated development environment (IDE) to actually work with Jupyter notebooks; here are three popular and free IDEs for working with them:\n\nJupyterLab - my personal favorite, created by the Jupyter project, which also creates the Jupyter notebook format.\nJupyter Notebook Classic - this was the predecessor to JupyterLab, also created by the Jupyter project.\nVSCode - an general purpose IDE created my Microsoft."
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#related-reading",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#related-reading",
    "title": "2  DataFrame Basics",
    "section": "2.10 Related Reading",
    "text": "2.10 Related Reading\nPython Data Science Handbook - Section 3.1 - Introducing Pandas Objects\nPython Data Science Handbook - Section 2.1 - Understanding Data Types in Python\nPython Data Science Handbook - Section 2.2 - The Basics of NumPy Arrays\nPython Data Science Handbook - Section 2.3 - Computation on NumPy Arrays: Universal Functions\nPython Data Science Handbook - Section 2.4 - Aggregations: Min, Max, and Everything In Between"
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#importing-packages",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#importing-packages",
    "title": "2  DataFrame Basics",
    "section": "2.1 Importing Packages",
    "text": "2.1 Importing Packages\nLet’s begin by importing the packages that we will need.\n\nimport pandas as pd\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr\npd.set_option('display.max_rows', 10)"
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#reading-in-data",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#reading-in-data",
    "title": "2  DataFrame Basics",
    "section": "2.2 Reading-In Data",
    "text": "2.2 Reading-In Data\nNext, let’s use pandas_datareader to read-in SPY prices from March 2020. SPY is an ETF that tracks the S&P500 index.\n\ndf_spy = pdr.get_data_yahoo('SPY', start='2020-02-28', end='2020-03-31')\ndf_spy = df_spy.round(2)\ndf_spy.head()\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n\n\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n\n\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n\n\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n\n\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800\n\n\n\n\n\n\n\nLet’s also make the Date a regular column, instead of an index, and also make the column names snake-case.\n\ndf_spy.reset_index(drop=False, inplace=True)\ndf_spy.columns = df_spy.columns.str.lower().str.replace(' ', '_')\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n0\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n\n\n1\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n\n\n2\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n\n\n3\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n\n\n4\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800"
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#exploring-a-dataframe",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#exploring-a-dataframe",
    "title": "2  DataFrame Basics",
    "section": "2.3 Exploring a DataFrame",
    "text": "2.3 Exploring a DataFrame\nWe can explore our df_spy DataFrame in a variety of ways.\nFirst, we can first use the type() method to make sure what we have created is in fact a DataFrame.\n\ntype(df_spy)\n\npandas.core.frame.DataFrame\n\n\nNext, we can use the .dtypes attribute of the DataFrame to see the data types of each of the columns.\n\ndf_spy.dtypes\n\ndate         datetime64[ns]\nopen                float64\nhigh                float64\nlow                 float64\nclose               float64\nadj_close           float64\nvolume                int64\ndtype: object\n\n\nWe can also check the number of rows and columns by using the .shape attribute.\n\ndf_spy.shape\n\n(22, 7)\n\n\nAs we can see, our DataFrame df_spy consists of 22 rows and 7 columns.\n\nCode Challenge: Try the DataFrame.info() and DataFrame.describe() methods on df_spy.\n\n\nSolution\ndf_spy.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 22 entries, 0 to 21\nData columns (total 7 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   date       22 non-null     datetime64[ns]\n 1   open       22 non-null     float64       \n 2   high       22 non-null     float64       \n 3   low        22 non-null     float64       \n 4   close      22 non-null     float64       \n 5   adj_close  22 non-null     float64       \n 6   volume     22 non-null     int64         \ndtypes: datetime64[ns](1), float64(5), int64(1)\nmemory usage: 1.3 KB\n\n\n\n\nSolution\ndf_spy.describe().round(2)\n\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\ncount\n22\n22.00\n22.00\n22.00\n22.00\n22.00\n2.200000e+01\n\n\nmean\n2020-03-14 12:00:00\n265.03\n272.89\n258.81\n266.54\n252.62\n2.780051e+08\n\n\nmin\n2020-02-28 00:00:00\n228.19\n229.68\n218.26\n222.95\n212.18\n1.713695e+08\n\n\n25%\n2020-03-06 18:00:00\n243.12\n256.22\n237.14\n244.06\n232.24\n2.362968e+08\n\n\n50%\n2020-03-14 12:00:00\n255.85\n264.73\n250.05\n261.42\n248.80\n2.828830e+08\n\n\n75%\n2020-03-22 06:00:00\n287.68\n295.55\n282.53\n294.30\n278.46\n3.218732e+08\n\n\nmax\n2020-03-30 00:00:00\n309.50\n313.84\n303.33\n312.86\n296.01\n3.922207e+08\n\n\nstd\nNaN\n26.43\n25.44\n26.98\n27.55\n25.74\n6.134551e+07"
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#dataframe-columns",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#dataframe-columns",
    "title": "2  DataFrame Basics",
    "section": "2.4 DataFrame Columns",
    "text": "2.4 DataFrame Columns\nIn order to isolate a particular column of a DataFrame we can use square brackets ([ ]). The following code isolates the close price column of df_spy.\n\ndf_spy['close']\n\n0     296.26\n1     309.09\n2     300.24\n3     312.86\n4     302.46\n       ...  \n17    243.15\n18    246.79\n19    261.20\n20    253.42\n21    261.65\nName: close, Length: 22, dtype: float64\n\n\n\nCode Challenge: Isolate the date column of df_spy.\n\n\nSolution\ndf_spy['date']\n\n\n0    2020-02-28\n1    2020-03-02\n2    2020-03-03\n3    2020-03-04\n4    2020-03-05\n        ...    \n17   2020-03-24\n18   2020-03-25\n19   2020-03-26\n20   2020-03-27\n21   2020-03-30\nName: date, Length: 22, dtype: datetime64[ns]\n\n\n\nAs we can see from the following code, each column of a DataFrame is actually a different kind of pandas structure called a Series.\n\ntype(df_spy['close'])\n\npandas.core.series.Series\n\n\nHere is a bit of pandas inside baseball:\n\nA DataFrame is collection of columns that are glued together.\nEach column is a Series.\nA Series has two main attributes: 1) .values; 2) .index.\nThe .values component of a Series is a numpy.array.\n\nLet’s look at the .values attribute of the close column of df_spy.\n\ndf_spy['close'].values\n\narray([296.26, 309.09, 300.24, 312.86, 302.46, 297.46, 274.23, 288.42,\n       274.36, 248.11, 269.32, 239.85, 252.8 , 240.  , 240.51, 228.8 ,\n       222.95, 243.15, 246.79, 261.2 , 253.42, 261.65])\n\n\n\nCode Challenge: Verify that the values component of the close column of df_spy is in fact a a numpy.array.\n\n\nSolution\ntype(df_spy['close'].values)\n\n\nnumpy.ndarray"
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#component-wise-column-operations",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#component-wise-column-operations",
    "title": "2  DataFrame Basics",
    "section": "2.5 Component-wise Column Operations",
    "text": "2.5 Component-wise Column Operations\nWe can perform component-wise (i.e. vector-like) calculations with DataFrame columns.\nThe following code divides all the close prices by 100.\n\ndf_spy['close'] / 100\n\n0     2.9626\n1     3.0909\n2     3.0024\n3     3.1286\n4     3.0246\n       ...  \n17    2.4315\n18    2.4679\n19    2.6120\n20    2.5342\n21    2.6165\nName: close, Length: 22, dtype: float64\n\n\nWe can also perform component-wise calculations between two colums.\nLet’s say we want to calculate the intraday range of SPY for each of the trade-dates in df_spy; this is the difference between the high and the low of each day. We can do this easily from the columns of our DataFrame.\n\ndf_spy['high'] - df_spy['low']\n\n0     12.35\n1     14.70\n2     16.27\n3      9.77\n4      8.46\n      ...  \n17    10.30\n18    16.60\n19    13.75\n20     9.76\n21     8.90\nLength: 22, dtype: float64\n\n\n\nCode Challenge: Calculate the difference between the close and open columns of df_spy.\n\n\nSolution\ndf_spy['close'] - df_spy['open']\n\n\n0      7.56\n1     10.88\n2     -9.26\n3      6.74\n4     -2.52\n      ...  \n17     8.73\n18     1.92\n19    11.68\n20     0.15\n21     5.95\nLength: 22, dtype: float64"
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#adding-columns-via-variable-assignment",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#adding-columns-via-variable-assignment",
    "title": "2  DataFrame Basics",
    "section": "2.6 Adding Columns via Variable Assignment",
    "text": "2.6 Adding Columns via Variable Assignment\nLet’s say we want to save our intraday ranges back into df_spy for further analysis later. The most straightforward way to do this is using variable assignment as follows.\n\ndf_spy['intraday_range'] = df_spy['high'] - df_spy['low']\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\nintraday_range\n\n\n\n\n0\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n12.35\n\n\n1\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n14.70\n\n\n2\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n16.27\n\n\n3\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n9.77\n\n\n4\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800\n8.46\n\n\n\n\n\n\n\n\nCode Challenge: Add a new column to df_spy called open_to_close that consists of the difference between the close and open of each day.\n\n\nSolution\ndf_spy['open_to_close'] = df_spy['close'] - df_spy['open']\ndf_spy.head()\n\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\nintraday_range\nopen_to_close\n\n\n\n\n0\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n12.35\n7.56\n\n\n1\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n14.70\n10.88\n\n\n2\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n16.27\n-9.26\n\n\n3\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n9.77\n6.74\n\n\n4\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800\n8.46\n-2.52"
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#adding-columns-via-.assign",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#adding-columns-via-.assign",
    "title": "2  DataFrame Basics",
    "section": "2.7 Adding Columns via .assign()",
    "text": "2.7 Adding Columns via .assign()\nA powerful but less intuitive way of adding a column to a DataFrame uses the .assign() function, which makes use of lambda functions (i.e. anonymous functions).\nThe following code adds another column called intraday_range_assign.\n\ndf_spy.assign(intraday_range_assign = lambda df: df['high'] - df['low'])\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\nintraday_range\nopen_to_close\nintraday_range_assign\n\n\n\n\n0\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n12.35\n7.56\n12.35\n\n\n1\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n14.70\n10.88\n14.70\n\n\n2\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n16.27\n-9.26\n16.27\n\n\n3\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n9.77\n6.74\n9.77\n\n\n4\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800\n8.46\n-2.52\n8.46\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17\n2020-03-24\n234.42\n244.10\n233.80\n243.15\n231.41\n235494500\n10.30\n8.73\n10.30\n\n\n18\n2020-03-25\n244.87\n256.35\n239.75\n246.79\n234.87\n299430300\n16.60\n1.92\n16.60\n\n\n19\n2020-03-26\n249.52\n262.80\n249.05\n261.20\n248.59\n257632800\n13.75\n11.68\n13.75\n\n\n20\n2020-03-27\n253.27\n260.81\n251.05\n253.42\n241.18\n224341200\n9.76\n0.15\n9.76\n\n\n21\n2020-03-30\n255.70\n262.43\n253.53\n261.65\n249.02\n171369500\n8.90\n5.95\n8.90\n\n\n\n\n22 rows × 10 columns\n\n\n\n\nCode Challenge: Verify that the column intraday_range_assign was not actually added to the df_spy.\n\n\nSolution\ndf_spy.head()\n\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\nintraday_range\nopen_to_close\n\n\n\n\n0\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n12.35\n7.56\n\n\n1\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n14.70\n10.88\n\n\n2\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n16.27\n-9.26\n\n\n3\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n9.77\n6.74\n\n\n4\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800\n8.46\n-2.52\n\n\n\n\n\n\n\n\nIn order to add the intraday_range_assign column to df_spy we will need to reassign to it.\n\ndf_spy = df_spy.assign(intraday_range_assign = lambda df: df['high'] - df['low'])\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\nintraday_range\nopen_to_close\nintraday_range_assign\n\n\n\n\n0\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n12.35\n7.56\n12.35\n\n\n1\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n14.70\n10.88\n14.70\n\n\n2\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n16.27\n-9.26\n16.27\n\n\n3\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n9.77\n6.74\n9.77\n\n\n4\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800\n8.46\n-2.52\n8.46\n\n\n\n\n\n\n\n\nCode Challenge: Use .assign() to create a new column in df_spy, call it open_to_close_assign, that contains the difference between the close and open.\n\n\nSolution\ndf_spy = df_spy.assign(open_to_close_assign = lambda df: df['close'] - df['open'])\ndf_spy.head()\n\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\nintraday_range\nopen_to_close\nintraday_range_assign\nopen_to_close_assign\n\n\n\n\n0\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n12.35\n7.56\n12.35\n7.56\n\n\n1\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n14.70\n10.88\n14.70\n10.88\n\n\n2\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n16.27\n-9.26\n16.27\n-9.26\n\n\n3\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n9.77\n6.74\n9.77\n6.74\n\n\n4\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800\n8.46\n-2.52\n8.46\n-2.52"
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#method-chaining",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#method-chaining",
    "title": "2  DataFrame Basics",
    "section": "2.8 Method Chaining",
    "text": "2.8 Method Chaining\nThe value of .assign() becomes clear when we start chaining methods together.\nIn order to see this let’s first drop the columns that we created.\n\nlst_cols = ['intraday_range', 'open_to_close', 'intraday_range_assign', 'open_to_close_assign']\ndf_spy.drop(columns=lst_cols, inplace=True)\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n0\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n\n\n1\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n\n\n2\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n\n\n3\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n\n\n4\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800\n\n\n\n\n\n\n\nThe following code adds the intraday and and open_to_close columns at the same time.\n\ndf_spy = \\\n    (\n    df_spy\n        .assign(intraday_range = lambda df: df['high'] - df['low'])\n        .assign(open_to_close = lambda df: df['close'] - df['open'])\n    )\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\nintraday_range\nopen_to_close\n\n\n\n\n0\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n12.35\n7.56\n\n\n1\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n14.70\n10.88\n\n\n2\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n16.27\n-9.26\n\n\n3\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n9.77\n6.74\n\n\n4\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800\n8.46\n-2.52\n\n\n\n\n\n\n\n\nCode Challenge: Use .assign() to add a two new column to df_spy:\n\nthe difference between the close and adj_close\nthe average of the low and open\n\n\n\nSolution\ndf_spy = \\\n    (\n    df_spy\n        .assign(div = lambda df: df['close'] - df['adj_close'])\n        .assign(avg = lambda df: (df['low'] + df['open']) / 2)\n    )\ndf_spy.head()\n\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\nintraday_range\nopen_to_close\ndiv\navg\n\n\n\n\n0\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n12.35\n7.56\n15.95\n287.120\n\n\n1\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n14.70\n10.88\n16.64\n296.335\n\n\n2\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n16.27\n-9.26\n16.17\n303.535\n\n\n3\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n9.77\n6.74\n16.85\n304.725\n\n\n4\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800\n8.46\n-2.52\n16.29\n302.495"
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#aggregating-calulations-on-series",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#aggregating-calulations-on-series",
    "title": "2  DataFrame Basics",
    "section": "2.9 Aggregating Calulations on Series",
    "text": "2.9 Aggregating Calulations on Series\nSeries have a variety of built-in aggregation functions.\nFor example, we can use the following code to calculate the total SPY volume during March 2020.\n\ndf_spy['volume'].sum()\n\n6116112300\n\n\nHere are some summary statistics on the intraday_range column that we added to our DataFrame earlier.\n\nprint(\"Mean:  \", df_spy['intraday_range'].mean()) # average\nprint(\"St Dev: \", df_spy['intraday_range'].std()) # standard deviation\nprint(\"Min:    \" , df_spy['intraday_range'].min()) # minimum\nprint(\"Max:   \" , df_spy['intraday_range'].max()) # maximum\n\nMean:   14.077727272727275\nSt Dev:  4.28352428533215\nMin:     8.460000000000036\nMax:    22.960000000000008\n\n\n\nCode Challenge: Calculate the average daily volume for the trade dates in df_spy.\n\n\nSolution\ndf_spy['volume'].mean()\n\n\n278005104.54545456"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#what-is-a-jupyter-notebook",
    "href": "chapters/01_jumpstart/jumpstart.html#what-is-a-jupyter-notebook",
    "title": "1  Python Jumpstart",
    "section": "1.1 What is a Jupyter Notebook?",
    "text": "1.1 What is a Jupyter Notebook?\nThe notebook format conveniently allows you to combine sentences, code, code outputs (including plots), and mathematical notation. Notebooks have proven to be a convenient and productive programming environment for data analysis.\nBehind the scenes of a Jupyter notebook is a kernel that is responsible for executing computations. The kernel can live locally on your machine or on a remote server."
  }
]