[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python for Data Analysis in Finance",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "parts/02_part_visualization/part_visualization.html",
    "href": "parts/02_part_visualization/part_visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "Unlike R, in which a single package (ggplot2) has become the de facto standard for visualization, in Python there are a variety of packages that form a visualization ecosystem. The matplotlib package is the foundation of much of this ecosystem, and many elements of this ecosystem can be thought of as wrappers around matplotlib. Such packages include seaborn and also the visualization tools in pandas which we discuss here."
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html",
    "href": "chapters/16_trend_following/trend_following.html",
    "title": "16  Trend Following",
    "section": "",
    "text": "17 Calculating Returns, Equity Curves, and Drawdowns\nWe now have all the data the we need to calculate daily returns, the equity curves, and drawdowns of the two strategies.\n# returns\ndf_asset['buy_hold_return'] = df_asset['adj_close'].pct_change()\ndf_asset['trend_return'] = df_asset['buy_hold_return'] * df_asset['trend_position'].shift(1)\n\n# growth factors\ndf_asset['buy_hold_factor'] = 1 + df_asset['buy_hold_return']\ndf_asset['trend_factor'] = 1 + df_asset['trend_return']\n\n# equity curves\ndf_asset['buy_hold_equity'] = df_asset['buy_hold_factor'].cumprod()\ndf_asset['trend_equity'] = df_asset['trend_factor'].cumprod()\n\n# maximum cummulative equity\ndf_asset['buy_hold_max_equity'] = df_asset['buy_hold_equity'].cummax()\ndf_asset['trend_max_equity'] = df_asset['trend_equity'].cummax()\n\n# drawdownd\ndf_asset['buy_hold_drawdown'] = (df_asset['buy_hold_equity'] - df_asset['buy_hold_max_equity']) / df_asset['buy_hold_max_equity']\ndf_asset['trend_drawdown'] = (df_asset['trend_equity'] - df_asset['trend_max_equity']) / df_asset['trend_max_equity']\n\n# graphing equity curves\ndf_asset.plot(x='date', y=['buy_hold_equity','trend_equity'], grid=True, title='Equity Graph');\nFinally, we calculate some basic performance metrics.\nAnnualized Returns\nprint('buy-hold return: ', np.round(df_asset['buy_hold_equity'].iloc[-1] ** (252 / (len(df_asset) - 1)) - 1, 3) * 100, '%')\nprint('trend return:    ', np.round(df_asset['trend_equity'].iloc[-1] ** (252 / (len(df_asset) - 1)) - 1, 3) * 100, '%')\n\nbuy-hold return:  8.200000000000001 %\ntrend return:     7.199999999999999 %\nSharpe-Ratio\nprint('buy-hold sharpe-ratio: ', np.round((np.mean(df_asset['buy_hold_return']) / np.std(df_asset['buy_hold_return'])) * np.sqrt(252), 2))\nprint('trend sharpe-ratio:    ', np.round((np.mean(df_asset['trend_return']) / np.std(df_asset['trend_return'])) * np.sqrt(252), 2))\n\nbuy-hold sharpe-ratio:  0.51\ntrend sharpe-ratio:     0.66\nMaximum Drawdown\nprint('buy-hold max-drawdown: ', np.round(np.min(df_asset['buy_hold_drawdown']), 2))\nprint('trend max-drawdown:    ', np.round(np.min(df_asset['trend_drawdown']), 2))\n\nbuy-hold max-drawdown:  -0.55\ntrend max-drawdown:     -0.24\nLet’s now take a look at a particular case study of the S&P500 in the period of 1990-2012. We will use the Total Returns futures to represent an investment in the S&P500. In order to get the result we describe, rerun this notebook with the following inputs:\nticker = '^SP500TR'\nstart_date = '1989-12-29'\nend_date = '2012-01-01'\nsma_days = 200\nWe first compare our equity graph above to Faber’s equity graph which we present below:\nAs we can see in Faber’s analysis, trend-following slightly outperforms the buy-and-hold strategy, while in our analysis the trend-following strategy underperforms. This difference is likely due to the difference in rebalance frequency. Our strategy rebalances daily (which is probably too much) while Faber’s rebalances monthly.\nAnalyzing our results more closely, we see that buy-and-hold has an annualized return of 8.2%, while trend-following has an annualized return of 7.2%. There is, however, significantly less downside risk with trend-following, as the strategy tends to sit-out bear markets (e.g. 2000 and 2008). This results in a max-drawdown of -24% for trend following, while buy-and-hold had a max-drawdown of -55% during the 2008 financial crisis. This reduced downside risk can also be seen in the Sharpe-ratio, a metric by which trend-following (0.66) outperforms buy-and-hold (0.51)."
  },
  {
    "objectID": "chapters/18_close_to_close/close_to_close.html#loading-packages",
    "href": "chapters/18_close_to_close/close_to_close.html#loading-packages",
    "title": "17  Volatility Forecasting: Close-to-Close Estimator",
    "section": "17.1 Loading Packages",
    "text": "17.1 Loading Packages\nLet’s begin by loading the packages that we will need.\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr\nimport sklearn\npd.options.display.max_rows = 10"
  },
  {
    "objectID": "chapters/18_close_to_close/close_to_close.html#reading-in-spy-data-from-yahoo-finance",
    "href": "chapters/18_close_to_close/close_to_close.html#reading-in-spy-data-from-yahoo-finance",
    "title": "17  Volatility Forecasting: Close-to-Close Estimator",
    "section": "17.2 Reading-In SPY Data From Yahoo Finance",
    "text": "17.2 Reading-In SPY Data From Yahoo Finance\nSepp’s analysis covers data starting from 1/1/2005 and ending on 4/2/2016. Let’s grab these SPY prices from Yahoo Finance using pandas_datareader.\n\ndf_spy = pdr.get_data_yahoo('SPY', start = '2004-12-31', end = '2016-04-02').reset_index()\ndf_spy.columns = df_spy.columns.str.lower().str.replace(' ', '_')\ndf_spy.rename(columns = {'date':'trade_date'}, inplace = True)\ndf_spy.insert(0, 'ticker', 'SPY')\ndf_spy\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nticker\ntrade_date\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n0\nSPY\n2004-12-31\n121.300003\n121.660004\n120.800003\n120.870003\n84.657806\n28648800\n\n\n1\nSPY\n2005-01-03\n121.559998\n121.760002\n119.900002\n120.300003\n84.258568\n55748000\n\n\n2\nSPY\n2005-01-04\n120.459999\n120.540001\n118.440002\n118.830002\n83.228973\n69167600\n\n\n3\nSPY\n2005-01-05\n118.739998\n119.250000\n118.000000\n118.010002\n82.654648\n65667300\n\n\n4\nSPY\n2005-01-06\n118.440002\n119.150002\n118.260002\n118.610001\n83.074875\n47814700\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2827\nSPY\n2016-03-28\n203.610001\n203.860001\n202.710007\n203.240005\n178.770126\n62408200\n\n\n2828\nSPY\n2016-03-29\n202.759995\n205.250000\n202.399994\n205.119995\n180.423782\n92922900\n\n\n2829\nSPY\n2016-03-30\n206.300003\n206.869995\n205.589996\n206.020004\n181.215378\n86365300\n\n\n2830\nSPY\n2016-03-31\n205.910004\n206.410004\n205.330002\n205.520004\n180.775589\n94584100\n\n\n2831\nSPY\n2016-04-01\n204.350006\n207.139999\n203.979996\n206.919998\n182.007065\n114423500\n\n\n\n\n2832 rows × 8 columns"
  },
  {
    "objectID": "chapters/18_close_to_close/close_to_close.html#calculating-daily-returns-realized-volatility",
    "href": "chapters/18_close_to_close/close_to_close.html#calculating-daily-returns-realized-volatility",
    "title": "17  Volatility Forecasting: Close-to-Close Estimator",
    "section": "17.3 Calculating Daily Returns & Realized Volatility",
    "text": "17.3 Calculating Daily Returns & Realized Volatility\nThe close-to-close estimator is a function of daily returns so let’s calculate those now. In particular, we will use log-returns.\n\ndf_spy['dly_ret'] = np.log(df_spy['close']).diff()\ndf_spy = df_spy[1:].reset_index(drop = True)\ndf_spy\n\n\n\n\n\n\n\n\nticker\ntrade_date\nopen\nhigh\nlow\nclose\nadj_close\nvolume\ndly_ret\n\n\n\n\n0\nSPY\n2005-01-03\n121.559998\n121.760002\n119.900002\n120.300003\n84.258568\n55748000\n-0.004727\n\n\n1\nSPY\n2005-01-04\n120.459999\n120.540001\n118.440002\n118.830002\n83.228973\n69167600\n-0.012295\n\n\n2\nSPY\n2005-01-05\n118.739998\n119.250000\n118.000000\n118.010002\n82.654648\n65667300\n-0.006925\n\n\n3\nSPY\n2005-01-06\n118.440002\n119.150002\n118.260002\n118.610001\n83.074875\n47814700\n0.005071\n\n\n4\nSPY\n2005-01-07\n118.970001\n119.230003\n118.129997\n118.440002\n82.955849\n55847700\n-0.001434\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2826\nSPY\n2016-03-28\n203.610001\n203.860001\n202.710007\n203.240005\n178.770126\n62408200\n0.000591\n\n\n2827\nSPY\n2016-03-29\n202.759995\n205.250000\n202.399994\n205.119995\n180.423782\n92922900\n0.009208\n\n\n2828\nSPY\n2016-03-30\n206.300003\n206.869995\n205.589996\n206.020004\n181.215378\n86365300\n0.004378\n\n\n2829\nSPY\n2016-03-31\n205.910004\n206.410004\n205.330002\n205.520004\n180.775589\n94584100\n-0.002430\n\n\n2830\nSPY\n2016-04-01\n204.350006\n207.139999\n203.979996\n206.919998\n182.007065\n114423500\n0.006789\n\n\n\n\n2831 rows × 9 columns"
  },
  {
    "objectID": "chapters/18_close_to_close/close_to_close.html#organizing-dates-for-backtest",
    "href": "chapters/18_close_to_close/close_to_close.html#organizing-dates-for-backtest",
    "title": "17  Volatility Forecasting: Close-to-Close Estimator",
    "section": "17.4 Organizing Dates for Backtest",
    "text": "17.4 Organizing Dates for Backtest\nOrganizing dates is an important step in a historical analysis.\nWe are performing a weekly analysis, which means that in later steps we will performing aggregation calculations of daily calculations grouped into weeks. Therefore, we will need to add a column to df_spy that will allow us to group by weeks.\nThe key to our approach will be to use the .dt.weekday attribute of the trade_date columns. In the following code, the variable weekday is a Series that contains the weekday associated with each date. Notice that Monday is encoded by 0 and Friday is encoded by 4.\n\nweekday = df_spy['trade_date'].dt.weekday\nweekday\n\n0       0\n1       1\n2       2\n3       3\n4       4\n       ..\n2826    0\n2827    1\n2828    2\n2829    3\n2830    4\nName: trade_date, Length: 2831, dtype: int32\n\n\nThe following code is a simple for-loop that has the effect of creating a week-number for each week.\n\nweek_num = []\nix_week = 0\nweek_num.append(ix_week)\nfor ix in range(0, len(weekday) - 1):\n    prev_day = weekday[ix]\n    curr_day = weekday[ix + 1]\n    if curr_day &lt; prev_day:\n        ix_week = ix_week + 1\n    week_num.append(ix_week)\nnp.array(week_num) # I use the array function simply because it looks better when it prints\n\narray([  0,   0,   0, ..., 586, 586, 586])\n\n\nLet’s now insert the week numbers into df_spy.\n\ndf_spy.insert(2, 'week_num', week_num)\ndf_spy\n\n\n\n\n\n\n\n\nticker\ntrade_date\nweek_num\nopen\nhigh\nlow\nclose\nadj_close\nvolume\ndly_ret\n\n\n\n\n0\nSPY\n2005-01-03\n0\n121.559998\n121.760002\n119.900002\n120.300003\n84.258568\n55748000\n-0.004727\n\n\n1\nSPY\n2005-01-04\n0\n120.459999\n120.540001\n118.440002\n118.830002\n83.228973\n69167600\n-0.012295\n\n\n2\nSPY\n2005-01-05\n0\n118.739998\n119.250000\n118.000000\n118.010002\n82.654648\n65667300\n-0.006925\n\n\n3\nSPY\n2005-01-06\n0\n118.440002\n119.150002\n118.260002\n118.610001\n83.074875\n47814700\n0.005071\n\n\n4\nSPY\n2005-01-07\n0\n118.970001\n119.230003\n118.129997\n118.440002\n82.955849\n55847700\n-0.001434\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2826\nSPY\n2016-03-28\n586\n203.610001\n203.860001\n202.710007\n203.240005\n178.770126\n62408200\n0.000591\n\n\n2827\nSPY\n2016-03-29\n586\n202.759995\n205.250000\n202.399994\n205.119995\n180.423782\n92922900\n0.009208\n\n\n2828\nSPY\n2016-03-30\n586\n206.300003\n206.869995\n205.589996\n206.020004\n181.215378\n86365300\n0.004378\n\n\n2829\nSPY\n2016-03-31\n586\n205.910004\n206.410004\n205.330002\n205.520004\n180.775589\n94584100\n-0.002430\n\n\n2830\nSPY\n2016-04-01\n586\n204.350006\n207.139999\n203.979996\n206.919998\n182.007065\n114423500\n0.006789\n\n\n\n\n2831 rows × 10 columns\n\n\n\n\nDiscussion Question: The pandas.Series.dt.week attribute gives the week-of-the-year for a give trade-date. My initial idea was to use .dt.week and dt.year for my grouping, but I ran into an issue. Can you think what the issue was?\n\n\nSolution\n##&gt; Weeks at the beginning and end of the year may be partial weeks.\n\n\n\nWe can now use .groupby() to calculate the starting and ending dates for each week.\n\ndf_start_end = \\\n    (\n    df_spy.groupby(['week_num'], as_index = False)[['trade_date']].agg([min, max])['trade_date']\n    .rename(columns = {'min':'week_start', 'max':'week_end'})\n    .reset_index()\n    .rename(columns = {'index':'week_num'})\n    )\ndf_start_end\n\n\n\n\n\n\n\n\nweek_num\nweek_start\nweek_end\n\n\n\n\n0\n0\n2005-01-03\n2005-01-07\n\n\n1\n1\n2005-01-10\n2005-01-14\n\n\n2\n2\n2005-01-18\n2005-01-21\n\n\n3\n3\n2005-01-24\n2005-01-28\n\n\n4\n4\n2005-01-31\n2005-02-04\n\n\n...\n...\n...\n...\n\n\n582\n582\n2016-02-29\n2016-03-04\n\n\n583\n583\n2016-03-07\n2016-03-11\n\n\n584\n584\n2016-03-14\n2016-03-18\n\n\n585\n585\n2016-03-21\n2016-03-24\n\n\n586\n586\n2016-03-28\n2016-04-01\n\n\n\n\n587 rows × 3 columns\n\n\n\nLet’s merge these columns into df_spy.\n\ndf_spy = df_spy.merge(df_start_end)\ndf_spy\n\n\n\n\n\n\n\n\nticker\ntrade_date\nweek_num\nopen\nhigh\nlow\nclose\nadj_close\nvolume\ndly_ret\nweek_start\nweek_end\n\n\n\n\n0\nSPY\n2005-01-03\n0\n121.559998\n121.760002\n119.900002\n120.300003\n84.258568\n55748000\n-0.004727\n2005-01-03\n2005-01-07\n\n\n1\nSPY\n2005-01-04\n0\n120.459999\n120.540001\n118.440002\n118.830002\n83.228973\n69167600\n-0.012295\n2005-01-03\n2005-01-07\n\n\n2\nSPY\n2005-01-05\n0\n118.739998\n119.250000\n118.000000\n118.010002\n82.654648\n65667300\n-0.006925\n2005-01-03\n2005-01-07\n\n\n3\nSPY\n2005-01-06\n0\n118.440002\n119.150002\n118.260002\n118.610001\n83.074875\n47814700\n0.005071\n2005-01-03\n2005-01-07\n\n\n4\nSPY\n2005-01-07\n0\n118.970001\n119.230003\n118.129997\n118.440002\n82.955849\n55847700\n-0.001434\n2005-01-03\n2005-01-07\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2826\nSPY\n2016-03-28\n586\n203.610001\n203.860001\n202.710007\n203.240005\n178.770126\n62408200\n0.000591\n2016-03-28\n2016-04-01\n\n\n2827\nSPY\n2016-03-29\n586\n202.759995\n205.250000\n202.399994\n205.119995\n180.423782\n92922900\n0.009208\n2016-03-28\n2016-04-01\n\n\n2828\nSPY\n2016-03-30\n586\n206.300003\n206.869995\n205.589996\n206.020004\n181.215378\n86365300\n0.004378\n2016-03-28\n2016-04-01\n\n\n2829\nSPY\n2016-03-31\n586\n205.910004\n206.410004\n205.330002\n205.520004\n180.775589\n94584100\n-0.002430\n2016-03-28\n2016-04-01\n\n\n2830\nSPY\n2016-04-01\n586\n204.350006\n207.139999\n203.979996\n206.919998\n182.007065\n114423500\n0.006789\n2016-03-28\n2016-04-01\n\n\n\n\n2831 rows × 12 columns"
  },
  {
    "objectID": "chapters/18_close_to_close/close_to_close.html#calculating-weekly-realized-volatility",
    "href": "chapters/18_close_to_close/close_to_close.html#calculating-weekly-realized-volatility",
    "title": "17  Volatility Forecasting: Close-to-Close Estimator",
    "section": "17.5 Calculating Weekly Realized Volatility",
    "text": "17.5 Calculating Weekly Realized Volatility\nNow that we have a week_num associated with each trade_date, we can use .groupby() to calculate the realized volatility.\nThese weekly realized volatilities are the labels that we will be predicting later in our analysis.\n\ndf_realized = \\\n    (\n    df_spy\n        .groupby(['week_num', 'week_start', 'week_end'], as_index = False)[['dly_ret']].agg(lambda x: np.std(x) * np.sqrt(252))\n        .rename(columns = {'dly_ret':'realized_vol'})\n    )\ndf_realized = df_realized[1:]\ndf_realized\n\n\n\n\n\n\n\n\nweek_num\nweek_start\nweek_end\nrealized_vol\n\n\n\n\n1\n1\n2005-01-10\n2005-01-14\n0.093295\n\n\n2\n2\n2005-01-18\n2005-01-21\n0.126557\n\n\n3\n3\n2005-01-24\n2005-01-28\n0.029753\n\n\n4\n4\n2005-01-31\n2005-02-04\n0.069583\n\n\n5\n5\n2005-02-07\n2005-02-11\n0.084567\n\n\n...\n...\n...\n...\n...\n\n\n582\n582\n2016-02-29\n2016-03-04\n0.159055\n\n\n583\n583\n2016-03-07\n2016-03-11\n0.137591\n\n\n584\n584\n2016-03-14\n2016-03-18\n0.057861\n\n\n585\n585\n2016-03-21\n2016-03-24\n0.048135\n\n\n586\n586\n2016-03-28\n2016-04-01\n0.066437\n\n\n\n\n586 rows × 4 columns"
  },
  {
    "objectID": "chapters/18_close_to_close/close_to_close.html#close-to-close-estimator",
    "href": "chapters/18_close_to_close/close_to_close.html#close-to-close-estimator",
    "title": "17  Volatility Forecasting: Close-to-Close Estimator",
    "section": "17.6 Close-to-Close Estimator",
    "text": "17.6 Close-to-Close Estimator\nLet’s now implement the close-to-close estimator.\n\ndef close_to_close(r):\n    T = r.shape[0]\n    r_bar = r.mean()\n    vol = np.sqrt((1 / (T - 1)) * ((r - r_bar) ** 2).sum()) * np.sqrt(252)\n    return(vol)\n\nNotice that close_to_close() is an aggregation function that takes in an array of daily returns and returns back a number. In order to calculate weekly estimates we use close_to_close() as the aggregation function applied to a .groupby().\n\ndf_close_to_close = \\\n    (\n    df_spy\n        .groupby(['week_num', 'week_start', 'week_end'], as_index = False)[['dly_ret']]\n        .agg(close_to_close)\n        .rename(columns = {'dly_ret':'close_to_close'})\n    )\ndf_close_to_close = df_close_to_close[0:-1]\ndf_close_to_close\n\n\n\n\n\n\n\n\nweek_num\nweek_start\nweek_end\nclose_to_close\n\n\n\n\n0\n0\n2005-01-03\n2005-01-07\n0.102492\n\n\n1\n1\n2005-01-10\n2005-01-14\n0.104307\n\n\n2\n2\n2005-01-18\n2005-01-21\n0.146136\n\n\n3\n3\n2005-01-24\n2005-01-28\n0.033265\n\n\n4\n4\n2005-01-31\n2005-02-04\n0.077796\n\n\n...\n...\n...\n...\n...\n\n\n581\n581\n2016-02-22\n2016-02-26\n0.175394\n\n\n582\n582\n2016-02-29\n2016-03-04\n0.177829\n\n\n583\n583\n2016-03-07\n2016-03-11\n0.153831\n\n\n584\n584\n2016-03-14\n2016-03-18\n0.064691\n\n\n585\n585\n2016-03-21\n2016-03-24\n0.055581\n\n\n\n\n586 rows × 4 columns\n\n\n\n\nDiscussion Question: Verify that the .groupby() above works just fine with out including week_start and week_end. If that is the case, then why did I include it?\n\n\nSolution\n(\ndf_spy\n    .groupby(['week_num', 'week_start', 'week_end'], as_index = False)[['dly_ret']]\n    .agg(close_to_close)\n    .rename(columns = {'dly_ret':'close_to_close'})\n)\n\n# It makes the code and the dataframe more readable.\n\n\n\n\n\n\n\n\n\nweek_num\nweek_start\nweek_end\nclose_to_close\n\n\n\n\n0\n0\n2005-01-03\n2005-01-07\n0.102492\n\n\n1\n1\n2005-01-10\n2005-01-14\n0.104307\n\n\n2\n2\n2005-01-18\n2005-01-21\n0.146136\n\n\n3\n3\n2005-01-24\n2005-01-28\n0.033265\n\n\n4\n4\n2005-01-31\n2005-02-04\n0.077796\n\n\n...\n...\n...\n...\n...\n\n\n582\n582\n2016-02-29\n2016-03-04\n0.177829\n\n\n583\n583\n2016-03-07\n2016-03-11\n0.153831\n\n\n584\n584\n2016-03-14\n2016-03-18\n0.064691\n\n\n585\n585\n2016-03-21\n2016-03-24\n0.055581\n\n\n586\n586\n2016-03-28\n2016-04-01\n0.074279\n\n\n\n\n587 rows × 4 columns\n\n\n\n\nCode Challenge: Create an alternative version of our close-to-close function using np.std(). Call the new function close_to_close_std(). Verify that your values match.\n\n\nSolution\ndef close_to_close_std(r):\n    vol = np.std(r, ddof = 1) * np.sqrt(252)\n    return(vol)\n\ndf_std = \\\n    (\n    df_spy\n        .groupby(['week_num', 'week_start', 'week_end'], as_index = False)[['dly_ret']]\n        .agg(close_to_close_std)\n        .rename(columns = {'dly_ret':'close_to_close'})\n    )\n    \ndf_std = df_std[:-1]\nprint(df_std['close_to_close'].sum())\nprint(df_close_to_close['close_to_close'].sum())\n\n\n90.6980642016463\n90.6980642016463\n\n\nIn Sepp 2016, the author uses the \\(R^2\\) between the forecasts and the realized labels as a means of assessing the quality of a particular estimator. Let’s utilize sklearn to do the same.\nWe being by importing the LinearRegression() constructor and instantiating a model.\n\nfrom sklearn.linear_model import LinearRegression\nmdl_reg = LinearRegression(fit_intercept = True)\n\nNext, let’s organize our features and labels.\n\nX = df_close_to_close[['close_to_close']]\ny = df_realized['realized_vol']\n\nWe can now fit the model.\n\nmdl_reg.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nThe .score() method of a LinearRegression model returns the \\(R^2\\).\n\nmdl_reg.score(X, y)\n\n0.4093645253435927\n\n\nAnd we can examine the slope and intercept of our model as follows:\n\nprint(\"Intercept:\", mdl_reg.intercept_)\nprint(\"Slope:   \", mdl_reg.coef_)\n\nIntercept: 0.04933384095053199\nSlope:    [0.57068844]\n\n\n\nDiscussion Question: How do our results compare to Sepp’s?\n\n\nSolution\n# They seem close enough that there probably isn't some error in my calculations.\n# The differences probably come down to differences in data.\n# I do wish the results were a bit closer to feel totally comfortable.\n\n\n\nLet’s also measure the bias and efficiency of the the close-to-close estimator.\n\n# bias\nprint(\"Bias:      \", np.mean(df_close_to_close['close_to_close'] - df_realized['realized_vol']))\n\n# efficiency\nprint(\"Efficiency:\", np.std(df_realized['realized_vol']) / np.std(df_close_to_close['close_to_close']))\n\nBias:       0.01708041424884867\nEfficiency: 0.8919571135609281"
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#importing-packages",
    "href": "chapters/16_trend_following/trend_following.html#importing-packages",
    "title": "16  Trend Following",
    "section": "16.1 Importing Packages",
    "text": "16.1 Importing Packages\nLet’s begin by importing the packages that we will need.\n\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr"
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#setting-parameters",
    "href": "chapters/16_trend_following/trend_following.html#setting-parameters",
    "title": "16  Trend Following",
    "section": "16.2 Setting Parameters",
    "text": "16.2 Setting Parameters\nNext we set the parameters of our analysis. The parameter sma_days determines the number of days used in the trailing simple moving average.\n\nticker = '^SP500TR'\nstart_date = '1989-12-29'\nend_date = '2012-01-01'\nsma_days = 200"
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#reading-in-data",
    "href": "chapters/16_trend_following/trend_following.html#reading-in-data",
    "title": "16  Trend Following",
    "section": "16.3 Reading-In Data",
    "text": "16.3 Reading-In Data\nWe now read-in our data from Yahoo Finance.\n\ndf_asset = pdr.get_data_yahoo(ticker)\ndf_asset.reset_index(inplace=True)\ndf_asset.columns = df_asset.columns.str.lower().str.replace(' ','_')\ndf_asset = df_asset[['date', 'adj_close']].copy()\ndf_asset['sma'] = df_asset['adj_close'].rolling(sma_days).mean()\ndf_asset = df_asset.query('date &gt;= @start_date and date &lt;= @end_date').copy()\ndf_asset.plot(x='date', y=['adj_close', 'sma'], grid=True, title='Adjusted Close and Moving Average Through Time');\n\n[*********************100%***********************]  1 of 1 completed"
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#determinging-position-based-on-trend",
    "href": "chapters/16_trend_following/trend_following.html#determinging-position-based-on-trend",
    "title": "16  Trend Following",
    "section": "16.4 Determinging Position Based on Trend",
    "text": "16.4 Determinging Position Based on Trend\nHere we define the function that will help to determine our daily position in the asset.\n\ndef calc_position(row):\n    adj_close = row['adj_close']\n    sma = row['sma']\n\n    position = 0\n    if sma &lt; adj_close:\n        position = 1\n\n    return position\n\nLet’s now use the DataFrame.apply() method to calculate all the positions through time.\n\ndf_asset['trend_position'] = df_asset.apply(calc_position, axis = 1)\ndf_asset\n\n\n\n\n\n\n\n\ndate\nadj_close\nsma\ntrend_position\n\n\n\n\n504\n1989-12-29\n379.410004\n351.375200\n1\n\n\n505\n1990-01-02\n386.160004\n351.776050\n1\n\n\n506\n1990-01-03\n385.170013\n352.186400\n1\n\n\n507\n1990-01-04\n382.019989\n352.573600\n1\n\n\n508\n1990-01-05\n378.299988\n352.946600\n1\n\n\n...\n...\n...\n...\n...\n\n\n6047\n2011-12-23\n2171.500000\n2141.526601\n1\n\n\n6048\n2011-12-27\n2171.709961\n2141.442350\n1\n\n\n6049\n2011-12-28\n2145.090088\n2141.347501\n1\n\n\n6050\n2011-12-29\n2168.120117\n2141.578401\n1\n\n\n6051\n2011-12-30\n2158.939941\n2141.620952\n1\n\n\n\n\n5548 rows × 4 columns"
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#calculating-returns-equity-curves-and-drawdowns",
    "href": "chapters/16_trend_following/trend_following.html#calculating-returns-equity-curves-and-drawdowns",
    "title": "16  Trend Following",
    "section": "16.5 Calculating Returns, Equity Curves, and Drawdowns",
    "text": "16.5 Calculating Returns, Equity Curves, and Drawdowns\nWe now have all the data the we need to calculate daily returns, the equity curves, and drawdowns of the two strategies.\n\n# returns\ndf_asset['buy_hold_return'] = df_asset['adj_close'].pct_change()\ndf_asset['trend_return'] = df_asset['buy_hold_return'] * df_asset['trend_position'].shift(1)\n\n# growth factors\ndf_asset['buy_hold_factor'] = 1 + df_asset['buy_hold_return']\ndf_asset['trend_factor'] = 1 + df_asset['trend_return']\n\n# equity curves\ndf_asset['buy_hold_equity'] = df_asset['buy_hold_factor'].cumprod()\ndf_asset['trend_equity'] = df_asset['trend_factor'].cumprod()\n\n# maximum cumulative equity\ndf_asset['buy_hold_max_equity'] = df_asset['buy_hold_equity'].cummax()\ndf_asset['trend_max_equity'] = df_asset['trend_equity'].cummax()\n\n# draw-down\ndf_asset['buy_hold_drawdown'] = (df_asset['buy_hold_equity'] - df_asset['buy_hold_max_equity']) / df_asset['buy_hold_max_equity']\ndf_asset['trend_drawdown'] = (df_asset['trend_equity'] - df_asset['trend_max_equity']) / df_asset['trend_max_equity']\n\n# graphing equity curves\ndf_asset.plot(x='date', y=['buy_hold_equity','trend_equity'], grid=True, title='Equity Graph');"
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#return-characteristics",
    "href": "chapters/16_trend_following/trend_following.html#return-characteristics",
    "title": "16  Trend Following",
    "section": "16.6 Return Characteristics",
    "text": "16.6 Return Characteristics\nFinally, we calculate some basic performance metrics.\nAnnualized Return\n\nprint('buy-hold return: ', np.round(df_asset['buy_hold_equity'].iloc[-1] ** (252 / (len(df_asset) - 1)) - 1, 3) * 100, '%')\nprint('trend return:    ', np.round(df_asset['trend_equity'].iloc[-1] ** (252 / (len(df_asset) - 1)) - 1, 3) * 100, '%')\n\nbuy-hold return:  8.200000000000001 %\ntrend return:     7.199999999999999 %\n\n\nSharpe-Ratio\n\nprint('buy-hold sharpe-ratio: ', np.round((np.mean(df_asset['buy_hold_return']) / np.std(df_asset['buy_hold_return'])) * np.sqrt(252), 2))\nprint('trend sharpe-ratio:    ', np.round((np.mean(df_asset['trend_return']) / np.std(df_asset['trend_return'])) * np.sqrt(252), 2))\n\nbuy-hold sharpe-ratio:  0.51\ntrend sharpe-ratio:     0.66\n\n\nMaximum Drawdown\n\nprint('buy-hold max-drawdown: ', np.round(np.min(df_asset['buy_hold_drawdown']), 2))\nprint('trend max-drawdown:    ', np.round(np.min(df_asset['trend_drawdown']), 2))\n\nbuy-hold max-drawdown:  -0.55\ntrend max-drawdown:     -0.24"
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#case-study-sp500-1990-2012",
    "href": "chapters/16_trend_following/trend_following.html#case-study-sp500-1990-2012",
    "title": "16  Trend Following",
    "section": "16.7 Case Study: S&P500 1990-2012",
    "text": "16.7 Case Study: S&P500 1990-2012\nLet’s now take a look at a particular case study of the S&P500 in the period of 1990-2012. We will use the Total Returns futures to represent an investment in the S&P500. In order to get the result we describe, rerun this notebook with the following inputs:\nticker = '^SP500TR'\nstart_date = '1989-12-29'\nend_date = '2012-01-01'\nsma_days = 200\nWe first compare our equity graph above to Faber’s equity graph which we present below:\n\nAs we can see in Faber’s analysis, trend-following slightly outperforms the buy-and-hold strategy, while in our analysis the trend-following strategy underperforms. This difference is likely due to the difference in rebalance frequency. Our strategy rebalances daily (which is probably too much) while Faber’s rebalances monthly.\nAnalyzing our results more closely, we see that buy-and-hold has an annualized return of 8.2%, while trend-following has an annualized return of 7.2%. There is, however, significantly less downside risk with trend-following, as the strategy tends to sit-out bear markets (e.g. 2000 and 2008). This results in a max-drawdown of -24% for trend following, while buy-and-hold had a max-drawdown of -55% during the 2008 financial crisis. This reduced downside risk can also be seen in the Sharpe-ratio, a metric by which trend-following (0.66) outperforms buy-and-hold (0.51)."
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#determining-position-based-on-trend",
    "href": "chapters/16_trend_following/trend_following.html#determining-position-based-on-trend",
    "title": "16  Trend Following",
    "section": "16.4 Determining Position Based on Trend",
    "text": "16.4 Determining Position Based on Trend\nHere we define the function that will help to determine our daily position in the asset.\n\ndef calc_position(row):\n    adj_close = row['adj_close']\n    sma = row['sma']\n\n    position = 0\n    if sma &lt; adj_close:\n        position = 1\n\n    return position\n\nLet’s now use the DataFrame.apply() method to calculate all the positions through time.\n\ndf_asset['trend_position'] = df_asset.apply(calc_position, axis = 1)\ndf_asset\n\n\n\n\n\n\n\n\ndate\nadj_close\nsma\ntrend_position\n\n\n\n\n504\n1989-12-29\n379.410004\n351.375200\n1\n\n\n505\n1990-01-02\n386.160004\n351.776050\n1\n\n\n506\n1990-01-03\n385.170013\n352.186400\n1\n\n\n507\n1990-01-04\n382.019989\n352.573600\n1\n\n\n508\n1990-01-05\n378.299988\n352.946600\n1\n\n\n...\n...\n...\n...\n...\n\n\n6047\n2011-12-23\n2171.500000\n2141.526601\n1\n\n\n6048\n2011-12-27\n2171.709961\n2141.442350\n1\n\n\n6049\n2011-12-28\n2145.090088\n2141.347501\n1\n\n\n6050\n2011-12-29\n2168.120117\n2141.578401\n1\n\n\n6051\n2011-12-30\n2158.939941\n2141.620952\n1\n\n\n\n\n5548 rows × 4 columns"
  }
]