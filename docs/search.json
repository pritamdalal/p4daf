[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python for Data Science in Finance",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#what-is-a-jupyter-notebook",
    "href": "chapters/01_jumpstart/jumpstart.html#what-is-a-jupyter-notebook",
    "title": "1  Python Jumpstart",
    "section": "1.1 What is a Jupyter Notebook?",
    "text": "1.1 What is a Jupyter Notebook?\nThe notebook format conveniently allows you to combine sentences, code, code outputs (including plots), and mathematical notation. Notebooks have proven to be a convenient and productive programming environment for data analysis.\nBehind the scenes of a Jupyter notebook is a kernel that is responsible for executing computations. The kernel can live locally on your machine or on a remote server."
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#ides-for-jupyter-notebooks",
    "href": "chapters/01_jumpstart/jumpstart.html#ides-for-jupyter-notebooks",
    "title": "1  Python Jumpstart",
    "section": "1.2 IDEs for Jupyter Notebooks",
    "text": "1.2 IDEs for Jupyter Notebooks\nYou will need another piece of software called an integrated development environment (IDE) to actually work with Jupyter notebooks; here are three popular and free IDEs for working with them:\n\nJupyterLab - my personal favorite, created by the Jupyter project, which also creates the Jupyter notebook format.\nJupyter Notebook Classic - this was the predecessor to JupyterLab, also created by the Jupyter project.\nVSCode - an general purpose IDE created my Microsoft."
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#code-cells",
    "href": "chapters/01_jumpstart/jumpstart.html#code-cells",
    "title": "1  Python Jumpstart",
    "section": "1.3 Code Cells",
    "text": "1.3 Code Cells\nA notebook is structured as a sequence of cells. There are three kinds of cells: 1) code cells that contain code; 2) markdown cells that contain markdown or latex; and 3) raw cells that contain raw text. We will work mainly with code cells and markdown cells.\nThe cell below is a code cell - try typing the code and then press shift + enter.\n\nfrom IPython.display import Image\nImage(\"not_ethical.png\")"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#edit-mode-vs-command-mode",
    "href": "chapters/01_jumpstart/jumpstart.html#edit-mode-vs-command-mode",
    "title": "1  Python Jumpstart",
    "section": "1.4 Edit Mode vs Command Mode",
    "text": "1.4 Edit Mode vs Command Mode\nThere are two modes in a notebook: 1) edit mode; 2) command mode.\nIn edit mode you are inside a cell and you can edit the contents of the cell.\nIn command mode, you are outside the cells and you can navigate between them."
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#keyboard-shortcuts",
    "href": "chapters/01_jumpstart/jumpstart.html#keyboard-shortcuts",
    "title": "1  Python Jumpstart",
    "section": "1.5 Keyboard Shortcuts",
    "text": "1.5 Keyboard Shortcuts\nHere are some of my favorite JupyterLab keyboard shortcuts:\nedit mode: enter\ncommand mode: esc\nnavigate up: k\nnavigate down: j\ninsert cell above: a\ninsert cell below: b\ndelete cell: d, d (press d twice)\nswitch to code cell: y\nswitch to markup cell: m\nexecute and stay on current cell: ctrl + enter\nexecute and move down a cell: shift + enter"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#drop-down-menus",
    "href": "chapters/01_jumpstart/jumpstart.html#drop-down-menus",
    "title": "1  Python Jumpstart",
    "section": "1.6 Drop Down Menus",
    "text": "1.6 Drop Down Menus\nHere are a few of the drop down menu functions in JupyterLab that I use frequently:\nKernel &gt; Restart Kernel and Clear All Outputs\nKernel &gt; Restart Kearnel and Run All Cells\nRun &gt; Run All Above Selected Cell"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#importing-packages",
    "href": "chapters/01_jumpstart/jumpstart.html#importing-packages",
    "title": "1  Python Jumpstart",
    "section": "1.7 Importing Packages",
    "text": "1.7 Importing Packages\nThe power and convenience of Python as a data analysis language comes from the ecosystem of freely available third party packages.\nHere are the packages that we will be using in this tutorial:\nnumpy - efficient vector and matrix computations\npandas - working with DataFrames\nyfinance - reading in data from Yahoo finance\npandas_datareader - also for reading data from Yahoo Finance\nThe following code imports these packages and assigns them each an alias.\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#reading-in-stock-data-into-a-dataframe",
    "href": "chapters/01_jumpstart/jumpstart.html#reading-in-stock-data-into-a-dataframe",
    "title": "1  Python Jumpstart",
    "section": "1.8 Reading-In Stock Data into a DataFrame",
    "text": "1.8 Reading-In Stock Data into a DataFrame\nLet’s begin by reading in 5 years of SPY price data from Yahoo Finance.\nSPY is an ETF that tracks the performace of the SP500 stock index.\n\ndf_spy = pdr.get_data_yahoo('SPY', start='2014-01-01', end='2019-01-01')\ndf_spy = df_spy.round(2)\ndf_spy.head()\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2014-01-02\n183.98\n184.07\n182.48\n182.92\n153.83\n119636900\n\n\n2014-01-03\n183.23\n183.60\n182.63\n182.89\n153.80\n81390600\n\n\n2014-01-06\n183.49\n183.56\n182.08\n182.36\n153.36\n108028200\n\n\n2014-01-07\n183.09\n183.79\n182.95\n183.48\n154.30\n86144200\n\n\n2014-01-08\n183.45\n183.83\n182.89\n183.52\n154.33\n96582300\n\n\n\n\n\n\n\nOur stock data now lives in the variable called df_spy, which is a pandas data structure known as a DataFrame. We can see this by using the following code:\n\ntype(df_spy)\n\npandas.core.frame.DataFrame"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#dataframe-index",
    "href": "chapters/01_jumpstart/jumpstart.html#dataframe-index",
    "title": "1  Python Jumpstart",
    "section": "1.9 DataFrame Index",
    "text": "1.9 DataFrame Index\nIn pandas, a DataFrame always has an index. For df_spy the Dates form the index.\n\ndf_spy.index\n\nDatetimeIndex(['2014-01-02', '2014-01-03', '2014-01-06', '2014-01-07',\n               '2014-01-08', '2014-01-09', '2014-01-10', '2014-01-13',\n               '2014-01-14', '2014-01-15',\n               ...\n               '2018-12-17', '2018-12-18', '2018-12-19', '2018-12-20',\n               '2018-12-21', '2018-12-24', '2018-12-26', '2018-12-27',\n               '2018-12-28', '2018-12-31'],\n              dtype='datetime64[ns]', name='Date', length=1258, freq=None)\n\n\nI don’t use indices very much, so let’s make the Date index just a regular column. Notice that we can modify DataFrames inplace.\n\ndf_spy.reset_index(inplace=True)\ndf_spy\n\n\n\n\n\n\n\n\nDate\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\n\n\n\n\n0\n2014-01-02\n183.98\n184.07\n182.48\n182.92\n153.83\n119636900\n\n\n1\n2014-01-03\n183.23\n183.60\n182.63\n182.89\n153.80\n81390600\n\n\n2\n2014-01-06\n183.49\n183.56\n182.08\n182.36\n153.36\n108028200\n\n\n3\n2014-01-07\n183.09\n183.79\n182.95\n183.48\n154.30\n86144200\n\n\n4\n2014-01-08\n183.45\n183.83\n182.89\n183.52\n154.33\n96582300\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1253\n2018-12-24\n239.04\n240.84\n234.27\n234.34\n217.60\n147311600\n\n\n1254\n2018-12-26\n235.97\n246.18\n233.76\n246.18\n228.59\n218485400\n\n\n1255\n2018-12-27\n242.57\n248.29\n238.96\n248.07\n230.35\n186267300\n\n\n1256\n2018-12-28\n249.58\n251.40\n246.45\n247.75\n230.05\n153100200\n\n\n1257\n2018-12-31\n249.56\n250.19\n247.47\n249.92\n232.07\n144299400\n\n\n\n\n1258 rows × 7 columns\n\n\n\nNotice that even though we ran the .reset_index() method of df_spy it still has an index; now its index is just a sequence of integers.\n\ndf_spy.index\n\nRangeIndex(start=0, stop=1258, step=1)"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#a-bit-of-cleaning",
    "href": "chapters/01_jumpstart/jumpstart.html#a-bit-of-cleaning",
    "title": "1  Python Jumpstart",
    "section": "1.10 A Bit of Cleaning",
    "text": "1.10 A Bit of Cleaning\nAs a matter of preference, I like my column names to be in snake case.\n\ndf_spy.columns = df_spy.columns.str.lower().str.replace(' ','_')\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n0\n2014-01-02\n183.98\n184.07\n182.48\n182.92\n153.83\n119636900\n\n\n1\n2014-01-03\n183.23\n183.60\n182.63\n182.89\n153.80\n81390600\n\n\n2\n2014-01-06\n183.49\n183.56\n182.08\n182.36\n153.36\n108028200\n\n\n3\n2014-01-07\n183.09\n183.79\n182.95\n183.48\n154.30\n86144200\n\n\n4\n2014-01-08\n183.45\n183.83\n182.89\n183.52\n154.33\n96582300\n\n\n\n\n\n\n\nLet’s also remove the columns that we won’t need. We first create a list of the column names that we want to get rid of and then we use the DataFrame.drop() method.\n\nlst_cols = ['high', 'low', 'open', 'close', 'volume',]\ndf_spy.drop(columns=lst_cols, inplace=True)\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nadj_close\n\n\n\n\n0\n2014-01-02\n153.83\n\n\n1\n2014-01-03\n153.80\n\n\n2\n2014-01-06\n153.36\n\n\n3\n2014-01-07\n154.30\n\n\n4\n2014-01-08\n154.33\n\n\n\n\n\n\n\nNotice that trailing commas do not cause errors in Python."
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#series",
    "href": "chapters/01_jumpstart/jumpstart.html#series",
    "title": "1  Python Jumpstart",
    "section": "1.11 Series",
    "text": "1.11 Series\nYou can isolate the columns of a DataFrame with square brackets as follows:\n\ndf_spy['adj_close']\n\n0       153.83\n1       153.80\n2       153.36\n3       154.30\n4       154.33\n         ...  \n1253    217.60\n1254    228.59\n1255    230.35\n1256    230.05\n1257    232.07\nName: adj_close, Length: 1258, dtype: float64\n\n\nThe columns of a DataFrame are a pandas data structure called a Series.\n\ntype(df_spy['adj_close'])\n\npandas.core.series.Series"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#numpy-and-ndarrays",
    "href": "chapters/01_jumpstart/jumpstart.html#numpy-and-ndarrays",
    "title": "1  Python Jumpstart",
    "section": "1.12 numpy and ndarrays",
    "text": "1.12 numpy and ndarrays\nPython is a general purpose programming language and was not created for scientific computing in particular. One of the foundational packages that makes Python well suited to scientific computing is numpy, which has a variety of features including a data type called ndarrays. One of the benefits of ndarrays is that they allow for efficient vector and matrix computation.\nThe values of a Series object is a numpy.ndarray. This is one sense in which pandas is built on top of numpy.\n\ndf_spy['adj_close'].values\n\narray([153.83, 153.8 , 153.36, ..., 230.35, 230.05, 232.07])\n\n\n\ntype(df_spy['adj_close'].values)\n\nnumpy.ndarray"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#series-built-in-methods",
    "href": "chapters/01_jumpstart/jumpstart.html#series-built-in-methods",
    "title": "1  Python Jumpstart",
    "section": "1.13 Series Built-In Methods",
    "text": "1.13 Series Built-In Methods\nSeries have a variety of built-in methods that provide convenient summarization and modification functionality. For example, you can .sum() all the elements of the Series.\n\ndf_spy['adj_close'].sum()\n\n251297.16\n\n\nNext, we calculate the standard deviation of all the elements of the Series using the .std() method.\n\ndf_spy['adj_close'].std()\n\n33.16746781625381\n\n\nThe .shift() built-in method will be useful for calculating returns in the next section - it has the effect of pushing down the values in a Series.\n\ndf_spy['adj_close'].shift()\n\n0          NaN\n1       153.83\n2       153.80\n3       153.36\n4       154.30\n         ...  \n1253    223.51\n1254    217.60\n1255    228.59\n1256    230.35\n1257    230.05\nName: adj_close, Length: 1258, dtype: float64"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#calculating-daily-returns",
    "href": "chapters/01_jumpstart/jumpstart.html#calculating-daily-returns",
    "title": "1  Python Jumpstart",
    "section": "1.14 Calculating Daily Returns",
    "text": "1.14 Calculating Daily Returns\nOur analysis analysis of the leverage effect will involve daily returns for all the days in df_spy. Let’s calculate those now.\nRecall that the end-of-day day \\(t\\) return of a stock is defined as: \\(r_{t} = \\frac{S_{t}}{S_{t-1}} - 1\\), where \\(S_{t}\\) is the stock price at end-of-day \\(t\\).\nHere is a vectorized approach to calculating all the daily returns in a single line of code.\n\ndf_spy['ret'] = df_spy['adj_close'] / df_spy['adj_close'].shift(1) - 1\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nadj_close\nret\n\n\n\n\n0\n2014-01-02\n153.83\nNaN\n\n\n1\n2014-01-03\n153.80\n-0.000195\n\n\n2\n2014-01-06\n153.36\n-0.002861\n\n\n3\n2014-01-07\n154.30\n0.006129\n\n\n4\n2014-01-08\n154.33\n0.000194\n\n\n\n\n\n\n\nNotice that we can create a new column of a DataFrame by using variable assignment syntax."
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#visualizing-adjusted-close-prices",
    "href": "chapters/01_jumpstart/jumpstart.html#visualizing-adjusted-close-prices",
    "title": "1  Python Jumpstart",
    "section": "1.15 Visualizing Adjusted Close Prices",
    "text": "1.15 Visualizing Adjusted Close Prices\nPython has a variety of packages that can be used for visualization. In this chapter we will focus on built-in plotting capabilities of pandas. These capabilities are built on top of the matplotlib package, which is the foundation of much of Python’s visualization ecosystem.\nDataFrames have a built-in .plot() method that makes creating simple line graphs quite easy.\n\ndf_spy.plot(x='date', y='adj_close');\n\n\n\n\nIf we wanted to make this graph more presentable we could do something like:\n\nax = df_spy.\\\n        plot(\n            x = 'date',\n            y = 'adj_close',\n            title = 'SPY: 2014-2018',\n            grid = True,\n            style = 'k',\n            alpha = 0.75,\n            figsize = (9, 4),\n        );\nax.set_xlabel('Trade Date');\nax.set_ylabel('Close Price');\n\n\n\n\nNotice that the ax variable created above is a matplotlib object.\n\ntype(ax)\n\nmatplotlib.axes._axes.Axes"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#visualizing-returns",
    "href": "chapters/01_jumpstart/jumpstart.html#visualizing-returns",
    "title": "1  Python Jumpstart",
    "section": "1.16 Visualizing Returns",
    "text": "1.16 Visualizing Returns\npandas also gives us the ability to simultaneously plot two different columns of a DataFrame in separate subplots of a single graph. Here is what that code looks like:\n\ndf_spy.plot(x='date', y=['adj_close', 'ret',], subplots=True, style='k', alpha=0.75, figsize=(9, 8), grid=True);\n\n\n\n\nThe returns graph above is a bit of a hack, it doesn’t really make sense to create a line graph of consecutive returns. However, because there are so many days jammed into the x-axis, it creates a desirable effect and it used all the time in finance to demonstrate properties of volatility.\nNotice that whenever there is a sharp drop in the adj_close price graph, that the magnitude of the nearby returns becomes large. In contrast, during periods of steady growth (e.g. all of 2017) the magnitude of the returns is small. This is precisely the leverage effect."
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#calculating-realized-volatility",
    "href": "chapters/01_jumpstart/jumpstart.html#calculating-realized-volatility",
    "title": "1  Python Jumpstart",
    "section": "1.17 Calculating Realized Volatility",
    "text": "1.17 Calculating Realized Volatility\nRealized volatility is defined as the standard deviation of the daily returns; it indicates how much variability in the stock price there has been. It is a matter of convention to annualize this quantity, so we multiply it by \\(\\sqrt{252}\\).\nThe following vectorized code calculates a rolling 2-month volatility for our SPY price data.\n\ndf_spy['ret'].rolling(42).std() * np.sqrt(252)\n\n0            NaN\n1            NaN\n2            NaN\n3            NaN\n4            NaN\n          ...   \n1253    0.226735\n1254    0.252813\n1255    0.249195\n1256    0.246019\n1257    0.247027\nName: ret, Length: 1258, dtype: float64\n\n\nLet’s add these realized volatility calculations todf_spy this with the following code.\n\ndf_spy['realized_vol'] = df_spy['ret'].rolling(42).std() * np.sqrt(252)\ndf_spy\n\n\n\n\n\n\n\n\ndate\nadj_close\nret\nrealized_vol\n\n\n\n\n0\n2014-01-02\n153.83\nNaN\nNaN\n\n\n1\n2014-01-03\n153.80\n-0.000195\nNaN\n\n\n2\n2014-01-06\n153.36\n-0.002861\nNaN\n\n\n3\n2014-01-07\n154.30\n0.006129\nNaN\n\n\n4\n2014-01-08\n154.33\n0.000194\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n1253\n2018-12-24\n217.60\n-0.026442\n0.226735\n\n\n1254\n2018-12-26\n228.59\n0.050506\n0.252813\n\n\n1255\n2018-12-27\n230.35\n0.007699\n0.249195\n\n\n1256\n2018-12-28\n230.05\n-0.001302\n0.246019\n\n\n1257\n2018-12-31\n232.07\n0.008781\n0.247027\n\n\n\n\n1258 rows × 4 columns"
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#visualizing-realized-volatility",
    "href": "chapters/01_jumpstart/jumpstart.html#visualizing-realized-volatility",
    "title": "1  Python Jumpstart",
    "section": "1.18 Visualizing Realized Volatility",
    "text": "1.18 Visualizing Realized Volatility\nWe can easily add realized_vol to our graph with the following code.\n\ndf_spy.plot(x = 'date', \n            y = ['adj_close','ret','realized_vol',], \n            subplots=True, style='k', alpha=0.75, \n            figsize=(9, 12), \n            grid=True);\n\n\n\n\nThis graph is an excellent illustration of the leverage effect. When SPY suffers losses, there is a spike in realized volatility, which is to say that the magnitude of the nearby returns increases."
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#further-reading",
    "href": "chapters/01_jumpstart/jumpstart.html#further-reading",
    "title": "1  Python Jumpstart",
    "section": "1.19 Further Reading",
    "text": "1.19 Further Reading\nPython Data Science Handbook - Jake VanderPlas\nPython for Finance 2e - Yves Hilpisch\nPython for Data Analysis 3e - Wes McKinney"
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#importing-packages",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#importing-packages",
    "title": "2  DataFrame Basics",
    "section": "2.1 Importing Packages",
    "text": "2.1 Importing Packages\nLet’s begin by importing the packages that we will need.\n\nimport pandas as pd\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr\npd.set_option('display.max_rows', 10)"
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#reading-in-data",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#reading-in-data",
    "title": "2  DataFrame Basics",
    "section": "2.2 Reading-In Data",
    "text": "2.2 Reading-In Data\nNext, let’s use pandas_datareader to read-in SPY prices from March 2020. SPY is an ETF that tracks the S&P500 index.\n\ndf_spy = pdr.get_data_yahoo('SPY', start='2020-02-28', end='2020-03-31')\ndf_spy = df_spy.round(2)\ndf_spy.head()\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n\n\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n\n\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n\n\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n\n\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800\n\n\n\n\n\n\n\nLet’s also make the Date a regular column, instead of an index, and also make the column names snake-case.\n\ndf_spy.reset_index(drop=False, inplace=True)\ndf_spy.columns = df_spy.columns.str.lower().str.replace(' ', '_')\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n0\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n\n\n1\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n\n\n2\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n\n\n3\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n\n\n4\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800"
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#exploring-a-dataframe",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#exploring-a-dataframe",
    "title": "2  DataFrame Basics",
    "section": "2.3 Exploring a DataFrame",
    "text": "2.3 Exploring a DataFrame\nWe can explore our df_spy DataFrame in a variety of ways.\nFirst, we can first use the type() method to make sure what we have created is in fact a DataFrame.\n\ntype(df_spy)\n\npandas.core.frame.DataFrame\n\n\nNext, we can use the .dtypes attribute of the DataFrame to see the data types of each of the columns.\n\ndf_spy.dtypes\n\ndate         datetime64[ns]\nopen                float64\nhigh                float64\nlow                 float64\nclose               float64\nadj_close           float64\nvolume                int64\ndtype: object\n\n\nWe can also check the number of rows and columns by using the .shape attribute.\n\ndf_spy.shape\n\n(22, 7)\n\n\nAs we can see, our DataFrame df_spy consists of 22 rows and 7 columns.\n\nCode Challenge: Try the DataFrame.info() and DataFrame.describe() methods on df_spy.\n\n\nSolution\ndf_spy.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 22 entries, 0 to 21\nData columns (total 7 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   date       22 non-null     datetime64[ns]\n 1   open       22 non-null     float64       \n 2   high       22 non-null     float64       \n 3   low        22 non-null     float64       \n 4   close      22 non-null     float64       \n 5   adj_close  22 non-null     float64       \n 6   volume     22 non-null     int64         \ndtypes: datetime64[ns](1), float64(5), int64(1)\nmemory usage: 1.3 KB\n\n\n\n\nSolution\ndf_spy.describe().round(2)\n\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\ncount\n22\n22.00\n22.00\n22.00\n22.00\n22.00\n2.200000e+01\n\n\nmean\n2020-03-14 12:00:00\n265.03\n272.89\n258.81\n266.54\n252.62\n2.780051e+08\n\n\nmin\n2020-02-28 00:00:00\n228.19\n229.68\n218.26\n222.95\n212.18\n1.713695e+08\n\n\n25%\n2020-03-06 18:00:00\n243.12\n256.22\n237.14\n244.06\n232.24\n2.362968e+08\n\n\n50%\n2020-03-14 12:00:00\n255.85\n264.73\n250.05\n261.42\n248.80\n2.828830e+08\n\n\n75%\n2020-03-22 06:00:00\n287.68\n295.55\n282.53\n294.30\n278.46\n3.218732e+08\n\n\nmax\n2020-03-30 00:00:00\n309.50\n313.84\n303.33\n312.86\n296.01\n3.922207e+08\n\n\nstd\nNaN\n26.43\n25.44\n26.98\n27.55\n25.74\n6.134551e+07"
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#dataframe-columns",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#dataframe-columns",
    "title": "2  DataFrame Basics",
    "section": "2.4 DataFrame Columns",
    "text": "2.4 DataFrame Columns\nIn order to isolate a particular column of a DataFrame we can use square brackets ([ ]). The following code isolates the close price column of df_spy.\n\ndf_spy['close']\n\n0     296.26\n1     309.09\n2     300.24\n3     312.86\n4     302.46\n       ...  \n17    243.15\n18    246.79\n19    261.20\n20    253.42\n21    261.65\nName: close, Length: 22, dtype: float64\n\n\n\nCode Challenge: Isolate the date column of df_spy.\n\n\nSolution\ndf_spy['date']\n\n\n0    2020-02-28\n1    2020-03-02\n2    2020-03-03\n3    2020-03-04\n4    2020-03-05\n        ...    \n17   2020-03-24\n18   2020-03-25\n19   2020-03-26\n20   2020-03-27\n21   2020-03-30\nName: date, Length: 22, dtype: datetime64[ns]\n\n\n\nAs we can see from the following code, each column of a DataFrame is actually a different kind of pandas structure called a Series.\n\ntype(df_spy['close'])\n\npandas.core.series.Series\n\n\nHere is a bit of pandas inside baseball:\n\nA DataFrame is collection of columns that are glued together.\nEach column is a Series.\nA Series has two main attributes: 1) .values; 2) .index.\nThe .values component of a Series is a numpy.array.\n\nLet’s look at the .values attribute of the close column of df_spy.\n\ndf_spy['close'].values\n\narray([296.26, 309.09, 300.24, 312.86, 302.46, 297.46, 274.23, 288.42,\n       274.36, 248.11, 269.32, 239.85, 252.8 , 240.  , 240.51, 228.8 ,\n       222.95, 243.15, 246.79, 261.2 , 253.42, 261.65])\n\n\n\nCode Challenge: Verify that the values component of the close column of df_spy is in fact a a numpy.array.\n\n\nSolution\ntype(df_spy['close'].values)\n\n\nnumpy.ndarray"
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#component-wise-column-operations",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#component-wise-column-operations",
    "title": "2  DataFrame Basics",
    "section": "2.5 Component-wise Column Operations",
    "text": "2.5 Component-wise Column Operations\nWe can perform component-wise (i.e. vector-like) calculations with DataFrame columns.\nThe following code divides all the close prices by 100.\n\ndf_spy['close'] / 100\n\n0     2.9626\n1     3.0909\n2     3.0024\n3     3.1286\n4     3.0246\n       ...  \n17    2.4315\n18    2.4679\n19    2.6120\n20    2.5342\n21    2.6165\nName: close, Length: 22, dtype: float64\n\n\nWe can also perform component-wise calculations between two colums.\nLet’s say we want to calculate the intraday range of SPY for each of the trade-dates in df_spy; this is the difference between the high and the low of each day. We can do this easily from the columns of our DataFrame.\n\ndf_spy['high'] - df_spy['low']\n\n0     12.35\n1     14.70\n2     16.27\n3      9.77\n4      8.46\n      ...  \n17    10.30\n18    16.60\n19    13.75\n20     9.76\n21     8.90\nLength: 22, dtype: float64\n\n\n\nCode Challenge: Calculate the difference between the close and open columns of df_spy.\n\n\nSolution\ndf_spy['close'] - df_spy['open']\n\n\n0      7.56\n1     10.88\n2     -9.26\n3      6.74\n4     -2.52\n      ...  \n17     8.73\n18     1.92\n19    11.68\n20     0.15\n21     5.95\nLength: 22, dtype: float64"
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#adding-columns-via-variable-assignment",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#adding-columns-via-variable-assignment",
    "title": "2  DataFrame Basics",
    "section": "2.6 Adding Columns via Variable Assignment",
    "text": "2.6 Adding Columns via Variable Assignment\nLet’s say we want to save our intraday ranges back into df_spy for further analysis later. The most straightforward way to do this is using variable assignment as follows.\n\ndf_spy['intraday_range'] = df_spy['high'] - df_spy['low']\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\nintraday_range\n\n\n\n\n0\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n12.35\n\n\n1\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n14.70\n\n\n2\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n16.27\n\n\n3\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n9.77\n\n\n4\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800\n8.46\n\n\n\n\n\n\n\n\nCode Challenge: Add a new column to df_spy called open_to_close that consists of the difference between the close and open of each day.\n\n\nSolution\ndf_spy['open_to_close'] = df_spy['close'] - df_spy['open']\ndf_spy.head()\n\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\nintraday_range\nopen_to_close\n\n\n\n\n0\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n12.35\n7.56\n\n\n1\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n14.70\n10.88\n\n\n2\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n16.27\n-9.26\n\n\n3\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n9.77\n6.74\n\n\n4\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800\n8.46\n-2.52"
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#adding-columns-via-.assign",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#adding-columns-via-.assign",
    "title": "2  DataFrame Basics",
    "section": "2.7 Adding Columns via .assign()",
    "text": "2.7 Adding Columns via .assign()\nA powerful but less intuitive way of adding a column to a DataFrame uses the .assign() function, which makes use of lambda functions (i.e. anonymous functions).\nThe following code adds another column called intraday_range_assign.\n\ndf_spy.assign(intraday_range_assign = lambda df: df['high'] - df['low'])\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\nintraday_range\nopen_to_close\nintraday_range_assign\n\n\n\n\n0\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n12.35\n7.56\n12.35\n\n\n1\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n14.70\n10.88\n14.70\n\n\n2\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n16.27\n-9.26\n16.27\n\n\n3\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n9.77\n6.74\n9.77\n\n\n4\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800\n8.46\n-2.52\n8.46\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17\n2020-03-24\n234.42\n244.10\n233.80\n243.15\n231.41\n235494500\n10.30\n8.73\n10.30\n\n\n18\n2020-03-25\n244.87\n256.35\n239.75\n246.79\n234.87\n299430300\n16.60\n1.92\n16.60\n\n\n19\n2020-03-26\n249.52\n262.80\n249.05\n261.20\n248.59\n257632800\n13.75\n11.68\n13.75\n\n\n20\n2020-03-27\n253.27\n260.81\n251.05\n253.42\n241.18\n224341200\n9.76\n0.15\n9.76\n\n\n21\n2020-03-30\n255.70\n262.43\n253.53\n261.65\n249.02\n171369500\n8.90\n5.95\n8.90\n\n\n\n\n22 rows × 10 columns\n\n\n\n\nCode Challenge: Verify that the column intraday_range_assign was not actually added to the df_spy.\n\n\nSolution\ndf_spy.head()\n\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\nintraday_range\nopen_to_close\n\n\n\n\n0\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n12.35\n7.56\n\n\n1\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n14.70\n10.88\n\n\n2\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n16.27\n-9.26\n\n\n3\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n9.77\n6.74\n\n\n4\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800\n8.46\n-2.52\n\n\n\n\n\n\n\n\nIn order to add the intraday_range_assign column to df_spy we will need to reassign to it.\n\ndf_spy = df_spy.assign(intraday_range_assign = lambda df: df['high'] - df['low'])\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\nintraday_range\nopen_to_close\nintraday_range_assign\n\n\n\n\n0\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n12.35\n7.56\n12.35\n\n\n1\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n14.70\n10.88\n14.70\n\n\n2\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n16.27\n-9.26\n16.27\n\n\n3\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n9.77\n6.74\n9.77\n\n\n4\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800\n8.46\n-2.52\n8.46\n\n\n\n\n\n\n\n\nCode Challenge: Use .assign() to create a new column in df_spy, call it open_to_close_assign, that contains the difference between the close and open.\n\n\nSolution\ndf_spy = df_spy.assign(open_to_close_assign = lambda df: df['close'] - df['open'])\ndf_spy.head()\n\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\nintraday_range\nopen_to_close\nintraday_range_assign\nopen_to_close_assign\n\n\n\n\n0\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n12.35\n7.56\n12.35\n7.56\n\n\n1\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n14.70\n10.88\n14.70\n10.88\n\n\n2\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n16.27\n-9.26\n16.27\n-9.26\n\n\n3\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n9.77\n6.74\n9.77\n6.74\n\n\n4\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800\n8.46\n-2.52\n8.46\n-2.52"
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#method-chaining",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#method-chaining",
    "title": "2  DataFrame Basics",
    "section": "2.8 Method Chaining",
    "text": "2.8 Method Chaining\nThe value of .assign() becomes clear when we start chaining methods together.\nIn order to see this let’s first drop the columns that we created.\n\nlst_cols = ['intraday_range', 'open_to_close', 'intraday_range_assign', 'open_to_close_assign']\ndf_spy.drop(columns=lst_cols, inplace=True)\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n0\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n\n\n1\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n\n\n2\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n\n\n3\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n\n\n4\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800\n\n\n\n\n\n\n\nThe following code adds the intraday and and open_to_close columns at the same time.\n\ndf_spy = \\\n    (\n    df_spy\n        .assign(intraday_range = lambda df: df['high'] - df['low'])\n        .assign(open_to_close = lambda df: df['close'] - df['open'])\n    )\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\nintraday_range\nopen_to_close\n\n\n\n\n0\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n12.35\n7.56\n\n\n1\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n14.70\n10.88\n\n\n2\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n16.27\n-9.26\n\n\n3\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n9.77\n6.74\n\n\n4\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800\n8.46\n-2.52\n\n\n\n\n\n\n\n\nCode Challenge: Use .assign() to add a two new column to df_spy:\n\nthe difference between the close and adj_close\nthe average of the low and open\n\n\n\nSolution\ndf_spy = \\\n    (\n    df_spy\n        .assign(div = lambda df: df['close'] - df['adj_close'])\n        .assign(avg = lambda df: (df['low'] + df['open']) / 2)\n    )\ndf_spy.head()\n\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\nintraday_range\nopen_to_close\ndiv\navg\n\n\n\n\n0\n2020-02-28\n288.70\n297.89\n285.54\n296.26\n280.31\n384975800\n12.35\n7.56\n15.95\n287.120\n\n\n1\n2020-03-02\n298.21\n309.16\n294.46\n309.09\n292.45\n238703600\n14.70\n10.88\n16.64\n296.335\n\n\n2\n2020-03-03\n309.50\n313.84\n297.57\n300.24\n284.07\n300139100\n16.27\n-9.26\n16.17\n303.535\n\n\n3\n2020-03-04\n306.12\n313.10\n303.33\n312.86\n296.01\n176613400\n9.77\n6.74\n16.85\n304.725\n\n\n4\n2020-03-05\n304.98\n308.47\n300.01\n302.46\n286.17\n186366800\n8.46\n-2.52\n16.29\n302.495"
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#aggregating-calulations-on-series",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#aggregating-calulations-on-series",
    "title": "2  DataFrame Basics",
    "section": "2.9 Aggregating Calulations on Series",
    "text": "2.9 Aggregating Calulations on Series\nSeries have a variety of built-in aggregation functions.\nFor example, we can use the following code to calculate the total SPY volume during March 2020.\n\ndf_spy['volume'].sum()\n\n6116112300\n\n\nHere are some summary statistics on the intraday_range column that we added to our DataFrame earlier.\n\nprint(\"Mean:  \", df_spy['intraday_range'].mean()) # average\nprint(\"St Dev: \", df_spy['intraday_range'].std()) # standard deviation\nprint(\"Min:    \" , df_spy['intraday_range'].min()) # minimum\nprint(\"Max:   \" , df_spy['intraday_range'].max()) # maximum\n\nMean:   14.077727272727275\nSt Dev:  4.28352428533215\nMin:     8.460000000000036\nMax:    22.960000000000008\n\n\n\nCode Challenge: Calculate the average daily volume for the trade dates in df_spy.\n\n\nSolution\ndf_spy['volume'].mean()\n\n\n278005104.54545456"
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#related-reading",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#related-reading",
    "title": "2  DataFrame Basics",
    "section": "2.10 Related Reading",
    "text": "2.10 Related Reading\nPython Data Science Handbook - Section 3.1 - Introducing Pandas Objects\nPython Data Science Handbook - Section 2.1 - Understanding Data Types in Python\nPython Data Science Handbook - Section 2.2 - The Basics of NumPy Arrays\nPython Data Science Handbook - Section 2.3 - Computation on NumPy Arrays: Universal Functions\nPython Data Science Handbook - Section 2.4 - Aggregations: Min, Max, and Everything In Between"
  },
  {
    "objectID": "chapters/03_index_slice/index_slice.html#importing-packages",
    "href": "chapters/03_index_slice/index_slice.html#importing-packages",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "3.1 Importing Packages",
    "text": "3.1 Importing Packages\nLet’s begin by importing the packages that we will need.\n\nimport pandas as pd\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr\npd.set_option('display.max_rows', 10)"
  },
  {
    "objectID": "chapters/03_index_slice/index_slice.html#reading-in-data",
    "href": "chapters/03_index_slice/index_slice.html#reading-in-data",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "3.2 Reading-In Data",
    "text": "3.2 Reading-In Data\nNext, lets grab some data from Yahoo finance. In particular, we’ll grab SPY price data from July 2021.\n\ndf_spy = pdr.get_data_yahoo('SPY', start='2021-06-30', end='2021-07-31')\ndf_spy = df_spy.round(2)\ndf_spy.head()\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2021-06-30\n427.21\n428.78\n427.18\n428.06\n415.28\n64827900\n\n\n2021-07-01\n428.87\n430.60\n428.80\n430.43\n417.58\n53441000\n\n\n2021-07-02\n431.67\n434.10\n430.52\n433.72\n420.77\n57697700\n\n\n2021-07-06\n433.78\n434.01\n430.01\n432.93\n420.00\n68710400\n\n\n2021-07-07\n433.66\n434.76\n431.51\n434.46\n421.49\n63549500\n\n\n\n\n\n\n\nThe following code resets the index so that Date is a regular column; it also puts the column names into snake-case.\n\ndf_spy.reset_index(inplace=True)\ndf_spy.columns = df_spy.columns.str.lower().str.replace(' ', '_')\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n0\n2021-06-30\n427.21\n428.78\n427.18\n428.06\n415.28\n64827900\n\n\n1\n2021-07-01\n428.87\n430.60\n428.80\n430.43\n417.58\n53441000\n\n\n2\n2021-07-02\n431.67\n434.10\n430.52\n433.72\n420.77\n57697700\n\n\n3\n2021-07-06\n433.78\n434.01\n430.01\n432.93\n420.00\n68710400\n\n\n4\n2021-07-07\n433.66\n434.76\n431.51\n434.46\n421.49\n63549500\n\n\n\n\n\n\n\nIt is often useful to look at the data type of each of the columns of a new data set. We can do so with the DataFrame.dtypes attribute.\n\ndf_spy.dtypes\n\ndate         datetime64[ns]\nopen                float64\nhigh                float64\nlow                 float64\nclose               float64\nadj_close           float64\nvolume                int64\ndtype: object"
  },
  {
    "objectID": "chapters/03_index_slice/index_slice.html#row-slicing",
    "href": "chapters/03_index_slice/index_slice.html#row-slicing",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "3.3 Row Slicing",
    "text": "3.3 Row Slicing\nThe simplest way to slice a DataFrame is to use square brackets: []. The syntax df[i:j] will generate a DataFrame who’s first row is the ith row of df and who’s last row is the (j-1)th row of df. Let’s demonstrate this with a some examples:\nStarting from the 0th row, and ending with the 0th row:\n\ndf_spy[0:1]\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n0\n2021-06-30\n427.21\n428.78\n427.18\n428.06\n415.28\n64827900\n\n\n\n\n\n\n\nStarting with the 3rd row, and ending with the 6th row:\n\ndf_spy[3:7]\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n3\n2021-07-06\n433.78\n434.01\n430.01\n432.93\n420.00\n68710400\n\n\n4\n2021-07-07\n433.66\n434.76\n431.51\n434.46\n421.49\n63549500\n\n\n5\n2021-07-08\n428.78\n431.73\n427.52\n430.92\n418.05\n97595200\n\n\n6\n2021-07-09\n432.53\n435.84\n430.71\n435.52\n422.51\n76238600\n\n\n\n\n\n\n\n\nCode Challenge: Retrieve the 15th, 16th, and 17th rows of df_spy.\n\n\nSolution\ndf_spy[15:18]\n\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n15\n2021-07-22\n434.74\n435.72\n433.69\n435.46\n422.46\n47878500\n\n\n16\n2021-07-23\n437.52\n440.30\n436.79\n439.94\n426.80\n63766600\n\n\n17\n2021-07-26\n439.31\n441.03\n439.26\n441.02\n427.85\n43719200\n\n\n\n\n\n\n\n\nUsing the syntax df[:n] automatically starts the indexing at 0. For example, the following code retrieves all of df_spy (notice that len(df_spy) gives the number of rows of df_spy):\n\ndf_spy[:len(df_spy)]\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n0\n2021-06-30\n427.21\n428.78\n427.18\n428.06\n415.28\n64827900\n\n\n1\n2021-07-01\n428.87\n430.60\n428.80\n430.43\n417.58\n53441000\n\n\n2\n2021-07-02\n431.67\n434.10\n430.52\n433.72\n420.77\n57697700\n\n\n3\n2021-07-06\n433.78\n434.01\n430.01\n432.93\n420.00\n68710400\n\n\n4\n2021-07-07\n433.66\n434.76\n431.51\n434.46\n421.49\n63549500\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17\n2021-07-26\n439.31\n441.03\n439.26\n441.02\n427.85\n43719200\n\n\n18\n2021-07-27\n439.91\n439.94\n435.99\n439.01\n425.90\n67397100\n\n\n19\n2021-07-28\n439.68\n440.30\n437.31\n438.83\n425.73\n52472400\n\n\n20\n2021-07-29\n439.82\n441.80\n439.81\n440.65\n427.49\n47435300\n\n\n21\n2021-07-30\n437.91\n440.06\n437.77\n438.51\n425.42\n68951200\n\n\n\n\n22 rows × 7 columns\n\n\n\n\nCode Challenge: Retrieve the first five rows of df_spy.\n\n\nSolution\ndf_spy[:5]\n\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n0\n2021-06-30\n427.21\n428.78\n427.18\n428.06\n415.28\n64827900\n\n\n1\n2021-07-01\n428.87\n430.60\n428.80\n430.43\n417.58\n53441000\n\n\n2\n2021-07-02\n431.67\n434.10\n430.52\n433.72\n420.77\n57697700\n\n\n3\n2021-07-06\n433.78\n434.01\n430.01\n432.93\n420.00\n68710400\n\n\n4\n2021-07-07\n433.66\n434.76\n431.51\n434.46\n421.49\n63549500\n\n\n\n\n\n\n\n\nThere are a couple of row slicing tricks that involve negative numbers that are worth mentioning.\nThe syntax df[-n:] retrieves the last n rows of df. The following code retrieves the last five rows of df_spy.\n\ndf_spy[-5:]\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n17\n2021-07-26\n439.31\n441.03\n439.26\n441.02\n427.85\n43719200\n\n\n18\n2021-07-27\n439.91\n439.94\n435.99\n439.01\n425.90\n67397100\n\n\n19\n2021-07-28\n439.68\n440.30\n437.31\n438.83\n425.73\n52472400\n\n\n20\n2021-07-29\n439.82\n441.80\n439.81\n440.65\n427.49\n47435300\n\n\n21\n2021-07-30\n437.91\n440.06\n437.77\n438.51\n425.42\n68951200\n\n\n\n\n\n\n\nThe syntax df[:-n] retrieves all but the last n rows of df. The following code retrieves all but the last 10 rows of df_spy:\n\ndf_spy[:-10]\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n0\n2021-06-30\n427.21\n428.78\n427.18\n428.06\n415.28\n64827900\n\n\n1\n2021-07-01\n428.87\n430.60\n428.80\n430.43\n417.58\n53441000\n\n\n2\n2021-07-02\n431.67\n434.10\n430.52\n433.72\n420.77\n57697700\n\n\n3\n2021-07-06\n433.78\n434.01\n430.01\n432.93\n420.00\n68710400\n\n\n4\n2021-07-07\n433.66\n434.76\n431.51\n434.46\n421.49\n63549500\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7\n2021-07-12\n435.43\n437.35\n434.97\n437.08\n424.03\n52889600\n\n\n8\n2021-07-13\n436.24\n437.84\n435.31\n435.59\n422.58\n52911300\n\n\n9\n2021-07-14\n437.40\n437.92\n434.91\n436.24\n423.21\n64130400\n\n\n10\n2021-07-15\n434.81\n435.53\n432.72\n434.75\n421.77\n55126400\n\n\n11\n2021-07-16\n436.01\n436.06\n430.92\n431.34\n418.46\n75874700\n\n\n\n\n12 rows × 7 columns\n\n\n\n\nCode Challenge: Retrieve the first row of df_spy with negative indexing.\n\n\nSolution\ndf_spy[:-(len(df_spy)-1)]\n\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n0\n2021-06-30\n427.21\n428.78\n427.18\n428.06\n415.28\n64827900\n\n\n\n\n\n\n\n\nCode Challenge: Use simple slicing to select the last three rows of a df_spy: 1) without explicitly using row numbers; 2) with explicitly using row numbers.\n\n\nSolution\ndf_spy[len(df_spy)-3:len(df_spy)]\n\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n19\n2021-07-28\n439.68\n440.30\n437.31\n438.83\n425.73\n52472400\n\n\n20\n2021-07-29\n439.82\n441.80\n439.81\n440.65\n427.49\n47435300\n\n\n21\n2021-07-30\n437.91\n440.06\n437.77\n438.51\n425.42\n68951200\n\n\n\n\n\n\n\n\n\nSolution\ndf_spy[-3:]\n\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n19\n2021-07-28\n439.68\n440.30\n437.31\n438.83\n425.73\n52472400\n\n\n20\n2021-07-29\n439.82\n441.80\n439.81\n440.65\n427.49\n47435300\n\n\n21\n2021-07-30\n437.91\n440.06\n437.77\n438.51\n425.42\n68951200"
  },
  {
    "objectID": "chapters/03_index_slice/index_slice.html#dataframe-indexes",
    "href": "chapters/03_index_slice/index_slice.html#dataframe-indexes",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "3.4 DataFrame Indexes",
    "text": "3.4 DataFrame Indexes\nUnder the hood, a DataFrame has several indexes:\ncolumns - the set of column names is an (explicit) index.\nrow - whenever a DataFrame is created, there is an explicit row index that is created. If one isn’t specified, then a sequence of non-negative integers is used.\nimplicit - each row has an implicit row-number, and each column has an implicit column-number.\nLet’s take a look at the columns index of df_spy.\n\ndf_spy.columns\n\nIndex(['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume'], dtype='object')\n\n\n\ntype(df_spy.columns)\n\npandas.core.indexes.base.Index\n\n\nNext, let’s take a look at the explicit row index attribute of df_spy.\n\ndf_spy.index\n\nRangeIndex(start=0, stop=22, step=1)\n\n\n\ntype(df_spy.index)\n\npandas.core.indexes.range.RangeIndex\n\n\nSince we reset the index for df_spy, a RangeIndex object is used for the explicit row index. You can think of a RangeIndex object as a glorified set of consecutive integers.\nFor the most part, we won’t be too concerned with indexes. A lot of data analysis can be done without worrying about them. However, it’s good to be aware indexes exist becase they can come into play for more advanced topics, such as joining tables together; they also come up in Stack Overflow examples frequently.\nFor the purposes of this chapter, our interest in indexes comes from how they are related to two built-in DataFrame indexers: DataFrame.iloc and DataFrame.loc."
  },
  {
    "objectID": "chapters/03_index_slice/index_slice.html#indexing-with-dataframe.iloc",
    "href": "chapters/03_index_slice/index_slice.html#indexing-with-dataframe.iloc",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "3.5 Indexing with DataFrame.iloc",
    "text": "3.5 Indexing with DataFrame.iloc\nThe indexer DataFrame.iloc can be used to access rows and columns using their implicit row and column numbers.\nHere is an example of iloc that retrieves the first two rows of df_spy.\n\ndf_spy.iloc[0:2,]\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n0\n2021-06-30\n427.21\n428.78\n427.18\n428.06\n415.28\n64827900\n\n\n1\n2021-07-01\n428.87\n430.60\n428.80\n430.43\n417.58\n53441000\n\n\n\n\n\n\n\nNotice, that because we didn’t specify any column numbers, the code above retrieves all columns.\nThe following code grabs the first three row and the first three columns of df_spy.\n\ndf_spy.iloc[0:3, 0:3]\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\n\n\n\n\n0\n2021-06-30\n427.21\n428.78\n\n\n1\n2021-07-01\n428.87\n430.60\n\n\n2\n2021-07-02\n431.67\n434.10\n\n\n\n\n\n\n\nWe can also supply .iloc with lists rather than ranges to specify custom sets of columns and rows:\n\nlst_row = [0, 2] # 0th and 2nd row\nlst_col = [0, 6] # date and adj_close columns\ndf_spy.iloc[lst_row, lst_col]\n\n\n\n\n\n\n\n\ndate\nvolume\n\n\n\n\n0\n2021-06-30\n64827900\n\n\n2\n2021-07-02\n57697700\n\n\n\n\n\n\n\nUsing lists as a means of indexing is sometimes referred to as fancy indexing.\n\nCode Challenge Use fancy indexing to grab the 14th, 0th, and 5th rows of df_spy - in that order.\n\n\nSolution\ndf_spy.iloc[[14, 0, 5]]\n\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n14\n2021-07-21\n432.34\n434.70\n431.01\n434.55\n421.57\n64724400\n\n\n0\n2021-06-30\n427.21\n428.78\n427.18\n428.06\n415.28\n64827900\n\n\n5\n2021-07-08\n428.78\n431.73\n427.52\n430.92\n418.05\n97595200"
  },
  {
    "objectID": "chapters/03_index_slice/index_slice.html#indexing-with-dataframe.loc",
    "href": "chapters/03_index_slice/index_slice.html#indexing-with-dataframe.loc",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "3.6 Indexing with DataFrame.loc",
    "text": "3.6 Indexing with DataFrame.loc\nRather than using the implicit row or column numbers, it is often more useful to access data by using the explicit row or column indices.\nLet’s use the DataFrame.set_index() method to set the date column as our new index. The dates will be a more interesting explicit index.\n\ndf_spy.set_index('date', inplace = True)\ndf_spy.head()\n\n\n\n\n\n\n\n\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2021-06-30\n427.21\n428.78\n427.18\n428.06\n415.28\n64827900\n\n\n2021-07-01\n428.87\n430.60\n428.80\n430.43\n417.58\n53441000\n\n\n2021-07-02\n431.67\n434.10\n430.52\n433.72\n420.77\n57697700\n\n\n2021-07-06\n433.78\n434.01\n430.01\n432.93\n420.00\n68710400\n\n\n2021-07-07\n433.66\n434.76\n431.51\n434.46\n421.49\n63549500\n\n\n\n\n\n\n\nTo see the effect of the above code, we can have a look at the index of df_spy.\n\ndf_spy.index\n\nDatetimeIndex(['2021-06-30', '2021-07-01', '2021-07-02', '2021-07-06',\n               '2021-07-07', '2021-07-08', '2021-07-09', '2021-07-12',\n               '2021-07-13', '2021-07-14', '2021-07-15', '2021-07-16',\n               '2021-07-19', '2021-07-20', '2021-07-21', '2021-07-22',\n               '2021-07-23', '2021-07-26', '2021-07-27', '2021-07-28',\n               '2021-07-29', '2021-07-30'],\n              dtype='datetime64[ns]', name='date', freq=None)\n\n\nAnd notice that date is no longer column of df_spy.\n\ndf_spy.columns\n\nIndex(['open', 'high', 'low', 'close', 'adj_close', 'volume'], dtype='object')\n\n\nNow that we have successfully set the row index of df_spy to be the date, let’s see how we can use this index to access the data via .loc.\nHere is an example of how we can grab a slice of rows, associated with a date-range.\n\ndf_spy.loc['2021-07-23':'2021-07-31']\n\n\n\n\n\n\n\n\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2021-07-23\n437.52\n440.30\n436.79\n439.94\n426.80\n63766600\n\n\n2021-07-26\n439.31\n441.03\n439.26\n441.02\n427.85\n43719200\n\n\n2021-07-27\n439.91\n439.94\n435.99\n439.01\n425.90\n67397100\n\n\n2021-07-28\n439.68\n440.30\n437.31\n438.83\n425.73\n52472400\n\n\n2021-07-29\n439.82\n441.80\n439.81\n440.65\n427.49\n47435300\n\n\n2021-07-30\n437.91\n440.06\n437.77\n438.51\n425.42\n68951200\n\n\n\n\n\n\n\nIf we want to select only the volume and adjusted columns for these dates, we would type the following:\n\ndf_spy.loc['2021-07-23':'2021-07-31', ['volume', 'adj_close']]\n\n\n\n\n\n\n\n\nvolume\nadj_close\n\n\ndate\n\n\n\n\n\n\n2021-07-23\n63766600\n426.80\n\n\n2021-07-26\n43719200\n427.85\n\n\n2021-07-27\n67397100\n425.90\n\n\n2021-07-28\n52472400\n425.73\n\n\n2021-07-29\n47435300\n427.49\n\n\n2021-07-30\n68951200\n425.42\n\n\n\n\n\n\n\n\nCode Challenge: Use .loc to grab the date, volume, and close columns from df_spy.\n\n\nSolution\ndf_spy.loc[:,['volume', 'close']]\n\n\n\n\n\n\n\n\n\nvolume\nclose\n\n\ndate\n\n\n\n\n\n\n2021-06-30\n64827900\n428.06\n\n\n2021-07-01\n53441000\n430.43\n\n\n2021-07-02\n57697700\n433.72\n\n\n2021-07-06\n68710400\n432.93\n\n\n2021-07-07\n63549500\n434.46\n\n\n...\n...\n...\n\n\n2021-07-26\n43719200\n441.02\n\n\n2021-07-27\n67397100\n439.01\n\n\n2021-07-28\n52472400\n438.83\n\n\n2021-07-29\n47435300\n440.65\n\n\n2021-07-30\n68951200\n438.51\n\n\n\n\n22 rows × 2 columns"
  },
  {
    "objectID": "chapters/03_index_slice/index_slice.html#related-reading",
    "href": "chapters/03_index_slice/index_slice.html#related-reading",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "3.7 Related Reading",
    "text": "3.7 Related Reading\nPython Data Science Handbook - Section 2.7 - Fancy Indexing\nPython Data Science Handbook - Section 3.2 - Data Indexing and Selection"
  },
  {
    "objectID": "chapters/04_query/query.html#importing-packages",
    "href": "chapters/04_query/query.html#importing-packages",
    "title": "4  DataFrame Querying",
    "section": "4.1 Importing Packages",
    "text": "4.1 Importing Packages\nLet’s first import the packages that we will need.\n\nimport pandas as pd\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr\npd.set_option('display.max_rows', 10)"
  },
  {
    "objectID": "chapters/04_query/query.html#reading-in-data",
    "href": "chapters/04_query/query.html#reading-in-data",
    "title": "4  DataFrame Querying",
    "section": "4.2 Reading-In Data",
    "text": "4.2 Reading-In Data\nNext, let’s use pandas_datareader to read-in some SPY data from July 2021.\n\ndf_spy = pdr.get_data_yahoo('SPY', start='2021-06-30', end='2021-07-31')\ndf_spy = df_spy.round(2)\ndf_spy.head()\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nAdj Close\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2021-06-30\n427.21\n428.78\n427.18\n428.06\n415.28\n64827900\n\n\n2021-07-01\n428.87\n430.60\n428.80\n430.43\n417.58\n53441000\n\n\n2021-07-02\n431.67\n434.10\n430.52\n433.72\n420.77\n57697700\n\n\n2021-07-06\n433.78\n434.01\n430.01\n432.93\n420.00\n68710400\n\n\n2021-07-07\n433.66\n434.76\n431.51\n434.46\n421.49\n63549500\n\n\n\n\n\n\n\nThe following code resets the index so that Date is a regular column, and then makes the all columns names snake-case.\n\ndf_spy.reset_index(inplace=True)\ndf_spy.columns = df_spy.columns.str.lower().str.replace(' ', '_')\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n0\n2021-06-30\n427.21\n428.78\n427.18\n428.06\n415.28\n64827900\n\n\n1\n2021-07-01\n428.87\n430.60\n428.80\n430.43\n417.58\n53441000\n\n\n2\n2021-07-02\n431.67\n434.10\n430.52\n433.72\n420.77\n57697700\n\n\n3\n2021-07-06\n433.78\n434.01\n430.01\n432.93\n420.00\n68710400\n\n\n4\n2021-07-07\n433.66\n434.76\n431.51\n434.46\n421.49\n63549500"
  },
  {
    "objectID": "chapters/04_query/query.html#comparison-and-dataframe-columns",
    "href": "chapters/04_query/query.html#comparison-and-dataframe-columns",
    "title": "4  DataFrame Querying",
    "section": "4.3 Comparison and DataFrame Columns",
    "text": "4.3 Comparison and DataFrame Columns\nAs discussed in a previous chapter, a column of a DataFrame is a Series object, which is a souped up numpy.array (think vector or matrix).\nLet’s separate out the adjusted column of df_spy and assign it to a variable.\n\npd.options.display.max_rows = 6 # this modifies the printing of dataframes\nser_adjusted = df_spy['adj_close']\nser_adjusted\n\n0     415.28\n1     417.58\n2     420.77\n       ...  \n19    425.73\n20    427.49\n21    425.42\nName: adj_close, Length: 22, dtype: float64\n\n\nRecall that a pandas.Series is smart with respect to component-wise arithmetic operations, meaning it behaves like a vector from linear algebra. This means that arithmetic operations are broadcasted as you might expect.\nFor example, division by 100 is broadcasted component-wise.\n\nser_adjusted / 100\n\n0     4.1528\n1     4.1758\n2     4.2077\n       ...  \n19    4.2573\n20    4.2749\n21    4.2542\nName: adj_close, Length: 22, dtype: float64\n\n\nIt is a convenient fact that this broadcasting behavior also occurs with comparison, and produces a Series of booleans.\nThe following code checks which elements of ser_adjusted are greater than 425.\n\nser_test = (ser_adjusted &gt; 425)\nser_test\n\n0     False\n1     False\n2     False\n      ...  \n19     True\n20     True\n21     True\nName: adj_close, Length: 22, dtype: bool\n\n\nLet’s check that the resulting variable ser_test is a pandas.Series.\n\ntype(ser_test)\n\npandas.core.series.Series\n\n\nAnd finally let’s observe the .values elements of ser_test.\n\nprint(ser_test.values)\n\n[False False False False False False False False False False False False\n False False False False  True  True  True  True  True  True]\n\n\nA few observation about what just happened:\n\nWhen we compare a Series of numerical values (ser_adjusted) to a single number (425), we get back a Series of booleans (ser_test).\nWe have that ser_test[i] = (ser_adjusted[i] &gt; 425).\nSo the comparison operation was broadcasted as advertised.\n\nThis is easy to see by appending ser_test to df_spy and then reprinting.\n\npd.options.display.max_rows = 25\ndf_spy['test'] = ser_test\ndf_spy\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\ntest\n\n\n\n\n0\n2021-06-30\n427.21\n428.78\n427.18\n428.06\n415.28\n64827900\nFalse\n\n\n1\n2021-07-01\n428.87\n430.60\n428.80\n430.43\n417.58\n53441000\nFalse\n\n\n2\n2021-07-02\n431.67\n434.10\n430.52\n433.72\n420.77\n57697700\nFalse\n\n\n3\n2021-07-06\n433.78\n434.01\n430.01\n432.93\n420.00\n68710400\nFalse\n\n\n4\n2021-07-07\n433.66\n434.76\n431.51\n434.46\n421.49\n63549500\nFalse\n\n\n5\n2021-07-08\n428.78\n431.73\n427.52\n430.92\n418.05\n97595200\nFalse\n\n\n6\n2021-07-09\n432.53\n435.84\n430.71\n435.52\n422.51\n76238600\nFalse\n\n\n7\n2021-07-12\n435.43\n437.35\n434.97\n437.08\n424.03\n52889600\nFalse\n\n\n8\n2021-07-13\n436.24\n437.84\n435.31\n435.59\n422.58\n52911300\nFalse\n\n\n9\n2021-07-14\n437.40\n437.92\n434.91\n436.24\n423.21\n64130400\nFalse\n\n\n10\n2021-07-15\n434.81\n435.53\n432.72\n434.75\n421.77\n55126400\nFalse\n\n\n11\n2021-07-16\n436.01\n436.06\n430.92\n431.34\n418.46\n75874700\nFalse\n\n\n12\n2021-07-19\n426.19\n431.41\n421.97\n424.97\n412.28\n147987000\nFalse\n\n\n13\n2021-07-20\n425.68\n432.42\n424.83\n431.06\n418.19\n99608200\nFalse\n\n\n14\n2021-07-21\n432.34\n434.70\n431.01\n434.55\n421.57\n64724400\nFalse\n\n\n15\n2021-07-22\n434.74\n435.72\n433.69\n435.46\n422.46\n47878500\nFalse\n\n\n16\n2021-07-23\n437.52\n440.30\n436.79\n439.94\n426.80\n63766600\nTrue\n\n\n17\n2021-07-26\n439.31\n441.03\n439.26\n441.02\n427.85\n43719200\nTrue\n\n\n18\n2021-07-27\n439.91\n439.94\n435.99\n439.01\n425.90\n67397100\nTrue\n\n\n19\n2021-07-28\n439.68\n440.30\n437.31\n438.83\n425.73\n52472400\nTrue\n\n\n20\n2021-07-29\n439.82\n441.80\n439.81\n440.65\n427.49\n47435300\nTrue\n\n\n21\n2021-07-30\n437.91\n440.06\n437.77\n438.51\n425.42\n68951200\nTrue\n\n\n\n\n\n\n\nAs we will see in the next two sections, the broadcasting of comparison can be used to query subsets of rows of a DataFrame."
  },
  {
    "objectID": "chapters/04_query/query.html#dataframe-masking",
    "href": "chapters/04_query/query.html#dataframe-masking",
    "title": "4  DataFrame Querying",
    "section": "4.4 DataFrame Masking",
    "text": "4.4 DataFrame Masking\nFrom the code below we know that df_spy has 22 rows.\n\ndf_spy.shape\n\n(22, 8)\n\n\nThe following code creates a list consisting of 22 booleans, all of them False.\n\nlst_bool = [False] * 22\nlst_bool\n\n[False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False]\n\n\nNow, let’s see what happens when we feed this list of False booleans into df_spy using square brackets.\n\ndf_spy[lst_bool]\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\ntest\n\n\n\n\n\n\n\n\n\n\nCode Challenge: Verify that df_spy[lst_bool] is an empty DataFrame.\n\n\nSolution\ntype(df_spy[lst_bool])\n\n\npandas.core.frame.DataFrame\n\n\n\n\nSolution\ndf_spy[lst_bool].shape\n\n\n(0, 8)\n\n\n\nNext, let’s modify lst_bool slightly by changing the 0th entry to True. Then lets feed lst_bool into df_spy again.\n\nlst_bool[0] = True\ndf_spy[lst_bool]\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\ntest\n\n\n\n\n0\n2021-06-30\n427.21\n428.78\n427.18\n428.06\n415.28\n64827900\nFalse\n\n\n\n\n\n\n\nSo what happened? Notice that df_spy[lst_bool] returns a DataFrame consisting only of the 0th row of df_spy.\nLet’s modify lst_bool once again, by setting the 1st entry of df_spy to True, and then once again feed it into df_spy.\n\nlst_bool[1] = True\ndf_spy[lst_bool]\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\ntest\n\n\n\n\n0\n2021-06-30\n427.21\n428.78\n427.18\n428.06\n415.28\n64827900\nFalse\n\n\n1\n2021-07-01\n428.87\n430.60\n428.80\n430.43\n417.58\n53441000\nFalse\n\n\n\n\n\n\n\nPunchline: What is returned by the code df_spy[lst_bool] will be a DataFrame consisting of all the rows corresponding to the True entries of lst_bool.\nThis is called DataFrame masking.\n\nCode Challenge: Modify lst_bool and then use DataFrame masking to grab the 0th, 1st and, 3rd rows of df_spy.\n\n\nSolution\nlst_bool[3] = True\ndf_spy[lst_bool]\n\n\n\n\n\n\n\n\n\ndate\nopen\nhigh\nlow\nclose\nadj_close\nvolume\ntest\n\n\n\n\n0\n2021-06-30\n427.21\n428.78\n427.18\n428.06\n415.28\n64827900\nFalse\n\n\n1\n2021-07-01\n428.87\n430.60\n428.80\n430.43\n417.58\n53441000\nFalse\n\n\n3\n2021-07-06\n433.78\n434.01\n430.01\n432.93\n420.00\n68710400\nFalse"
  },
  {
    "objectID": "chapters/04_query/query.html#querying-with-dataframe-masking",
    "href": "chapters/04_query/query.html#querying-with-dataframe-masking",
    "title": "4  DataFrame Querying",
    "section": "4.5 Querying with DataFrame Masking",
    "text": "4.5 Querying with DataFrame Masking\nWe often want to query a DataFrame based on some kind of comparison involving its column values.\nWe can achieve this kind of querying by combining the broadcasting of comparison over DataFrame columns with DataFrame masking.\nIn order to consider concrete examples, let’s read-in some data.\nThe following code reads in a data set consisting of end-of-day prices for four different ETFs (SPY, IWM, QQQ, DIA), during the month of July 2021.\n\npd.options.display.max_rows = 25\ndf_etf = pdr.get_data_yahoo(['SPY', 'QQQ', 'IWM', 'DIA'], start='2021-06-30', end='2021-07-31')\ndf_etf = df_etf.round(2)\ndf_etf.head()\n\n[*********************100%***********************]  4 of 4 completed\n\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\n...\nLow\nOpen\nVolume\n\n\n\nDIA\nIWM\nQQQ\nSPY\nDIA\nIWM\nQQQ\nSPY\nDIA\nIWM\n...\nQQQ\nSPY\nDIA\nIWM\nQQQ\nSPY\nDIA\nIWM\nQQQ\nSPY\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2021-06-30\n331.30\n223.29\n349.98\n415.28\n344.95\n229.37\n354.43\n428.06\n345.51\n230.32\n...\n353.83\n427.18\n342.38\n228.65\n354.83\n427.21\n3778900\n26039000\n32724000\n64827900\n\n\n2021-07-01\n332.66\n225.26\n350.11\n417.58\n346.36\n231.39\n354.57\n430.43\n346.40\n231.85\n...\n352.68\n428.80\n345.78\n230.81\n354.07\n428.87\n3606900\n18089100\n29290000\n53441000\n\n\n2021-07-02\n334.17\n223.11\n354.13\n420.77\n347.94\n229.19\n358.64\n433.72\n348.29\n232.08\n...\n356.28\n430.52\n347.04\n232.00\n356.52\n431.67\n3013500\n21029700\n32727200\n57697700\n\n\n2021-07-06\n332.14\n219.87\n355.66\n420.00\n345.82\n225.86\n360.19\n432.93\n348.11\n229.46\n...\n356.49\n430.01\n347.75\n229.36\n359.26\n433.78\n3910600\n27771300\n38842400\n68710400\n\n\n2021-07-07\n333.20\n217.83\n356.41\n421.49\n346.92\n223.76\n360.95\n434.46\n347.14\n226.67\n...\n358.94\n431.51\n345.65\n225.54\n362.45\n433.66\n3347000\n28521500\n35265200\n63549500\n\n\n\n\n5 rows × 24 columns\n\n\n\nThis data is not as tidy as we would like. Let’s use method chaining to perform a series of data munging operations.\n\ndf_etf = \\\n    (\n    df_etf\n        .stack() #pivot the table\n        .reset_index() #turn date into a column\n        .rename(columns={'level_1':'Symbols'}) #renaming a column\n        .sort_values(by=['Symbols', 'Date']) #sort\n        .rename(columns={'Date':'date', 'Symbols':'symbol', 'Adj Close':'adj_close','Close':'close', \n                         'High':'high', 'Low':'low', 'Open':'open', 'Volume':'volume'}) #renaming columns\n        [['date', 'symbol','open', 'high', 'low', 'close', 'volume', 'adj_close']] #reordering columns\n        .reset_index(drop=True)    \n    )\ndf_etf\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\n\n\n\n\n0\n2021-06-30\nDIA\n342.38\n345.51\n342.35\n344.95\n3778900\n331.30\n\n\n1\n2021-07-01\nDIA\n345.78\n346.40\n344.92\n346.36\n3606900\n332.66\n\n\n2\n2021-07-02\nDIA\n347.04\n348.29\n346.18\n347.94\n3013500\n334.17\n\n\n3\n2021-07-06\nDIA\n347.75\n348.11\n343.60\n345.82\n3910600\n332.14\n\n\n4\n2021-07-07\nDIA\n345.65\n347.14\n344.43\n346.92\n3347000\n333.20\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n83\n2021-07-26\nSPY\n439.31\n441.03\n439.26\n441.02\n43719200\n427.85\n\n\n84\n2021-07-27\nSPY\n439.91\n439.94\n435.99\n439.01\n67397100\n425.90\n\n\n85\n2021-07-28\nSPY\n439.68\n440.30\n437.31\n438.83\n52472400\n425.73\n\n\n86\n2021-07-29\nSPY\n439.82\n441.80\n439.81\n440.65\n47435300\n427.49\n\n\n87\n2021-07-30\nSPY\n437.91\n440.06\n437.77\n438.51\n68951200\n425.42\n\n\n\n\n88 rows × 8 columns\n\n\n\n\n4.5.1 Querying for One Symbol\nWe are now ready to apply DataFrame masking to our ETF data set.\nAs a first example, let’s isolate all the rows of df_etf that correspond to IWM.\n\npd.options.display.max_rows = 6\nser_bool = (df_etf['symbol'] == \"IWM\")\ndf_etf[ser_bool]\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\n\n\n\n\n22\n2021-06-30\nIWM\n228.65\n230.32\n227.76\n229.37\n26039000\n223.29\n\n\n23\n2021-07-01\nIWM\n230.81\n231.85\n229.71\n231.39\n18089100\n225.26\n\n\n24\n2021-07-02\nIWM\n232.00\n232.08\n228.56\n229.19\n21029700\n223.11\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41\n2021-07-28\nIWM\n219.00\n222.59\n217.40\n220.82\n33043700\n214.97\n\n\n42\n2021-07-29\nIWM\n222.79\n224.44\n222.14\n222.52\n22634800\n216.62\n\n\n43\n2021-07-30\nIWM\n221.65\n224.05\n220.28\n221.05\n28473000\n215.19\n\n\n\n\n22 rows × 8 columns\n\n\n\nNotice that we did this in two steps:\n\nCalculate the series of booleans called ser_bool using comparison broadcasting.\nPerform the masking by using square brackets [] and ser_bool.\n\nWe can actually perform this masking in a single line of code, without creating the intermediate variable ser_bool.\n\ndf_etf[df_etf['symbol'] == \"IWM\"]\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\n\n\n\n\n22\n2021-06-30\nIWM\n228.65\n230.32\n227.76\n229.37\n26039000\n223.29\n\n\n23\n2021-07-01\nIWM\n230.81\n231.85\n229.71\n231.39\n18089100\n225.26\n\n\n24\n2021-07-02\nIWM\n232.00\n232.08\n228.56\n229.19\n21029700\n223.11\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41\n2021-07-28\nIWM\n219.00\n222.59\n217.40\n220.82\n33043700\n214.97\n\n\n42\n2021-07-29\nIWM\n222.79\n224.44\n222.14\n222.52\n22634800\n216.62\n\n\n43\n2021-07-30\nIWM\n221.65\n224.05\n220.28\n221.05\n28473000\n215.19\n\n\n\n\n22 rows × 8 columns\n\n\n\n\nCode Challenge: Select all the rows of df_etf for QQQ.\n\n\nSolution\ndf_etf[df_etf['symbol'] == 'QQQ']\n\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\n\n\n\n\n44\n2021-06-30\nQQQ\n354.83\n355.23\n353.83\n354.43\n32724000\n349.98\n\n\n45\n2021-07-01\nQQQ\n354.07\n355.09\n352.68\n354.57\n29290000\n350.11\n\n\n46\n2021-07-02\nQQQ\n356.52\n358.97\n356.28\n358.64\n32727200\n354.13\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n63\n2021-07-28\nQQQ\n365.60\n367.45\n363.24\n365.83\n42066200\n361.23\n\n\n64\n2021-07-29\nQQQ\n365.25\n367.68\n365.25\n366.48\n25672500\n361.88\n\n\n65\n2021-07-30\nQQQ\n362.44\n365.17\n362.41\n364.57\n36484600\n359.99\n\n\n\n\n22 rows × 8 columns\n\n\n\n\n\n\n4.5.2 Querying for Multiple Symbols\nWe can use the .isin() method to query a DataFrame for multiple symbols. The technique is to feed .isin() a list of symbols you want to query for.\nThe following code grabs all the rows of df_etf for both QQQ and DIA.\n\ndf_etf[df_etf['symbol'].isin(['QQQ', 'DIA'])]\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\n\n\n\n\n0\n2021-06-30\nDIA\n342.38\n345.51\n342.35\n344.95\n3778900\n331.30\n\n\n1\n2021-07-01\nDIA\n345.78\n346.40\n344.92\n346.36\n3606900\n332.66\n\n\n2\n2021-07-02\nDIA\n347.04\n348.29\n346.18\n347.94\n3013500\n334.17\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n63\n2021-07-28\nQQQ\n365.60\n367.45\n363.24\n365.83\n42066200\n361.23\n\n\n64\n2021-07-29\nQQQ\n365.25\n367.68\n365.25\n366.48\n25672500\n361.88\n\n\n65\n2021-07-30\nQQQ\n362.44\n365.17\n362.41\n364.57\n36484600\n359.99\n\n\n\n\n44 rows × 8 columns\n\n\n\n\nCode Challenge: Grab all rows of df_etf corresponding to SPY, IWM, and QQQ.\n\n\nSolution\ndf_etf[df_etf['symbol'].isin(['SPY', 'IWM', 'QQQ'])]\n\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\n\n\n\n\n22\n2021-06-30\nIWM\n228.65\n230.32\n227.76\n229.37\n26039000\n223.29\n\n\n23\n2021-07-01\nIWM\n230.81\n231.85\n229.71\n231.39\n18089100\n225.26\n\n\n24\n2021-07-02\nIWM\n232.00\n232.08\n228.56\n229.19\n21029700\n223.11\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n85\n2021-07-28\nSPY\n439.68\n440.30\n437.31\n438.83\n52472400\n425.73\n\n\n86\n2021-07-29\nSPY\n439.82\n441.80\n439.81\n440.65\n47435300\n427.49\n\n\n87\n2021-07-30\nSPY\n437.91\n440.06\n437.77\n438.51\n68951200\n425.42\n\n\n\n\n66 rows × 8 columns\n\n\n\n\n\n\n4.5.3 Querying for Dates\nThe following code grabs all the rows of df_etf that come after the middle of the month.\n\ndf_etf[df_etf['date'] &gt; '2021-07-15']\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\n\n\n\n\n11\n2021-07-16\nDIA\n350.72\n350.74\n346.34\n346.74\n5710400\n333.22\n\n\n12\n2021-07-19\nDIA\n341.79\n350.03\n337.38\n339.88\n9715300\n326.63\n\n\n13\n2021-07-20\nDIA\n340.29\n346.12\n339.75\n345.08\n5802200\n331.62\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n85\n2021-07-28\nSPY\n439.68\n440.30\n437.31\n438.83\n52472400\n425.73\n\n\n86\n2021-07-29\nSPY\n439.82\n441.80\n439.81\n440.65\n47435300\n427.49\n\n\n87\n2021-07-30\nSPY\n437.91\n440.06\n437.77\n438.51\n68951200\n425.42\n\n\n\n\n44 rows × 8 columns\n\n\n\n\nCode Challenge: Grab all the rows of df_etf for the last trade date of the month.\n\n\nSolution\ndf_etf[df_etf['date'] == '2021-07-30']\n\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\n\n\n\n\n21\n2021-07-30\nDIA\n349.88\n351.01\n348.67\n349.48\n3576700\n335.85\n\n\n43\n2021-07-30\nIWM\n221.65\n224.05\n220.28\n221.05\n28473000\n215.19\n\n\n65\n2021-07-30\nQQQ\n362.44\n365.17\n362.41\n364.57\n36484600\n359.99\n\n\n87\n2021-07-30\nSPY\n437.91\n440.06\n437.77\n438.51\n68951200\n425.42\n\n\n\n\n\n\n\n\n\n\n4.5.4 Querying on Multiple Criteria\nWe can filter on muliple criteria by using the & operator, which is the vectorized version of and.\nSuppose that we want all rows for SPY that come before July fourth.\n\nbln_ticker = (df_etf['symbol'] == 'SPY')\nbln_date = (df_etf['date'] &lt; '2021-07-04')\nbln_combined = bln_ticker & bln_date\n\ndf_etf[bln_combined]\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\n\n\n\n\n66\n2021-06-30\nSPY\n427.21\n428.78\n427.18\n428.06\n64827900\n415.28\n\n\n67\n2021-07-01\nSPY\n428.87\n430.60\n428.80\n430.43\n53441000\n417.58\n\n\n68\n2021-07-02\nSPY\n431.67\n434.10\n430.52\n433.72\n57697700\n420.77\n\n\n\n\n\n\n\n\nCode Challenge: Isolate the rows for QQQ and IWM on the last trading day before July 4th - try to not use intermediate variables.\n\ndf_etf[(df_etf['symbol'].isin([\"QQQ\", \"IWM\"])) & (df_etf['date']=='2021-07-02')]\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\n\n\n\n\n24\n2021-07-02\nIWM\n232.00\n232.08\n228.56\n229.19\n21029700\n223.11\n\n\n46\n2021-07-02\nQQQ\n356.52\n358.97\n356.28\n358.64\n32727200\n354.13"
  },
  {
    "objectID": "chapters/04_query/query.html#querying-with-.query",
    "href": "chapters/04_query/query.html#querying-with-.query",
    "title": "4  DataFrame Querying",
    "section": "4.6 Querying with .query()",
    "text": "4.6 Querying with .query()\nI find querying a DataFrame via masking to be rather cumbersome.\nI greatly prefer the use of the DataFrame.query() method which uses SQL-like strings to define queries.\nFor example, the following code grabs all the rows corresponding to IWM.\n\ndf_etf.query('symbol == \"IWM\"')\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\n\n\n\n\n22\n2021-06-30\nIWM\n228.65\n230.32\n227.76\n229.37\n26039000\n223.29\n\n\n23\n2021-07-01\nIWM\n230.81\n231.85\n229.71\n231.39\n18089100\n225.26\n\n\n24\n2021-07-02\nIWM\n232.00\n232.08\n228.56\n229.19\n21029700\n223.11\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41\n2021-07-28\nIWM\n219.00\n222.59\n217.40\n220.82\n33043700\n214.97\n\n\n42\n2021-07-29\nIWM\n222.79\n224.44\n222.14\n222.52\n22634800\n216.62\n\n\n43\n2021-07-30\nIWM\n221.65\n224.05\n220.28\n221.05\n28473000\n215.19\n\n\n\n\n22 rows × 8 columns\n\n\n\nThis code queries all rows corresponding to QQQ and DIA.\n\ndf_etf.query('symbol in (\"QQQ\", \"DIA\")')\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\n\n\n\n\n0\n2021-06-30\nDIA\n342.38\n345.51\n342.35\n344.95\n3778900\n331.30\n\n\n1\n2021-07-01\nDIA\n345.78\n346.40\n344.92\n346.36\n3606900\n332.66\n\n\n2\n2021-07-02\nDIA\n347.04\n348.29\n346.18\n347.94\n3013500\n334.17\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n63\n2021-07-28\nQQQ\n365.60\n367.45\n363.24\n365.83\n42066200\n361.23\n\n\n64\n2021-07-29\nQQQ\n365.25\n367.68\n365.25\n366.48\n25672500\n361.88\n\n\n65\n2021-07-30\nQQQ\n362.44\n365.17\n362.41\n364.57\n36484600\n359.99\n\n\n\n\n44 rows × 8 columns\n\n\n\nHere we grab the rows corresponding to the first half of July.\n\ndf_etf.query('date &lt; \"2021-07-15\"')\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\n\n\n\n\n0\n2021-06-30\nDIA\n342.38\n345.51\n342.35\n344.95\n3778900\n331.30\n\n\n1\n2021-07-01\nDIA\n345.78\n346.40\n344.92\n346.36\n3606900\n332.66\n\n\n2\n2021-07-02\nDIA\n347.04\n348.29\n346.18\n347.94\n3013500\n334.17\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n73\n2021-07-12\nSPY\n435.43\n437.35\n434.97\n437.08\n52889600\n424.03\n\n\n74\n2021-07-13\nSPY\n436.24\n437.84\n435.31\n435.59\n52911300\n422.58\n\n\n75\n2021-07-14\nSPY\n437.40\n437.92\n434.91\n436.24\n64130400\n423.21\n\n\n\n\n40 rows × 8 columns\n\n\n\nAnd we can filter on multiple criteria via method chaining. Here we grab all the rows for SPY and IWM from the second half of the month.\n\n(\ndf_etf\n    .query('symbol in (\"SPY\", \"IWM\")')\n    .query('date &gt; \"2021-07-15\"')\n)\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\n\n\n\n\n33\n2021-07-16\nIWM\n219.83\n219.88\n214.47\n214.95\n36620200\n209.25\n\n\n34\n2021-07-19\nIWM\n210.63\n214.45\n209.05\n211.73\n58571000\n206.12\n\n\n35\n2021-07-20\nIWM\n212.20\n219.27\n211.26\n218.30\n40794600\n212.51\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n85\n2021-07-28\nSPY\n439.68\n440.30\n437.31\n438.83\n52472400\n425.73\n\n\n86\n2021-07-29\nSPY\n439.82\n441.80\n439.81\n440.65\n47435300\n427.49\n\n\n87\n2021-07-30\nSPY\n437.91\n440.06\n437.77\n438.51\n68951200\n425.42\n\n\n\n\n22 rows × 8 columns\n\n\n\n\nCode Challenge: Grab all the rows of df_etf that correspond to the following criteria: 1. SPY 2. first half of month 3. close less than 435\n\n(\ndf_etf\n    .query('symbol == \"SPY\"')\n    .query('date &lt; \"2021-07-15\"')\n    .query('close &lt; 435')\n)\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\n\n\n\n\n66\n2021-06-30\nSPY\n427.21\n428.78\n427.18\n428.06\n64827900\n415.28\n\n\n67\n2021-07-01\nSPY\n428.87\n430.60\n428.80\n430.43\n53441000\n417.58\n\n\n68\n2021-07-02\nSPY\n431.67\n434.10\n430.52\n433.72\n57697700\n420.77\n\n\n69\n2021-07-06\nSPY\n433.78\n434.01\n430.01\n432.93\n68710400\n420.00\n\n\n70\n2021-07-07\nSPY\n433.66\n434.76\n431.51\n434.46\n63549500\n421.49\n\n\n71\n2021-07-08\nSPY\n428.78\n431.73\n427.52\n430.92\n97595200\n418.05"
  },
  {
    "objectID": "chapters/04_query/query.html#related-reading",
    "href": "chapters/04_query/query.html#related-reading",
    "title": "4  DataFrame Querying",
    "section": "4.7 Related Reading",
    "text": "4.7 Related Reading\nPython Data Science Handbook - Section 2.6 - Comparisons, Masks, and Boolean Logic\nPython Data Science Handbook - Section 2.7 - Fancy Indexing\nPython Data Science Handbook - Section 3.2 - Data Indexing and Selection\nPython Data Science Handbook - Section 3.12 - High Performance Pandas"
  },
  {
    "objectID": "chapters/05_functions_apply/functions_apply.html#defining-functions",
    "href": "chapters/05_functions_apply/functions_apply.html#defining-functions",
    "title": "5  Functions and the DataFrame.apply() Method",
    "section": "5.1 Defining Functions",
    "text": "5.1 Defining Functions\nDefining functions in Python is straightforward.\nThey syntax is simply def function_name(arguments):.\nThe following function squares two numbers.\n\ndef square(x):\n    sq = x ** 2\n    return(sq)\n\nLet’s verify that our function works.\n\nprint(square(2))\nprint(square(5))\n\n4\n25\n\n\n\nCode Challenge: Write a cube() function that cubes a number, and along the way, verify that indentation is required after the def statement.\n\n\nSolution\ndef cube(x):\n    cb = x ** 3\n    return(cb)\n\nprint(cube(3))\n\n\n27"
  },
  {
    "objectID": "chapters/05_functions_apply/functions_apply.html#option-payoff-function",
    "href": "chapters/05_functions_apply/functions_apply.html#option-payoff-function",
    "title": "5  Functions and the DataFrame.apply() Method",
    "section": "5.2 Option Payoff Function",
    "text": "5.2 Option Payoff Function\nLet’s now write a more financially interesting function.\nOptions are insurance contracts that are written on top of an underlying stock, much like car insurance is written on top of your car. There are two types of options: puts and calls. Put options protect you from the stock price going too low, while call options protect you from the stock price going too high. Both types have a feature called a strike price, which acts much like the deductible of your car insurance. Options expire sometime in the future, and the payoff (payout) of the option at the time of the expiration is as follows:\nLet \\(K\\) be the strike price of an option, and let \\(S_{T}\\) price of its underlying at the time of expiration. Then the payoff of each type of option is as follows:\n\ncall: \\(\\max(S_T - K, 0)\\)\nput: \\(\\max(K - S_T, 0)\\)\n\nWe can codify this as follows.\n\ndef option_payoff(cp, strike, upx):\n    if cp == 'call':\n        payoff = max(upx - strike, 0)\n    elif cp == 'put':\n        payoff = max(strike - upx, 0)\n    \n    return payoff\n\nLet’s verify that our function works.\n\nprint(option_payoff(\"call\", 100, 110))\nprint(option_payoff(\"put\", 100, 110))\nprint(option_payoff(\"call\", 100, 90))\nprint(option_payoff(\"put\", 100, 90))\n\n10\n0\n0\n10"
  },
  {
    "objectID": "chapters/05_functions_apply/functions_apply.html#loading-packages",
    "href": "chapters/05_functions_apply/functions_apply.html#loading-packages",
    "title": "5  Functions and the DataFrame.apply() Method",
    "section": "5.3 Loading Packages",
    "text": "5.3 Loading Packages\nLet’s now load the packages that we will need.\n\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "chapters/05_functions_apply/functions_apply.html#reading-in-data",
    "href": "chapters/05_functions_apply/functions_apply.html#reading-in-data",
    "title": "5  Functions and the DataFrame.apply() Method",
    "section": "5.4 Reading-In Data",
    "text": "5.4 Reading-In Data\nNext, let’s read in a data file called spy_expiring_options.csv.\nThis data set consists of 21 different options on SPY that expired on November 16, 2018.\nThe upx column is the settle price of SPY from that day, and it will be used to calculate the payoff of each of these options.\n\ndf_opt = pd.read_csv(\"spy_expiring_options.csv\")\ndf_opt = df_opt.round(2)\ndf_opt.head()\n\n\n\n\n\n\n\n\nunderlying\nupx\ntype\nexpiration\ndata_date\nstrike\n\n\n\n\n0\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n270.0\n\n\n1\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n270.5\n\n\n2\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n271.0\n\n\n3\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n271.5\n\n\n4\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n272.0"
  },
  {
    "objectID": "chapters/05_functions_apply/functions_apply.html#initializing-payoff-columns",
    "href": "chapters/05_functions_apply/functions_apply.html#initializing-payoff-columns",
    "title": "5  Functions and the DataFrame.apply() Method",
    "section": "5.5 Initializing Payoff Columns",
    "text": "5.5 Initializing Payoff Columns\nOur ultimate objective is to add a column of option payoffs to df_opt. We are going to accomplish this task using two different methods: (1) a for loop; (2) the DataFrame.apply() method.\nAs a first step, let’s add two columns to df_opt, one for each method, and then initialize them both with np.nan, which is a special data type that represents missing numerical data.\n\ndf_opt['payoff_loop'] = np.nan\ndf_opt['payoff_apply'] = np.nan\ndf_opt.head()\n\n\n\n\n\n\n\n\nunderlying\nupx\ntype\nexpiration\ndata_date\nstrike\npayoff_loop\npayoff_apply\n\n\n\n\n0\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n270.0\nNaN\nNaN\n\n\n1\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n270.5\nNaN\nNaN\n\n\n2\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n271.0\nNaN\nNaN\n\n\n3\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n271.5\nNaN\nNaN\n\n\n4\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n272.0\nNaN\nNaN"
  },
  {
    "objectID": "chapters/05_functions_apply/functions_apply.html#calculate-payoff_loop-via-for-loop",
    "href": "chapters/05_functions_apply/functions_apply.html#calculate-payoff_loop-via-for-loop",
    "title": "5  Functions and the DataFrame.apply() Method",
    "section": "5.6 Calculate payoff_loop via for loop",
    "text": "5.6 Calculate payoff_loop via for loop\nLet’s iterate through df_opt with a for loop and calculate the payoffs one by one. Notice that we are useing the .at indexer which is specifically designed to grab a single value from a column.\n\nfor ix in df_opt.index:\n    \n    # grabbing data from dataframe\n    opt_type = df_opt.at[ix, 'type']\n    strike = df_opt.at[ix, 'strike']\n    upx = df_opt.at[ix, 'upx']\n    \n    # calculating payoff\n    payoff = option_payoff(opt_type, strike, upx)\n    \n    # writing payoff to dataframe\n    df_opt.at[ix, 'payoff_loop'] = payoff\n      \ndf_opt\n\n\n\n\n\n\n\n\nunderlying\nupx\ntype\nexpiration\ndata_date\nstrike\npayoff_loop\npayoff_apply\n\n\n\n\n0\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n270.0\n0.00\nNaN\n\n\n1\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n270.5\n0.00\nNaN\n\n\n2\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n271.0\n0.00\nNaN\n\n\n3\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n271.5\n0.00\nNaN\n\n\n4\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n272.0\n0.00\nNaN\n\n\n5\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n272.5\n0.00\nNaN\n\n\n6\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n273.0\n0.00\nNaN\n\n\n7\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n273.5\n0.00\nNaN\n\n\n8\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n274.0\n0.27\nNaN\n\n\n9\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n274.5\n0.77\nNaN\n\n\n10\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n275.0\n1.27\nNaN\n\n\n11\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n275.5\n1.77\nNaN\n\n\n12\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n276.0\n2.27\nNaN\n\n\n13\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n276.5\n2.77\nNaN\n\n\n14\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n277.0\n3.27\nNaN\n\n\n15\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n277.5\n3.77\nNaN\n\n\n16\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n278.0\n0.00\nNaN\n\n\n17\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n278.5\n0.00\nNaN\n\n\n18\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n279.0\n0.00\nNaN\n\n\n19\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n279.5\n0.00\nNaN\n\n\n20\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n280.0\n0.00\nNaN"
  },
  {
    "objectID": "chapters/05_functions_apply/functions_apply.html#calculate-payoff_apply-via-.apply",
    "href": "chapters/05_functions_apply/functions_apply.html#calculate-payoff_apply-via-.apply",
    "title": "5  Functions and the DataFrame.apply() Method",
    "section": "5.7 Calculate payoff_apply via .apply()",
    "text": "5.7 Calculate payoff_apply via .apply()\nThe DataFrame.apply() method allows us to perform these calculations without explicitly iterating through df_opt with a for loop. It is a way to vectorize user defined functions.\nIn order to make use of .apply(), we will have to construct our custom payoff function slightly differently. The following opt_pay() function expects as its argument the row of a DataFrame.\n\ndef opt_pay(row):\n    # reading function inputs from DataFrame row\n    cp = row['type']\n    strike = row['strike']\n    upx = row['upx']\n    \n    # option payoff logic\n    if cp == 'call':\n        payoff = max(upx - strike, 0)\n    elif cp == 'put':\n        payoff = max(strike - upx, 0)\n    \n    return payoff\n\nWe can use .apply() to calculate the payoffs in a single line of code.\n\ndf_opt['payoff_apply'] = df_opt.apply(opt_pay, axis = 1)\ndf_opt\n\n\n\n\n\n\n\n\nunderlying\nupx\ntype\nexpiration\ndata_date\nstrike\npayoff_loop\npayoff_apply\n\n\n\n\n0\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n270.0\n0.00\n0.00\n\n\n1\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n270.5\n0.00\n0.00\n\n\n2\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n271.0\n0.00\n0.00\n\n\n3\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n271.5\n0.00\n0.00\n\n\n4\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n272.0\n0.00\n0.00\n\n\n5\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n272.5\n0.00\n0.00\n\n\n6\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n273.0\n0.00\n0.00\n\n\n7\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n273.5\n0.00\n0.00\n\n\n8\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n274.0\n0.27\n0.27\n\n\n9\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n274.5\n0.77\n0.77\n\n\n10\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n275.0\n1.27\n1.27\n\n\n11\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n275.5\n1.77\n1.77\n\n\n12\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n276.0\n2.27\n2.27\n\n\n13\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n276.5\n2.77\n2.77\n\n\n14\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n277.0\n3.27\n3.27\n\n\n15\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n277.5\n3.77\n3.77\n\n\n16\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n278.0\n0.00\n0.00\n\n\n17\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n278.5\n0.00\n0.00\n\n\n18\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n279.0\n0.00\n0.00\n\n\n19\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n279.5\n0.00\n0.00\n\n\n20\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n280.0\n0.00\n0.00\n\n\n\n\n\n\n\n\nCode Challenge: Add a column to df_opt that identifies if the upx is bigger or smaller than strike. Do this by writing a custom function and then using DataFrame.apply().\n\n\nSolution\ndef big_small(row):\n    upx = row['upx']\n    strike = row['strike']\n    \n    if upx &gt;= strike:\n        return('bigger')\n    else:\n        return('smaller')\n\n\n\n\nSolution\ndf_opt['big_small'] = df_opt.apply(big_small, axis = 1)\ndf_opt\n\n\n\n\n\n\n\n\n\nunderlying\nupx\ntype\nexpiration\ndata_date\nstrike\npayoff_loop\npayoff_apply\nbig_small\n\n\n\n\n0\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n270.0\n0.00\n0.00\nbigger\n\n\n1\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n270.5\n0.00\n0.00\nbigger\n\n\n2\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n271.0\n0.00\n0.00\nbigger\n\n\n3\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n271.5\n0.00\n0.00\nbigger\n\n\n4\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n272.0\n0.00\n0.00\nbigger\n\n\n5\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n272.5\n0.00\n0.00\nbigger\n\n\n6\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n273.0\n0.00\n0.00\nbigger\n\n\n7\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n273.5\n0.00\n0.00\nbigger\n\n\n8\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n274.0\n0.27\n0.27\nsmaller\n\n\n9\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n274.5\n0.77\n0.77\nsmaller\n\n\n10\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n275.0\n1.27\n1.27\nsmaller\n\n\n11\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n275.5\n1.77\n1.77\nsmaller\n\n\n12\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n276.0\n2.27\n2.27\nsmaller\n\n\n13\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n276.5\n2.77\n2.77\nsmaller\n\n\n14\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n277.0\n3.27\n3.27\nsmaller\n\n\n15\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n277.5\n3.77\n3.77\nsmaller\n\n\n16\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n278.0\n0.00\n0.00\nsmaller\n\n\n17\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n278.5\n0.00\n0.00\nsmaller\n\n\n18\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n279.0\n0.00\n0.00\nsmaller\n\n\n19\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n279.5\n0.00\n0.00\nsmaller\n\n\n20\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n280.0\n0.00\n0.00\nsmaller"
  },
  {
    "objectID": "chapters/05_functions_apply/functions_apply.html#related-reading",
    "href": "chapters/05_functions_apply/functions_apply.html#related-reading",
    "title": "5  Functions and the DataFrame.apply() Method",
    "section": "5.8 Related Reading",
    "text": "5.8 Related Reading\nA Whirlwind Tour of Python - Chapter 8 - Control Flow\nA Whirlwind Tour of Python - Chapter 9 - Defining Functions"
  },
  {
    "objectID": "chapters/06_merge/merge.html#loading-packages",
    "href": "chapters/06_merge/merge.html#loading-packages",
    "title": "6  Merging DataFrames",
    "section": "6.1 Loading Packages",
    "text": "6.1 Loading Packages\nLet’s load the packages we will need for this tutorial.\n\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "chapters/06_merge/merge.html#reading-in-data",
    "href": "chapters/06_merge/merge.html#reading-in-data",
    "title": "6  Merging DataFrames",
    "section": "6.2 Reading-In Data",
    "text": "6.2 Reading-In Data\nThe data set we are going to use is a list of ETFs that have weekly expiring options. What does that mean? Most stocks or ETFs have exchange traded options that expire every month, and at any given time the monthly expiring options go out about a year. The most liquid underlyings have options that expire every week; these weekly expiring options go out about 6-8 weeks.\nThis is a list that is published by the CBOE and it consists of all the ETFs that have weekly options trading.\n\ndf_weekly = pd.read_csv('weekly_etf.csv')\ndf_weekly.head()\n\n\n\n\n\n\n\n\nticker\nname\n\n\n\n\n0\nAMJ\nJP Morgan Alerian MLP Index ETN\n\n\n1\nAMLP\nAlerian MLP ETF\n\n\n2\nASHR\nXtrackers Harvest CSI 300 China A-Shares ETF\n\n\n3\nDIA\nSPDR Dow Jones Ind Av ETF Trust\n\n\n4\nDUST\nDirexion Daily Gold Miners Index Bear 3X Shares\n\n\n\n\n\n\n\nThe next data set that we are going to load is a comprehensive list of all ETFs that are trading in the market.\n\ndf_etf = pd.read_csv(\"etf.csv\")\ndf_etf.head()\n\n\n\n\n\n\n\n\nsymbol\nname\nissuer\nexpense_ratio\naum\nspread\nsegment\n\n\n\n\n0\nSPY\nSPDR S&P 500 ETF Trust\nState Street Global Advisors\n0.09%\n$275.42B\n0.00%\nEquity: U.S. - Large Cap\n\n\n1\nIVV\niShares Core S&P 500 ETF\nBlackRock\n0.04%\n$155.86B\n0.01%\nEquity: U.S. - Large Cap\n\n\n2\nVTI\nVanguard Total Stock Market ETF\nVanguard\n0.04%\n$103.58B\n0.01%\nEquity: U.S. - Total Market\n\n\n3\nVOO\nVanguard S&P 500 ETF\nVanguard\n0.04%\n$96.91B\n0.01%\nEquity: U.S. - Large Cap\n\n\n4\nEFA\niShares MSCI EAFE ETF\nBlackRock\n0.32%\n$72.12B\n0.01%\nEquity: Developed Markets Ex-U.S. - Total Market\n\n\n\n\n\n\n\nMotivation: Notice that df_etf has a segment column which df_weekly does not. This segment column contains asset-class information that could be useful for categorizing the weekly ETFs.\nObjective: we want to get the segment column into df_weekly.\nThere are a couple of ways of accomplishing this in pandas and both of them involve the pd.merge() method:\n\ninner-merge\nleft/right-merge (sometimes called outer merge)"
  },
  {
    "objectID": "chapters/06_merge/merge.html#inner",
    "href": "chapters/06_merge/merge.html#inner",
    "title": "6  Merging DataFrames",
    "section": "6.3 Inner",
    "text": "6.3 Inner\nAs with many of the basic operations in data analysis, it’s easiest to understand inner-merges by digging into an example.\nHere is the line of code that accomplishes most of the work that we want done:\n\npd.merge(df_weekly, df_etf, how='inner', left_on='ticker', right_on='symbol')\n\n\n\n\n\n\n\n\nticker\nname_x\nsymbol\nname_y\nissuer\nexpense_ratio\naum\nspread\nsegment\n\n\n\n\n0\nAMJ\nJP Morgan Alerian MLP Index ETN\nAMJ\nJ.P. Morgan Alerian MLP Index ETN\nJPMorgan\n0.85%\n$3.45B\n0.04%\nEquity: U.S. MLPs\n\n\n1\nAMLP\nAlerian MLP ETF\nAMLP\nAlerian MLP ETF\nALPS\n0.85%\n$10.64B\n0.10%\nEquity: U.S. MLPs\n\n\n2\nASHR\nXtrackers Harvest CSI 300 China A-Shares ETF\nASHR\nXtrackers Harvest CSI 300 China A-Shares ETF\nDeutsche Bank\n0.65%\n$630.14M\n0.04%\nEquity: China - Total Market\n\n\n3\nDIA\nSPDR Dow Jones Ind Av ETF Trust\nDIA\nSPDR Dow Jones Industrial Average ETF Trust\nState Street Global Advisors\n0.17%\n$21.70B\n0.01%\nEquity: U.S. - Large Cap\n\n\n4\nDUST\nDirexion Daily Gold Miners Index Bear 3X Shares\nDUST\nDirexion Daily Gold Miners Index Bear 3x Shares\nDirexion\n1.08%\n$122.21M\n0.06%\nInverse Equity: Global Gold Miners\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n61\nXLV\nHEALTH CARE SELECT SECTOR SPDR\nXLV\nHealth Care Select Sector SPDR Fund\nState Street Global Advisors\n0.13%\n$17.49B\n0.01%\nEquity: U.S. Health Care\n\n\n62\nXLY\nConsumer Discretionary Select Sector SPDR\nXLY\nConsumer Discretionary Select Sector SPDR Fund\nState Street Global Advisors\n0.13%\n$14.35B\n0.01%\nEquity: U.S. Consumer Cyclicals\n\n\n63\nXME\nSPDR S&P Metals & Mining ETF\nXME\nSPDR S&P Metals & Mining ETF\nState Street Global Advisors\n0.35%\n$879.10M\n0.03%\nEquity: U.S. Metals & Mining\n\n\n64\nXOP\nP Oil & Gas Exploration & Production ETF\nXOP\nSPDR S&P Oil & Gas Exploration & Production ETF\nState Street Global Advisors\n0.35%\n$3.06B\n0.02%\nEquity: U.S. Oil & Gas Exploration & Production\n\n\n65\nXRT\nSPDR S&P Oil & Gas Exploration & Production ETF\nXRT\nSPDR S&P Retail ETF\nState Street Global Advisors\n0.35%\n$704.67M\n0.02%\nEquity: U.S. Retail\n\n\n\n\n66 rows × 9 columns\n\n\n\nObservations on the syntax:\n\nThe first two arguments of pd.merge() are the two DataFrames we want to merge together. The first DataFrame is the left DataFrame and the second one is the right DataFrame.\nThe how argument defines the type of merge.\nleft_on is the column in the left table that will be used for matching, right_on is the column in the right table that will be used for matching.\n\nObservations on output:\n\nThe output is basically each of the two tables smashed together, however only the rows with matching ticker/symbol are retained in the output. All columns of both tables are included.\ndf_weekly had 67 rows in it, and df_etf had 2,160 row in it. The DataFrame that results from pd.merge() has 66 rows in it.\nNotice that both df_weekly and df_etf have a column called name. In the merged DataFrame, suffixes of _x and _y have been added to the column names to make them unique.\n\nLet’s do a little clean up of our DataFrame so that it’s just the information that we wanted: df_weekly with the segment column added to it. Notice that .merge() is also a DataFrame method, and we use this form to invoke method chaining.\n\ndf_inner = \\\n    (\n    df_weekly\n        .merge(df_etf, how='inner', left_on='ticker', right_on='symbol')\n        [['ticker', 'name_x', 'segment']]\n        .rename(columns={'name_x':'name'})\n    )\ndf_inner\n\n\n\n\n\n\n\n\nticker\nname\nsegment\n\n\n\n\n0\nAMJ\nJP Morgan Alerian MLP Index ETN\nEquity: U.S. MLPs\n\n\n1\nAMLP\nAlerian MLP ETF\nEquity: U.S. MLPs\n\n\n2\nASHR\nXtrackers Harvest CSI 300 China A-Shares ETF\nEquity: China - Total Market\n\n\n3\nDIA\nSPDR Dow Jones Ind Av ETF Trust\nEquity: U.S. - Large Cap\n\n\n4\nDUST\nDirexion Daily Gold Miners Index Bear 3X Shares\nInverse Equity: Global Gold Miners\n\n\n...\n...\n...\n...\n\n\n61\nXLV\nHEALTH CARE SELECT SECTOR SPDR\nEquity: U.S. Health Care\n\n\n62\nXLY\nConsumer Discretionary Select Sector SPDR\nEquity: U.S. Consumer Cyclicals\n\n\n63\nXME\nSPDR S&P Metals & Mining ETF\nEquity: U.S. Metals & Mining\n\n\n64\nXOP\nP Oil & Gas Exploration & Production ETF\nEquity: U.S. Oil & Gas Exploration & Production\n\n\n65\nXRT\nSPDR S&P Oil & Gas Exploration & Production ETF\nEquity: U.S. Retail\n\n\n\n\n66 rows × 3 columns"
  },
  {
    "objectID": "chapters/06_merge/merge.html#left",
    "href": "chapters/06_merge/merge.html#left",
    "title": "6  Merging DataFrames",
    "section": "6.4 Left",
    "text": "6.4 Left\nNotice that in the inner-join example from the previous section, the original DataFrame of ETFs with weekly options (df_weekly) had 67 rows, but the merged DataFrame with the segment column added (df_inner) only has 66 rows.\n\nprint(df_weekly.shape)\nprint(df_inner.shape)\n\n(67, 2)\n(66, 3)\n\n\nSo what happened? This means that one of the tickers from df_weekly had no matching symbol in df_etf.\nInner-merges, by design, are only intended to retain rows that have matches in both tables. This may or may not be the desired behavior you are looking for.\nLet’s say that instead we wanted to keep ALL the rows in the left DataFrame, df_weekly, irrespective of whether there is a match in the right DataFrame.\nThis is precisely what a left-merge is. The syntax is the exact same as before except for the how argument is set to 'left'.\n\npd.merge(df_weekly, df_etf, how='left', left_on='ticker', right_on='symbol')\n\n\n\n\n\n\n\n\nticker\nname_x\nsymbol\nname_y\nissuer\nexpense_ratio\naum\nspread\nsegment\n\n\n\n\n0\nAMJ\nJP Morgan Alerian MLP Index ETN\nAMJ\nJ.P. Morgan Alerian MLP Index ETN\nJPMorgan\n0.85%\n$3.45B\n0.04%\nEquity: U.S. MLPs\n\n\n1\nAMLP\nAlerian MLP ETF\nAMLP\nAlerian MLP ETF\nALPS\n0.85%\n$10.64B\n0.10%\nEquity: U.S. MLPs\n\n\n2\nASHR\nXtrackers Harvest CSI 300 China A-Shares ETF\nASHR\nXtrackers Harvest CSI 300 China A-Shares ETF\nDeutsche Bank\n0.65%\n$630.14M\n0.04%\nEquity: China - Total Market\n\n\n3\nDIA\nSPDR Dow Jones Ind Av ETF Trust\nDIA\nSPDR Dow Jones Industrial Average ETF Trust\nState Street Global Advisors\n0.17%\n$21.70B\n0.01%\nEquity: U.S. - Large Cap\n\n\n4\nDUST\nDirexion Daily Gold Miners Index Bear 3X Shares\nDUST\nDirexion Daily Gold Miners Index Bear 3x Shares\nDirexion\n1.08%\n$122.21M\n0.06%\nInverse Equity: Global Gold Miners\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n62\nXLV\nHEALTH CARE SELECT SECTOR SPDR\nXLV\nHealth Care Select Sector SPDR Fund\nState Street Global Advisors\n0.13%\n$17.49B\n0.01%\nEquity: U.S. Health Care\n\n\n63\nXLY\nConsumer Discretionary Select Sector SPDR\nXLY\nConsumer Discretionary Select Sector SPDR Fund\nState Street Global Advisors\n0.13%\n$14.35B\n0.01%\nEquity: U.S. Consumer Cyclicals\n\n\n64\nXME\nSPDR S&P Metals & Mining ETF\nXME\nSPDR S&P Metals & Mining ETF\nState Street Global Advisors\n0.35%\n$879.10M\n0.03%\nEquity: U.S. Metals & Mining\n\n\n65\nXOP\nP Oil & Gas Exploration & Production ETF\nXOP\nSPDR S&P Oil & Gas Exploration & Production ETF\nState Street Global Advisors\n0.35%\n$3.06B\n0.02%\nEquity: U.S. Oil & Gas Exploration & Production\n\n\n66\nXRT\nSPDR S&P Oil & Gas Exploration & Production ETF\nXRT\nSPDR S&P Retail ETF\nState Street Global Advisors\n0.35%\n$704.67M\n0.02%\nEquity: U.S. Retail\n\n\n\n\n67 rows × 9 columns\n\n\n\nLet’s put this left-merged table into a DataFrame called df_left, and perform a bit of data munging.\n\ndf_left = \\\n    (\n    df_weekly\n        .merge(df_etf, how='left', left_on='ticker', right_on='symbol')\n        [['ticker', 'name_x', 'segment']]\n        .rename(columns={'name_x':'name'})\n    )\ndf_left\n\n\n\n\n\n\n\n\nticker\nname\nsegment\n\n\n\n\n0\nAMJ\nJP Morgan Alerian MLP Index ETN\nEquity: U.S. MLPs\n\n\n1\nAMLP\nAlerian MLP ETF\nEquity: U.S. MLPs\n\n\n2\nASHR\nXtrackers Harvest CSI 300 China A-Shares ETF\nEquity: China - Total Market\n\n\n3\nDIA\nSPDR Dow Jones Ind Av ETF Trust\nEquity: U.S. - Large Cap\n\n\n4\nDUST\nDirexion Daily Gold Miners Index Bear 3X Shares\nInverse Equity: Global Gold Miners\n\n\n...\n...\n...\n...\n\n\n62\nXLV\nHEALTH CARE SELECT SECTOR SPDR\nEquity: U.S. Health Care\n\n\n63\nXLY\nConsumer Discretionary Select Sector SPDR\nEquity: U.S. Consumer Cyclicals\n\n\n64\nXME\nSPDR S&P Metals & Mining ETF\nEquity: U.S. Metals & Mining\n\n\n65\nXOP\nP Oil & Gas Exploration & Production ETF\nEquity: U.S. Oil & Gas Exploration & Production\n\n\n66\nXRT\nSPDR S&P Oil & Gas Exploration & Production ETF\nEquity: U.S. Retail\n\n\n\n\n67 rows × 3 columns\n\n\n\n\nCode Challenge: Use .query() on df_left to verify that ticker FTK has NaNs for all the columns from df_etf. Do this in two separate ways:\n\nquerying on ticker\nquerying on segment\n\n\n\nSolution\ndf_left.query('ticker == \"FTK\"')\n\n\n\n\n\n\n\n\n\nticker\nname\nsegment\n\n\n\n\n17\nFTK\nFLOTEK INDUSTRIES INC\nNaN\n\n\n\n\n\n\n\n\n\nSolution\ndf_left.query('segment.isnull()')\n\n\n\n\n\n\n\n\n\nticker\nname\nsegment\n\n\n\n\n17\nFTK\nFLOTEK INDUSTRIES INC\nNaN\n\n\n\n\n\n\n\n\nResearch Challenge: Google FTK and figure out why it’s not in df_etf.\n\n\nSolution\n# FTK is a stock not and ETF."
  },
  {
    "objectID": "chapters/06_merge/merge.html#related-reading",
    "href": "chapters/06_merge/merge.html#related-reading",
    "title": "6  Merging DataFrames",
    "section": "6.5 Related Reading",
    "text": "6.5 Related Reading\nPython Data Science Handboook - Section 3.7 - Combining Datasets: Merging and Joining"
  },
  {
    "objectID": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#loading-packages",
    "href": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#loading-packages",
    "title": "7  .groupby() and .agg() - 1",
    "section": "7.1 Loading Packages",
    "text": "7.1 Loading Packages\nLet’s begin by loading the packages that we will need.\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr"
  },
  {
    "objectID": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#reading-in-data",
    "href": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#reading-in-data",
    "title": "7  .groupby() and .agg() - 1",
    "section": "7.2 Reading-In Data",
    "text": "7.2 Reading-In Data\nOur analysis will be on the set of of July 2021 prices for SPY, IWM, QQQ, DIA.\nLet’s read-in that data with pandas_datareader.\n\npd.options.display.max_rows = 25\ndf_etf = pdr.get_data_yahoo(['SPY', 'QQQ', 'IWM', 'DIA'], start='2021-06-30', end='2021-07-31')\ndf_etf = df_etf.round(2)\ndf_etf.head()\n\n[*********************100%***********************]  4 of 4 completed\n\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\n...\nLow\nOpen\nVolume\n\n\n\nDIA\nIWM\nQQQ\nSPY\nDIA\nIWM\nQQQ\nSPY\nDIA\nIWM\n...\nQQQ\nSPY\nDIA\nIWM\nQQQ\nSPY\nDIA\nIWM\nQQQ\nSPY\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2021-06-30\n331.30\n223.29\n349.98\n415.28\n344.95\n229.37\n354.43\n428.06\n345.51\n230.32\n...\n353.83\n427.18\n342.38\n228.65\n354.83\n427.21\n3778900\n26039000\n32724000\n64827900\n\n\n2021-07-01\n332.66\n225.26\n350.11\n417.58\n346.36\n231.39\n354.57\n430.43\n346.40\n231.85\n...\n352.68\n428.80\n345.78\n230.81\n354.07\n428.87\n3606900\n18089100\n29290000\n53441000\n\n\n2021-07-02\n334.17\n223.11\n354.13\n420.77\n347.94\n229.19\n358.64\n433.72\n348.29\n232.08\n...\n356.28\n430.52\n347.04\n232.00\n356.52\n431.67\n3013500\n21029700\n32727200\n57697700\n\n\n2021-07-06\n332.14\n219.87\n355.66\n420.00\n345.82\n225.86\n360.19\n432.93\n348.11\n229.46\n...\n356.49\n430.01\n347.75\n229.36\n359.26\n433.78\n3910600\n27771300\n38842400\n68710400\n\n\n2021-07-07\n333.20\n217.83\n356.41\n421.49\n346.92\n223.76\n360.95\n434.46\n347.14\n226.67\n...\n358.94\n431.51\n345.65\n225.54\n362.45\n433.66\n3347000\n28521500\n35265200\n63549500\n\n\n\n\n5 rows × 24 columns\n\n\n\nThis data is not as tidy as we would like. Let’s use method chaining to perform a series of data munging operations.\n\ndf_etf = \\\n    (\n    df_etf\n        .stack() #pivot the table\n        .reset_index() #turn date into a column\n        .rename(columns={'level_1':'Symbols'}) #renaming a column\n        .sort_values(by=['Symbols', 'Date']) #sort\n        .rename(columns={'Date':'date', 'Symbols':'symbol', 'Adj Close':'adj_close','Close':'close', \n                         'High':'high', 'Low':'low', 'Open':'open', 'Volume':'volume'}) #renaming columns\n        [['date', 'symbol','open', 'high', 'low', 'close', 'volume', 'adj_close']] #reordering columns\n        .reset_index(drop=True)    \n    )\ndf_etf\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\n\n\n\n\n0\n2021-06-30\nDIA\n342.38\n345.51\n342.35\n344.95\n3778900\n331.30\n\n\n1\n2021-07-01\nDIA\n345.78\n346.40\n344.92\n346.36\n3606900\n332.66\n\n\n2\n2021-07-02\nDIA\n347.04\n348.29\n346.18\n347.94\n3013500\n334.17\n\n\n3\n2021-07-06\nDIA\n347.75\n348.11\n343.60\n345.82\n3910600\n332.14\n\n\n4\n2021-07-07\nDIA\n345.65\n347.14\n344.43\n346.92\n3347000\n333.20\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n83\n2021-07-26\nSPY\n439.31\n441.03\n439.26\n441.02\n43719200\n427.85\n\n\n84\n2021-07-27\nSPY\n439.91\n439.94\n435.99\n439.01\n67397100\n425.90\n\n\n85\n2021-07-28\nSPY\n439.68\n440.30\n437.31\n438.83\n52472400\n425.73\n\n\n86\n2021-07-29\nSPY\n439.82\n441.80\n439.81\n440.65\n47435300\n427.49\n\n\n87\n2021-07-30\nSPY\n437.91\n440.06\n437.77\n438.51\n68951200\n425.42\n\n\n\n\n88 rows × 8 columns"
  },
  {
    "objectID": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#daily-returns-with-groupby",
    "href": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#daily-returns-with-groupby",
    "title": "7  .groupby() and .agg() - 1",
    "section": "7.3 Daily Returns with groupby()",
    "text": "7.3 Daily Returns with groupby()\nOur ultimate goal is to calculate monthly returns and monthly volatilities for each ETF in df_etf. These quantities are both functions of daily returns. So our first order of business is to calculate daily returns.\nIn a previous tutorial we calculated daily returns in a simple vectorized fashion. Unfortunately, we can’t use the exact same approach here because there are multiple ETFs in the data set.\nTo overcome this challenge we will use our first application of .groupby().\nHere is the .groupby() code that calculates daily returns for each ETF.\n\n# sorting values to get everything in the right order\ndf_etf.sort_values(['symbol', 'date'], inplace=True)\n\n# vectorized return calculation\ndf_etf['ret'] = \\\n    df_etf['close'].groupby(df_etf['symbol']).pct_change()\ndf_etf.head()\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\nret\n\n\n\n\n0\n2021-06-30\nDIA\n342.38\n345.51\n342.35\n344.95\n3778900\n331.30\nNaN\n\n\n1\n2021-07-01\nDIA\n345.78\n346.40\n344.92\n346.36\n3606900\n332.66\n0.004088\n\n\n2\n2021-07-02\nDIA\n347.04\n348.29\n346.18\n347.94\n3013500\n334.17\n0.004562\n\n\n3\n2021-07-06\nDIA\n347.75\n348.11\n343.60\n345.82\n3910600\n332.14\n-0.006093\n\n\n4\n2021-07-07\nDIA\n345.65\n347.14\n344.43\n346.92\n3347000\n333.20\n0.003181\n\n\n\n\n\n\n\n\nCode Challenge: If the .group_by() worked correctly, we should see a NaN value in the ret column for the first trade-date of each ETF. Use DataFrame.query() to confirm this.\n\n\nSolution\ndf_etf.query('ret.isnull()')\n\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\nret\n\n\n\n\n0\n2021-06-30\nDIA\n342.38\n345.51\n342.35\n344.95\n3778900\n331.30\nNaN\n\n\n22\n2021-06-30\nIWM\n228.65\n230.32\n227.76\n229.37\n26039000\n223.29\nNaN\n\n\n44\n2021-06-30\nQQQ\n354.83\n355.23\n353.83\n354.43\n32724000\n349.98\nNaN\n\n\n66\n2021-06-30\nSPY\n427.21\n428.78\n427.18\n428.06\n64827900\n415.28\nNaN"
  },
  {
    "objectID": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#monthly-return-for-each-symbol",
    "href": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#monthly-return-for-each-symbol",
    "title": "7  .groupby() and .agg() - 1",
    "section": "7.4 Monthly Return for Each symbol",
    "text": "7.4 Monthly Return for Each symbol\nWe’ll now proceed to calculate monthly returns and monthly volatilities for each of the ETFs in our data set. This amounts to first grouping by symbol, and then performing an aggregation calculation on ret (daily returns).\nLet’s start with monthly returns. As a preliminary step we’ll calculate the daily growth factor in a separate column.\n\ndf_etf['daily_factor'] = 1 + df_etf['ret']\ndf_etf.head()\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\nret\ndaily_factor\n\n\n\n\n0\n2021-06-30\nDIA\n342.38\n345.51\n342.35\n344.95\n3778900\n331.30\nNaN\nNaN\n\n\n1\n2021-07-01\nDIA\n345.78\n346.40\n344.92\n346.36\n3606900\n332.66\n0.004088\n1.004088\n\n\n2\n2021-07-02\nDIA\n347.04\n348.29\n346.18\n347.94\n3013500\n334.17\n0.004562\n1.004562\n\n\n3\n2021-07-06\nDIA\n347.75\n348.11\n343.60\n345.82\n3910600\n332.14\n-0.006093\n0.993907\n\n\n4\n2021-07-07\nDIA\n345.65\n347.14\n344.43\n346.92\n3347000\n333.20\n0.003181\n1.003181\n\n\n\n\n\n\n\nRecall that the monthly growth factor is the product of the daily growth factors. Here is a way to write all that logic in a single line using .groupby() and .agg():\n\ndf_grouped_factor = \\\n    df_etf.groupby(['symbol'])['daily_factor'].agg([np.prod]).reset_index()\ndf_grouped_factor\n\n\n\n\n\n\n\n\nsymbol\nprod\n\n\n\n\n0\nDIA\n1.013132\n\n\n1\nIWM\n0.963727\n\n\n2\nQQQ\n1.028609\n\n\n3\nSPY\n1.024412\n\n\n\n\n\n\n\nNotice that pandas isn’t very sophisticated about the name that it gives to the column that stores the aggregation calculation. It just gave it the name prod, which is the name of the function that was used in the aggregation calculation. Let’s make df_grouped_factor a bit more readable by renaming that column.\n\ndf_grouped_factor.rename(columns={'prod': 'monthly_factor'}, inplace=True)\ndf_grouped_factor\n\n\n\n\n\n\n\n\nsymbol\nmonthly_factor\n\n\n\n\n0\nDIA\n1.013132\n\n\n1\nIWM\n0.963727\n\n\n2\nQQQ\n1.028609\n\n\n3\nSPY\n1.024412\n\n\n\n\n\n\n\nAnd finally, recall that the monthly return is calculated by subtracting one from the monthly growth factor.\n\ndf_grouped_factor['monthly_return'] = df_grouped_factor['monthly_factor'] - 1\ndf_grouped_factor[['symbol', 'monthly_return']]\n\n\n\n\n\n\n\n\nsymbol\nmonthly_return\n\n\n\n\n0\nDIA\n0.013132\n\n\n1\nIWM\n-0.036273\n\n\n2\nQQQ\n0.028609\n\n\n3\nSPY\n0.024412"
  },
  {
    "objectID": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#monthly-volatility-for-each-symbol",
    "href": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#monthly-volatility-for-each-symbol",
    "title": "7  .groupby() and .agg() - 1",
    "section": "7.5 Monthly Volatility for Each symbol",
    "text": "7.5 Monthly Volatility for Each symbol\nNow let’s calculate the (realized/historical) volatility for each of the ETFs.\nWe once again use .groupby() and .agg() to do most of the work in a single line of code.\n\ndf_grouped_vol = \\\n    df_etf.groupby(['symbol'])['ret'].agg([np.std]).reset_index()\n\ndf_grouped_vol\n\n\n\n\n\n\n\n\nsymbol\nstd\n\n\n\n\n0\nDIA\n0.007733\n\n\n1\nIWM\n0.014032\n\n\n2\nQQQ\n0.006832\n\n\n3\nSPY\n0.007152\n\n\n\n\n\n\n\nAgain, let’s rename our aggregation column to something more descriptive.\n\ndf_grouped_vol.rename(columns={'std':'daily_vol'}, inplace=True)\ndf_grouped_vol\n\n\n\n\n\n\n\n\nsymbol\ndaily_vol\n\n\n\n\n0\nDIA\n0.007733\n\n\n1\nIWM\n0.014032\n\n\n2\nQQQ\n0.006832\n\n\n3\nSPY\n0.007152\n\n\n\n\n\n\n\nWhat we have calculated is a daily volatility, but when practitioners talk about volatility, they typically annualize it. A daily volatility is annualized by multiplying by \\(\\sqrt{252}\\).\n\ndf_grouped_vol['ann_vol'] = df_grouped_vol['daily_vol'] * np.sqrt(252)\ndf_grouped_vol\n\n\n\n\n\n\n\n\nsymbol\ndaily_vol\nann_vol\n\n\n\n\n0\nDIA\n0.007733\n0.122752\n\n\n1\nIWM\n0.014032\n0.222744\n\n\n2\nQQQ\n0.006832\n0.108455\n\n\n3\nSPY\n0.007152\n0.113542\n\n\n\n\n\n\n\n\nCode Challenge Use .groupby() and .agg() to calculate the average daily return for each of the ETFs.\n\n(\ndf_etf\n    .groupby(['symbol'])[['ret']].agg(np.mean)\n    .reset_index()\n    .rename(columns={'ret':'daily_avg_ret'})\n)\n\n\n\n\n\n\n\n\nsymbol\ndaily_avg_ret\n\n\n\n\n0\nDIA\n0.000650\n\n\n1\nIWM\n-0.001665\n\n\n2\nQQQ\n0.001366\n\n\n3\nSPY\n0.001174"
  },
  {
    "objectID": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#related-reading",
    "href": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#related-reading",
    "title": "7  .groupby() and .agg() - 1",
    "section": "7.6 Related Reading",
    "text": "7.6 Related Reading\nPython Data Science Handbook (VanderPlas) - Section 3.8 - Aggregation and Grouping\nPython for Data Analysis (McKinney) - Chapter 9 (pp 251-274) Data Aggregation and Grouping Operations\nOptions, Futures, and Other Derivatives (Hull) - Chapter 15 (pp 325-329) The Black-Scholes-Merton Model"
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#loading-packages",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#loading-packages",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.1 Loading Packages",
    "text": "8.1 Loading Packages\nLet’s load the packages that we will need.\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr"
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#reading-in-data",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#reading-in-data",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.2 Reading-In Data",
    "text": "8.2 Reading-In Data\nIn this chapter we will be working with price data for the Select Sector SPDR ETFs. Each of these funds tracks a particular subset (sector) of the SP&500 Index. For example, XLF tracks the financial sector and has major holdings in JP Morgan, Wells Fargo, and Bank of America.\nLet’s use pandas_datareader to grab the data from Yahoo Finance.\n\npd.options.display.max_rows = 25\nlst_symbols = ['XLY', 'XLP', 'XLE', 'XLF', 'XLV', 'XLI', 'XLB', 'XLRE', 'XLK', 'XLU',]\ndf_etf = pdr.get_data_yahoo(lst_symbols, start='2020-01-01', end='2020-12-31')\ndf_etf = df_etf.round(2)\ndf_etf.head()\n\n[*********************100%***********************]  10 of 10 completed\n\n\n\n\n\n\n\n\n\nAdj Close\n...\nVolume\n\n\n\nXLB\nXLE\nXLF\nXLI\nXLK\nXLP\nXLRE\nXLU\nXLV\nXLY\n...\nXLB\nXLE\nXLF\nXLI\nXLK\nXLP\nXLRE\nXLU\nXLV\nXLY\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020-01-02\n56.50\n51.06\n28.88\n78.28\n90.25\n57.09\n34.06\n57.12\n96.64\n123.04\n...\n7357400\n11944700\n28843300\n16121300\n13283500\n14460700\n4380100\n19107700\n6277400\n6295500\n\n\n2020-01-03\n55.58\n50.90\n28.58\n78.13\n89.24\n57.00\n34.31\n57.24\n95.80\n122.00\n...\n12423200\n29502900\n51363600\n17571300\n15011800\n26388900\n3499000\n17989300\n8247500\n5596400\n\n\n2020-01-06\n55.34\n51.30\n28.56\n78.15\n89.45\n57.12\n34.32\n57.29\n96.39\n122.34\n...\n15764400\n22458100\n27956100\n16153100\n7815000\n22541700\n3097200\n10444500\n6441800\n6411600\n\n\n2020-01-07\n55.28\n51.16\n28.37\n77.99\n89.41\n56.68\n33.94\n57.21\n96.20\n122.14\n...\n20266900\n11462500\n39627500\n16675400\n7681800\n15607600\n3550600\n13070300\n6335300\n9150800\n\n\n2020-01-08\n55.47\n50.32\n28.56\n78.26\n90.37\n56.89\n34.11\n57.19\n96.83\n122.51\n...\n8079600\n19021400\n47966600\n10677700\n11627200\n11451400\n5089000\n12741400\n7494700\n4725900\n\n\n\n\n5 rows × 60 columns\n\n\n\nThis data is not as tidy as we would like. Let’s use method chaining to perform a series of data munging operations.\n\ndf_etf = \\\n    (\n    df_etf\n        .stack() #pivot the table\n        .reset_index() #turn date into a column\n        .rename(columns={'level_1':'Symbols'}) #renaming a column\n        .sort_values(by=['Symbols', 'Date']) #sort\n        .rename(columns={'Date':'date', 'Symbols':'symbol', 'Adj Close':'adj_close','Close':'close', \n                         'High':'high', 'Low':'low', 'Open':'open', 'Volume':'volume'}) #renaming columns\n        [['date', 'symbol','open', 'high', 'low', 'close', 'volume', 'adj_close']] #reordering columns\n        .reset_index(drop=True)    \n    )\ndf_etf\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\n\n\n\n\n0\n2020-01-02\nXLB\n61.83\n61.94\n60.63\n60.70\n7357400\n56.50\n\n\n1\n2020-01-03\nXLB\n60.08\n60.44\n59.70\n59.72\n12423200\n55.58\n\n\n2\n2020-01-06\nXLB\n59.55\n59.83\n59.41\n59.46\n15764400\n55.34\n\n\n3\n2020-01-07\nXLB\n59.36\n59.80\n59.20\n59.39\n20266900\n55.28\n\n\n4\n2020-01-08\nXLB\n59.40\n59.84\n59.20\n59.60\n8079600\n55.47\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2515\n2020-12-23\nXLY\n157.24\n158.05\n156.57\n157.55\n2173000\n154.46\n\n\n2516\n2020-12-24\nXLY\n157.70\n158.12\n157.21\n157.88\n1048800\n154.79\n\n\n2517\n2020-12-28\nXLY\n159.42\n160.32\n158.60\n159.68\n2912400\n156.55\n\n\n2518\n2020-12-29\nXLY\n160.24\n160.53\n158.98\n159.73\n2431200\n156.60\n\n\n2519\n2020-12-30\nXLY\n160.30\n160.93\n160.13\n160.69\n2440700\n157.54\n\n\n\n\n2520 rows × 8 columns\n\n\n\n\nCoding Challenge: Use a DataFrame attribute to determine the number of rows and columns in df_etf.\n\n\nSolution\ndf_etf.shape\n\n\n(2520, 8)"
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#exploring-and-cleaning-the-data",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#exploring-and-cleaning-the-data",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.3 Exploring and Cleaning the Data",
    "text": "8.3 Exploring and Cleaning the Data\nAs we can see from the coding challenge, this data set is large (by our standards). Whenever I encounter a new data set that I can’t look at in its entirety, I like to do a bit of exploration via the built-in pandas methods.\nWe know we have a variety of ETFs in our data, but it would be useful to know how many (especially if we were expecting a certain number).\n\nprint(df_etf['symbol'].unique())\nprint(df_etf['symbol'].unique().size)\n\n['XLB' 'XLE' 'XLF' 'XLI' 'XLK' 'XLP' 'XLRE' 'XLU' 'XLV' 'XLY']\n10\n\n\n\nCoding Challenge: What DataFrame attribute could we use to check the data types of the columns of df_etf?\n\n\nSolution\ndf_etf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2520 entries, 0 to 2519\nData columns (total 8 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   date       2520 non-null   datetime64[ns]\n 1   symbol     2520 non-null   object        \n 2   open       2520 non-null   float64       \n 3   high       2520 non-null   float64       \n 4   low        2520 non-null   float64       \n 5   close      2520 non-null   float64       \n 6   volume     2520 non-null   int64         \n 7   adj_close  2520 non-null   float64       \ndtypes: datetime64[ns](1), float64(5), int64(1), object(1)\nmemory usage: 157.6+ KB\n\n\n\nWhen I work with a time series of daily prices that I expect to come from a certain date range, I like to check the first and last trade dates that are represented in the data.\n\nprint(df_etf['date'].min())\nprint(df_etf['date'].max())\n\n2020-01-02 00:00:00\n2020-12-30 00:00:00\n\n\nHere is what we know about our data set thus far:\n\n10 different ETFs are represented.\nPrices are coming from the entirety of 2020.\n\nHere are some things that we aren’t necessarily sure of that would be worth checking in a high-stakes situation:\n\nIs there a row/price for each symbol on each trade date?\nIs there ever more than one row/price for a given symbol on a given trade date?\n\nWe won’t bother answering these questions for the purposes of this chapter, but these are the types of data-integrity questions I will often try to answer when encountering a new data set."
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#calculating-daily-returns-with-groupby",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#calculating-daily-returns-with-groupby",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.4 Calculating Daily Returns with groupby()",
    "text": "8.4 Calculating Daily Returns with groupby()\nOur ultimate goal is to calculate monthly returns and monthly volatilities for each ETF in df_etf. These quantities are both functions of daily returns. So our first order of business is to calculate daily returns.\nIn a previous tutorial we calculated daily returns in a simple vectorized fashion. Unfortunately, we can’t use the exact same approach here because there are multiple ETFs in the data set.\nTo overcome this challenge we will use our first application of .groupby().\nHere is the .groupby() code that calculates daily returns for each ETF.\n\n# sorting values to get everything in the right order\ndf_etf.sort_values(['symbol', 'date'], inplace=True)\n\n# vectorized return calculation\ndf_etf['dly_ret'] = \\\n    df_etf['close'].groupby(df_etf['symbol']).pct_change()\ndf_etf.head()\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadj_close\ndly_ret\n\n\n\n\n0\n2020-01-02\nXLB\n61.83\n61.94\n60.63\n60.70\n7357400\n56.50\nNaN\n\n\n1\n2020-01-03\nXLB\n60.08\n60.44\n59.70\n59.72\n12423200\n55.58\n-0.016145\n\n\n2\n2020-01-06\nXLB\n59.55\n59.83\n59.41\n59.46\n15764400\n55.34\n-0.004354\n\n\n3\n2020-01-07\nXLB\n59.36\n59.80\n59.20\n59.39\n20266900\n55.28\n-0.001177\n\n\n4\n2020-01-08\nXLB\n59.40\n59.84\n59.20\n59.60\n8079600\n55.47\n0.003536"
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#adding-year-and-month-columns",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#adding-year-and-month-columns",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.5 Adding year and month Columns",
    "text": "8.5 Adding year and month Columns\nThe ultimate goal is to calculate monthly statistics for each of the ETFs in our data set.\nAs a preliminary step let’s add a month and year column to the df_etf by utilizing the .dt attribute that pandas provides for date columns.\n\ndf_etf['year'] = df_etf['date'].dt.year\ndf_etf['month'] = df_etf['date'].dt.month\ndf_etf[['date', 'year', 'month']].head()\n\n\n\n\n\n\n\n\ndate\nyear\nmonth\n\n\n\n\n0\n2020-01-02\n2020\n1\n\n\n1\n2020-01-03\n2020\n1\n\n\n2\n2020-01-06\n2020\n1\n\n\n3\n2020-01-07\n2020\n1\n\n\n4\n2020-01-08\n2020\n1\n\n\n\n\n\n\n\nLet’s do a quick data-integrity check: There are 10 ETFs in our data set and there are 12 months in a year, so the number of symbol-year-month combinations should be 120.\nThe following code counts the number of rows associated with each symbol-year-month combination and puts that data into a DataFrame.\n\ndf_num_rows = \\\n    df_etf.groupby(['symbol', 'year', 'month']).size().reset_index()\ndf_num_rows.head()\n\n\n\n\n\n\n\n\nsymbol\nyear\nmonth\n0\n\n\n\n\n0\nXLB\n2020\n1\n21\n\n\n1\nXLB\n2020\n2\n19\n\n\n2\nXLB\n2020\n3\n22\n\n\n3\nXLB\n2020\n4\n21\n\n\n4\nXLB\n2020\n5\n20\n\n\n\n\n\n\n\n\nCoding Challenge: Confirm that there are the correct number of symbol-year-month combinations in df_num_rows.\n\n\nSolution\ndf_num_rows.shape\n\n\n(120, 4)\n\n\n\nNow that we’ve added the year and month columns we can proceed to calculating our monthly statistics."
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#average-daily-volume",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#average-daily-volume",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.6 Average Daily Volume",
    "text": "8.6 Average Daily Volume\nLet’s start with the most straight-forward calculation: average daily volume, over each month, for all 10 of the ETFs in our data set.\nThis amounts to:\n\ngrouping by symbol, month, and year\napplying the built-in np.mean() function to the volume column\n\n\ndf_volume = \\\n    df_etf.groupby(['symbol', 'year', 'month'])['volume'].agg([np.mean]).reset_index()\ndf_volume.rename(columns={'mean':'avg_volume'}, inplace=True)\ndf_volume.head()\n\n\n\n\n\n\n\n\nsymbol\nyear\nmonth\navg_volume\n\n\n\n\n0\nXLB\n2020\n1\n7.235429e+06\n\n\n1\nXLB\n2020\n2\n1.058022e+07\n\n\n2\nXLB\n2020\n3\n1.432920e+07\n\n\n3\nXLB\n2020\n4\n9.000557e+06\n\n\n4\nXLB\n2020\n5\n4.829185e+06\n\n\n\n\n\n\n\n\nCoding Challenge: Calculate the maximum daily volume for each symbol, over the entire year.\n\n\nSolution\ndf_etf.groupby(['symbol', 'year'])['volume'].agg([np.max]).reset_index()\n\n\n\n\n\n\n\n\n\nsymbol\nyear\namax\n\n\n\n\n0\nXLB\n2020\n30741700\n\n\n1\nXLE\n2020\n99356700\n\n\n2\nXLF\n2020\n256525000\n\n\n3\nXLI\n2020\n79118200\n\n\n4\nXLK\n2020\n61727100\n\n\n5\nXLP\n2020\n50978800\n\n\n6\nXLRE\n2020\n49899800\n\n\n7\nXLU\n2020\n90263100\n\n\n8\nXLV\n2020\n39561900\n\n\n9\nXLY\n2020\n20616100"
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#monthly-returns",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#monthly-returns",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.7 Monthly Returns",
    "text": "8.7 Monthly Returns\nNext, let’s calculate monthly returns for each of the ETFs in our data set. This amounts to:\n\ngrouping by symbol, month, and year\napplying an aggregation function to the daily_returns column\n\nThese are the same two steps that we have done in our previous aggregation examples. However, there is one additional wrinkle that we are going to have to contend with.\nIn the previous section, we used simple built-in aggregation funtions available through numpy, such as np.max and np.mean. Calculating monthly returns from daily returns is a little more complicated.\nThus, we are going to have to first create a custom function for calculating monthly returns from daily returns, and then use this custom function in .agg().\nThe following code defines our monthly returns function in terms of daily returns:\n\ndef monthly_ret(dly_ret):\n    return np.prod(1 + dly_ret) - 1\n\nNow we can apply our monthly_ret() function for all of our ETFs using the following code.\n\ndf_ret = \\\n    df_etf.groupby(['symbol', 'month', 'year'])['dly_ret'].agg([monthly_ret]).reset_index()\ndf_ret.head()\n\n\n\n\n\n\n\n\nsymbol\nmonth\nyear\nmonthly_ret\n\n\n\n\n0\nXLB\n1\n2020\n-0.050577\n\n\n1\nXLB\n2\n2020\n-0.085199\n\n\n2\nXLB\n3\n2020\n-0.145675\n\n\n3\nXLB\n4\n2020\n0.151865\n\n\n4\nXLB\n5\n2020\n0.068813\n\n\n\n\n\n\n\nWe can see from our calculation that in March of 2020 XLB had a monthly return of -14.6%.\n\nCoding Challenge: Which ETF had the highest single monthly return in all of 2020? What was the month?\n\n\nSolution\ndf_ret.sort_values(by=['monthly_ret'], ascending=False)\n\n\n\n\n\n\n\n\n\nsymbol\nmonth\nyear\nmonthly_ret\n\n\n\n\n15\nXLE\n4\n2020\n0.307639\n\n\n22\nXLE\n11\n2020\n0.279944\n\n\n111\nXLY\n4\n2020\n0.188825\n\n\n34\nXLF\n11\n2020\n0.168483\n\n\n46\nXLI\n11\n2020\n0.160274\n\n\n...\n...\n...\n...\n...\n\n\n74\nXLRE\n3\n2020\n-0.157380\n\n\n20\nXLE\n9\n2020\n-0.159888\n\n\n38\nXLI\n3\n2020\n-0.192529\n\n\n26\nXLF\n3\n2020\n-0.216999\n\n\n14\nXLE\n3\n2020\n-0.358074\n\n\n\n\n120 rows × 4 columns"
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#monthly-volatility",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#monthly-volatility",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.8 Monthly Volatility",
    "text": "8.8 Monthly Volatility\nLet’s use a similar process to calculate the monthly volatility for each of the ETFs.\nWe begin by defining a custom function that calculates the monthly volatility from daily returns. Recall that industry convention is to state these volatilities in annualized terms.\n\ndef monthly_vol(dly_ret):\n    return np.std(dly_ret) * np.sqrt(252)\n\nWe can now use our monthly_vol() function in to perform an aggregating calculation.\n\ndf_vol = \\\n    df_etf.groupby(['symbol', 'month', 'year'])['dly_ret'].agg([monthly_vol]).reset_index()\ndf_vol.head()\n\n\n\n\n\n\n\n\nsymbol\nmonth\nyear\nmonthly_vol\n\n\n\n\n0\nXLB\n1\n2020\n0.150336\n\n\n1\nXLB\n2\n2020\n0.282201\n\n\n2\nXLB\n3\n2020\n0.932265\n\n\n3\nXLB\n4\n2020\n0.503394\n\n\n4\nXLB\n5\n2020\n0.277311\n\n\n\n\n\n\n\n\nCoding Challenge: What was the volatility for XLF in December 2018?\n\n\nSolution\ndf_vol.query('symbol == \"XLF\" & month == 12')\n\n\n\n\n\n\n\n\n\nsymbol\nmonth\nyear\nmonthly_vol\n\n\n\n\n35\nXLF\n12\n2020\n0.137471"
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#combining-metrics---inner-.merge",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#combining-metrics---inner-.merge",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.9 Combining Metrics - inner .merge()",
    "text": "8.9 Combining Metrics - inner .merge()\nNow, suppose that we want to combine our three metrics into one report - meaning that we want them organized into one DataFrame in an easy to read fashion.\nOne way to do this is to use the pandas.merge() method that we learned in the previous tutorial to join together df_volume (average daily volume), df_ret (monthly returns), and df_vol (monthly volatility).\n\ndf_joined = \\\n    (\n    df_volume\n        .merge(df_ret, on=['symbol', 'year', 'month'])\n        .merge(df_vol, on=['symbol', 'year', 'month'])\n    )\ndf_joined.head()\n\n\n\n\n\n\n\n\nsymbol\nyear\nmonth\navg_volume\nmonthly_ret\nmonthly_vol\n\n\n\n\n0\nXLB\n2020\n1\n7.235429e+06\n-0.050577\n0.150336\n\n\n1\nXLB\n2020\n2\n1.058022e+07\n-0.085199\n0.282201\n\n\n2\nXLB\n2020\n3\n1.432920e+07\n-0.145675\n0.932265\n\n\n3\nXLB\n2020\n4\n9.000557e+06\n0.151865\n0.503394\n\n\n4\nXLB\n2020\n5\n4.829185e+06\n0.068813\n0.277311"
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#combining-metrics---multiple-aggregation",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#combining-metrics---multiple-aggregation",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.10 Combining Metrics - multiple aggregation",
    "text": "8.10 Combining Metrics - multiple aggregation\nAnother way to combine all our statistics into a single DataFrame is to supply all of our custom aggregation functions as arguments to the .agg() function at the same time.\nHere is what that looks like:\n\n# defining aggregations\nagg_funcs = \\\n    {'volume':[np.mean], 'dly_ret':[monthly_ret, monthly_vol]}\n\n# performing all aggregations all three aggregations at once\ndf_joined = \\\n    df_etf.groupby(['symbol', 'month', 'year']).agg(agg_funcs).reset_index()\n\n# looking at the data frame\ndf_joined.head()\n\n\n\n\n\n\n\n\nsymbol\nmonth\nyear\nvolume\ndly_ret\n\n\n\n\n\n\nmean\nmonthly_ret\nmonthly_vol\n\n\n\n\n0\nXLB\n1\n2020\n7.235429e+06\n-0.050577\n0.150336\n\n\n1\nXLB\n2\n2020\n1.058022e+07\n-0.085199\n0.282201\n\n\n2\nXLB\n3\n2020\n1.432920e+07\n-0.145675\n0.932265\n\n\n3\nXLB\n4\n2020\n9.000557e+06\n0.151865\n0.503394\n\n\n4\nXLB\n5\n2020\n4.829185e+06\n0.068813\n0.277311\n\n\n\n\n\n\n\nNotice that the input into the .agg() method is a dict whose elements are pairs that look like:\n'column_name':[list_of_aggregating_functions].\n\nCode Challenge: Modify the code above to add maximum daily volume to the report.\n\n\nSolution\n# defining aggregations\nagg_funcs = \\\n    {'volume':[np.mean, np.max], 'dly_ret':[monthly_ret, monthly_vol]}\n\n# performing all aggregations all three aggregations at once\ndf_joined = \\\n    df_etf.groupby(['symbol', 'month', 'year']).agg(agg_funcs).reset_index()\n\n# looking at the data frame\ndf_joined.head()\n\n\n\n\n\n\n\n\n\nsymbol\nmonth\nyear\nvolume\ndly_ret\n\n\n\n\n\n\nmean\namax\nmonthly_ret\nmonthly_vol\n\n\n\n\n0\nXLB\n1\n2020\n7.235429e+06\n20266900\n-0.050577\n0.150336\n\n\n1\nXLB\n2\n2020\n1.058022e+07\n30741700\n-0.085199\n0.282201\n\n\n2\nXLB\n3\n2020\n1.432920e+07\n28390200\n-0.145675\n0.932265\n\n\n3\nXLB\n4\n2020\n9.000557e+06\n30738000\n0.151865\n0.503394\n\n\n4\nXLB\n5\n2020\n4.829185e+06\n7386300\n0.068813\n0.277311"
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#related-reading",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#related-reading",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.11 Related Reading",
    "text": "8.11 Related Reading\nPython Data Science Handbook (VanderPlas) - Section 3.7 - Combining Datasets: Merging and Joining\nPython Data Science Handbook (VanderPlas) - Section 3.8 - Aggregation and Grouping\nPython for Data Analysis (McKinney) - Chapter 9 (pp 251-274) Data Aggregation and Grouping Operations"
  },
  {
    "objectID": "parts/02_part_visualization/part_visualization.html",
    "href": "parts/02_part_visualization/part_visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "Unlike R, in which a single package (ggplot2) has become the de facto standard for visualization, in Python there are a variety of packages that form a visualization ecosystem. The matplotlib package is the foundation of much of this ecosystem, and many elements of this ecosystem can be thought of as wrappers around matplotlib. Such packages include seaborn and also the visualization tools in pandas which we discuss here."
  },
  {
    "objectID": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#load-packages",
    "href": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#load-packages",
    "title": "9  Line Graphs with pandas",
    "section": "9.1 Load Packages",
    "text": "9.1 Load Packages\nLet’s begin by loading the packages that we will need.\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr"
  },
  {
    "objectID": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#ipython-magic-commands",
    "href": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#ipython-magic-commands",
    "title": "9  Line Graphs with pandas",
    "section": "9.2 IPython Magic Commands",
    "text": "9.2 IPython Magic Commands\nNow that we are (implictly) using matplotlib, we will have occasion to use our first IPython magic command. These magic commands are often referred to as simply magics.\nThe following line of code tells Jupyter to print graphs as output just below the code cell - in the same way that other ouput is printed to the screen.\n\n%matplotlib inline\n\nMagics are convenience functions that IPython adds to base Python to make a variety of analysis tasks easier.\nAnother example is %timeit that performs time tests on code.\n\n%timeit L = [n ** 2 for n in range(1000)]\n\n203 µs ± 8.65 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nYou can read more about magics in Section 1.3 of Python Data Science Handbook. The primary magic command that we will utilize is %matplotlib inline."
  },
  {
    "objectID": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#reading-in-data",
    "href": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#reading-in-data",
    "title": "9  Line Graphs with pandas",
    "section": "9.3 Reading In Data",
    "text": "9.3 Reading In Data\nLet’s now use pandas_datareader to read-in SPY and VIX data for 2016Q1 through 2021Q2.\n\ndf_spy = pdr.get_data_yahoo(['SPY', '^VIX'], start='2016-01-01', end='2021-06-30')\ndf_spy = df_spy.round(2)\ndf_spy.head()\n\n[*********************100%***********************]  2 of 2 completed\n\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\n\nSPY\n^VIX\nSPY\n^VIX\nSPY\n^VIX\nSPY\n^VIX\nSPY\n^VIX\nSPY\n^VIX\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-01-04\n175.91\n20.70\n201.02\n20.70\n201.03\n23.36\n198.59\n20.67\n200.49\n22.48\n222353500\n0\n\n\n2016-01-05\n176.21\n19.34\n201.36\n19.34\n201.90\n21.06\n200.05\n19.25\n201.40\n20.75\n110845800\n0\n\n\n2016-01-06\n173.98\n20.59\n198.82\n20.59\n200.06\n21.86\n197.60\n19.80\n198.34\n21.67\n152112600\n0\n\n\n2016-01-07\n169.81\n24.99\n194.05\n24.99\n197.44\n25.86\n193.59\n22.40\n195.33\n23.22\n213436100\n0\n\n\n2016-01-08\n167.95\n27.01\n191.92\n27.01\n195.85\n27.08\n191.58\n22.48\n195.19\n22.96\n209817200\n0\n\n\n\n\n\n\n\nThe following code cleans up the data by isolating the the Close prices, resetting the index, and then changing the column names\n\ndf_spy = df_spy['Close'].reset_index()\ndf_spy.rename(columns={'Date':'date','SPY':'spy','^VIX':'vix'}, inplace=True)\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nspy\nvix\n\n\n\n\n0\n2016-01-04\n201.02\n20.70\n\n\n1\n2016-01-05\n201.36\n19.34\n\n\n2\n2016-01-06\n198.82\n20.59\n\n\n3\n2016-01-07\n194.05\n24.99\n\n\n4\n2016-01-08\n191.92\n27.01"
  },
  {
    "objectID": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#creating-a-basic-price-plot",
    "href": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#creating-a-basic-price-plot",
    "title": "9  Line Graphs with pandas",
    "section": "9.4 Creating a Basic Price Plot",
    "text": "9.4 Creating a Basic Price Plot\npandas was created by Wes McKinney when he was a quantitative analyst at the hedge fund called AQR. One of McKinney’s goals for pandas was to facilitate the analysis of financial time series. For example, plotting stock prices and returns over time can be done very easily.\nThe following single line of code produces a line graph consisting of the close prices of SPY over time.\n\ndf_spy.plot(x='date', y='spy');\n\n\n\n\nNote that .plot() is a DataFrame method.\nNow, for the purposes of exploratory data analysis (EDA), this plot may be all that we need. However, if we needed to share this graph in a publication or presentation, there are a variety of shortcomings that we would need address by utilizing various arguments of the .plot() method."
  },
  {
    "objectID": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#improving-our-graph",
    "href": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#improving-our-graph",
    "title": "9  Line Graphs with pandas",
    "section": "9.5 Improving Our Graph",
    "text": "9.5 Improving Our Graph\nAs a first improvement, let’s add a title to our graph, and add some grid lines to make it a little easier to read.\n\ndf_spy.\\\n    plot(\n        x = 'date',\n        y = 'spy',\n        title = 'SPY: 2016Q1-2021Q2',\n        grid = True,\n    );\n\n\n\n\nIn order to add custom labels to the x-axis and y-axis we will have to work with the matplotlib API. Don’t worry about the details too much right now, just copy this code if you need to relabel your axes.\n\nax = df_spy.\\\n        plot(\n            x = 'date',\n            y = 'spy',\n            title = 'SPY: 2016Q1-2021Q2',\n            grid = True,\n        );\nax.set_xlabel('Trade Date');\nax.set_ylabel('Close Price');\n\n\n\n\n\nCoding Challenge: Copy the code above and then see what the effect is of adding these arguments to .plot():\n1. figsize = (10, 5)\n2. style = 'k--'\n3. alpha = 0.5\n\n\nSolution\nax = df_spy.\\\n        plot(\n            x = 'date',\n            y = 'spy',\n            title = 'SPY: 2016Q1-2021Q2',\n            grid = True,\n            figsize = (10, 5),\n            style = 'k--',\n            alpha = 0.5,\n        );\nax.set_xlabel('Trade Date');\nax.set_ylabel('Close Price');\n\n\n\n\n\n\nFor the remainder of the chapter we will utilize graphs as we would for EDA, so we won’t concern ourselves with titles and labels.\nHowever, whenever graphs are being used to communicate results with a broader audience, they should be properly labeled."
  },
  {
    "objectID": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#subplot-of-price-and-returns",
    "href": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#subplot-of-price-and-returns",
    "title": "9  Line Graphs with pandas",
    "section": "9.6 Subplot of Price and Returns",
    "text": "9.6 Subplot of Price and Returns\nIn this section, we will plot prices and returns as subplots on the same x-axis. This dual plot will be our first observation of the leverage effect: when the stock market suffers losses there is greater volatility.\nLet’s begin by adding a return column to df_spy.\n\ndf_spy['return'] = df_spy['spy'] / df_spy['spy'].shift(1) - 1\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nspy\nvix\nreturn\n\n\n\n\n0\n2016-01-04\n201.02\n20.70\nNaN\n\n\n1\n2016-01-05\n201.36\n19.34\n0.001691\n\n\n2\n2016-01-06\n198.82\n20.59\n-0.012614\n\n\n3\n2016-01-07\n194.05\n24.99\n-0.023992\n\n\n4\n2016-01-08\n191.92\n27.01\n-0.010977\n\n\n\n\n\n\n\nNow we can use the subplots argument of .plot() to plot both the prices and returns simulatneously.\n\ndf_spy.plot(x='date', y=['spy', 'return'], subplots=True, style='k', grid=True, alpha=0.75, figsize=(8, 8),);\n\n\n\n\nIt is easy to confirm visually that when the market goes down, the magnitude of the proximate returns is large, i.e. there is greater volatility. Similarly, during bull markets, returns tend to be smaller in magnitude."
  },
  {
    "objectID": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#realized-volatility",
    "href": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#realized-volatility",
    "title": "9  Line Graphs with pandas",
    "section": "9.7 Realized Volatility",
    "text": "9.7 Realized Volatility\nPlotting the realized volatility - i.e. the rolling standard deviation - is another way to observe the leverage effect.\nIn order to execute rolling calculations in pandas we will use the DataFrame.rolling() method.\n\npd.options.display.max_rows = 6\ndf_spy['return'].rolling(42).std() * np.sqrt(252)\n\n0            NaN\n1            NaN\n2            NaN\n          ...   \n1379    0.118643\n1380    0.118678\n1381    0.117790\nName: return, Length: 1382, dtype: float64\n\n\nNote that the argument of .rolling() is the window size, which we have set to two months.\nLet’s add this rolling realized volatility calculation to df_spy.\n\ndf_spy['realized_vol'] = df_spy['return'].rolling(42).std() * np.sqrt(252)\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nspy\nvix\nreturn\nrealized_vol\n\n\n\n\n0\n2016-01-04\n201.02\n20.70\nNaN\nNaN\n\n\n1\n2016-01-05\n201.36\n19.34\n0.001691\nNaN\n\n\n2\n2016-01-06\n198.82\n20.59\n-0.012614\nNaN\n\n\n3\n2016-01-07\n194.05\n24.99\n-0.023992\nNaN\n\n\n4\n2016-01-08\n191.92\n27.01\n-0.010977\nNaN\n\n\n\n\n\n\n\nAgain, we can use the subplot argument of .plot() to plot all three time series.\n\ndf_spy.plot(x='date', y=['spy', 'return', 'realized_vol',], subplots=True, style='k', grid=True, alpha=0.75, figsize=(8, 12));\n\n\n\n\nNotice that when there is a market downturn, there is a spike in the realized volatility graph."
  },
  {
    "objectID": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#implied-volatility---the-vix-index",
    "href": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#implied-volatility---the-vix-index",
    "title": "9  Line Graphs with pandas",
    "section": "9.8 Implied Volatility - The VIX Index",
    "text": "9.8 Implied Volatility - The VIX Index\nDuring times of market stress, options all become more expensive. One measure of the relative cheapness or expensiveness of options is implied volatility. When options become more expensive, implied volatility rises.\nOne of the complexities of implied volatility measurements is that even for a single underlying they differ depending on strike and expiration. However, all implied volatility measurements tend to rise and fall together.\nThe VIX index is a single number that summarizes the general level of option implied volatility for options on the S&P500. The S&P500 represents a large number of the most important stocks in America. Moreover, S&P500 options are the most actively traded options in the world. For these reasons, the VIX is a good barometer for overall implied volatility level in the stock market.\nLet’s plot vix along side the other volatility measures in the same graph.\n\ndf_spy.plot(x='date', y=['spy', 'return', 'realized_vol', 'vix',], subplots=True, style='k', grid=True, alpha=0.75, figsize=(8, 16));\n\n\n\n\nThis plot demonstrates typical behavior of stock market returns and volatility: when there is a market downturn, there is a spike in both implied volatility and realized volatility."
  },
  {
    "objectID": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#further-reading",
    "href": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#further-reading",
    "title": "9  Line Graphs with pandas",
    "section": "9.9 Further Reading",
    "text": "9.9 Further Reading\nPython Data Science Handbook (VanderPlas) - Section 1.3 - Python Magic Commands\nPython for Finance (Hilpisch) - Section 6.2 - Financial Data\nPython for Data Analysis (McKinney) - Section 8.1 - A Brief matplotlib API Primer\nPython for Data Analysis (McKinney) - Section 8.2 - Plotting Functions in pandas"
  },
  {
    "objectID": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#load-packages",
    "href": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#load-packages",
    "title": "10  Bar Charts with pandas",
    "section": "10.1 Load Packages",
    "text": "10.1 Load Packages\nLet’s begin by loading the packages we need:\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr\n%matplotlib inline\n\nKnowledge Challenge: What is the purpose of this line of code in the above cell: %matplotlib inline?\n\n\nSolution\n# plotting graphs below code cells"
  },
  {
    "objectID": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#reading-in-data",
    "href": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#reading-in-data",
    "title": "10  Bar Charts with pandas",
    "section": "10.2 Reading-In Data",
    "text": "10.2 Reading-In Data\nNext, let’s read in the data from the CSV file.\n\ndf_pnl = pd.read_csv('spy_2018_call_pnl.csv')\ndf_pnl.head()\n\n\n\n\n\n\n\n\nunderlying\nupx\ntype\nexpiration\ndata_date\nstrike\nbid\nask\nimplied_vol\ndelta\ndly_opt_pnl\ndly_dh_pnl\n\n\n\n\n0\nSPY\n266.529999\ncall\n2018-01-19\n2017-12-15\n270\n1.14\n1.16\n0.068257\n0.328344\n-0.02\n0.000000\n\n\n1\nSPY\n268.230011\ncall\n2018-01-19\n2017-12-18\n270\n1.68\n1.69\n0.071450\n0.421353\n-0.53\n0.558189\n\n\n2\nSPY\n267.250000\ncall\n2018-01-19\n2017-12-19\n270\n1.39\n1.41\n0.074841\n0.365808\n0.28\n-0.412931\n\n\n3\nSPY\n267.100006\ncall\n2018-01-19\n2017-12-20\n270\n1.10\n1.11\n0.070911\n0.327058\n0.30\n-0.054869\n\n\n4\nSPY\n267.540009\ncall\n2018-01-19\n2017-12-21\n270\n1.31\n1.32\n0.072183\n0.372113\n-0.21\n0.143906\n\n\n\n\n\n\n\nThis data consists of daily PNLs from 12 different SPY short call trades throughout 2018."
  },
  {
    "objectID": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#wrangling",
    "href": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#wrangling",
    "title": "10  Bar Charts with pandas",
    "section": "10.3 Wrangling",
    "text": "10.3 Wrangling\nFirst, we will refactor the expiration and data_date columns to datetime using the pd.to_datetime() method.\n\ndf_pnl['expiration'] = pd.to_datetime(df_pnl['expiration'])\ndf_pnl['data_date'] = pd.to_datetime(df_pnl['data_date'])\ndf_pnl.head()\n\n\n\n\n\n\n\n\nunderlying\nupx\ntype\nexpiration\ndata_date\nstrike\nbid\nask\nimplied_vol\ndelta\ndly_opt_pnl\ndly_dh_pnl\n\n\n\n\n0\nSPY\n266.529999\ncall\n2018-01-19\n2017-12-15\n270\n1.14\n1.16\n0.068257\n0.328344\n-0.02\n0.000000\n\n\n1\nSPY\n268.230011\ncall\n2018-01-19\n2017-12-18\n270\n1.68\n1.69\n0.071450\n0.421353\n-0.53\n0.558189\n\n\n2\nSPY\n267.250000\ncall\n2018-01-19\n2017-12-19\n270\n1.39\n1.41\n0.074841\n0.365808\n0.28\n-0.412931\n\n\n3\nSPY\n267.100006\ncall\n2018-01-19\n2017-12-20\n270\n1.10\n1.11\n0.070911\n0.327058\n0.30\n-0.054869\n\n\n4\nSPY\n267.540009\ncall\n2018-01-19\n2017-12-21\n270\n1.31\n1.32\n0.072183\n0.372113\n-0.21\n0.143906\n\n\n\n\n\n\n\nWe are interested in total pnl, which is the sum of the option pnl and the delta-hedge PNL. Let’s add a column called dly_tot_pnl which captures this logic.\n\ndf_pnl['dly_tot_pnl'] = df_pnl['dly_opt_pnl'] + df_pnl['dly_dh_pnl']\ndf_pnl.head()\n\n\n\n\n\n\n\n\nunderlying\nupx\ntype\nexpiration\ndata_date\nstrike\nbid\nask\nimplied_vol\ndelta\ndly_opt_pnl\ndly_dh_pnl\ndly_tot_pnl\n\n\n\n\n0\nSPY\n266.529999\ncall\n2018-01-19\n2017-12-15\n270\n1.14\n1.16\n0.068257\n0.328344\n-0.02\n0.000000\n-0.020000\n\n\n1\nSPY\n268.230011\ncall\n2018-01-19\n2017-12-18\n270\n1.68\n1.69\n0.071450\n0.421353\n-0.53\n0.558189\n0.028189\n\n\n2\nSPY\n267.250000\ncall\n2018-01-19\n2017-12-19\n270\n1.39\n1.41\n0.074841\n0.365808\n0.28\n-0.412931\n-0.132931\n\n\n3\nSPY\n267.100006\ncall\n2018-01-19\n2017-12-20\n270\n1.10\n1.11\n0.070911\n0.327058\n0.30\n-0.054869\n0.245131\n\n\n4\nSPY\n267.540009\ncall\n2018-01-19\n2017-12-21\n270\n1.31\n1.32\n0.072183\n0.372113\n-0.21\n0.143906\n-0.066094\n\n\n\n\n\n\n\nAs the final step of our wrangling, let’s extract the year and month of the expiration, as this is what we will use for grouping.\n\ndf_pnl['year'] = df_pnl['expiration'].dt.year\ndf_pnl['month'] = df_pnl['expiration'].dt.month\ndf_pnl.head()\n\n\n\n\n\n\n\n\nunderlying\nupx\ntype\nexpiration\ndata_date\nstrike\nbid\nask\nimplied_vol\ndelta\ndly_opt_pnl\ndly_dh_pnl\ndly_tot_pnl\nyear\nmonth\n\n\n\n\n0\nSPY\n266.529999\ncall\n2018-01-19\n2017-12-15\n270\n1.14\n1.16\n0.068257\n0.328344\n-0.02\n0.000000\n-0.020000\n2018\n1\n\n\n1\nSPY\n268.230011\ncall\n2018-01-19\n2017-12-18\n270\n1.68\n1.69\n0.071450\n0.421353\n-0.53\n0.558189\n0.028189\n2018\n1\n\n\n2\nSPY\n267.250000\ncall\n2018-01-19\n2017-12-19\n270\n1.39\n1.41\n0.074841\n0.365808\n0.28\n-0.412931\n-0.132931\n2018\n1\n\n\n3\nSPY\n267.100006\ncall\n2018-01-19\n2017-12-20\n270\n1.10\n1.11\n0.070911\n0.327058\n0.30\n-0.054869\n0.245131\n2018\n1\n\n\n4\nSPY\n267.540009\ncall\n2018-01-19\n2017-12-21\n270\n1.31\n1.32\n0.072183\n0.372113\n-0.21\n0.143906\n-0.066094\n2018\n1"
  },
  {
    "objectID": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#groupby-and-agg",
    "href": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#groupby-and-agg",
    "title": "10  Bar Charts with pandas",
    "section": "10.4 groupby() and agg()",
    "text": "10.4 groupby() and agg()\nWe are interested in graphing the PNLs by expiration, so let’s sum up the dly_tot_pnl by the year and month of the expiration.\n\ndf_monthly = \\\n    df_pnl.groupby(['year', 'month'])['dly_tot_pnl'].agg([np.sum]).reset_index()\ndf_monthly.head()\n\n\n\n\n\n\n\n\nyear\nmonth\nsum\n\n\n\n\n0\n2018\n1\n0.091963\n\n\n1\n2018\n2\n-2.759090\n\n\n2\n2018\n3\n-0.340270\n\n\n3\n2018\n4\n-1.174222\n\n\n4\n2018\n5\n1.487206\n\n\n\n\n\n\n\nBefore we proceed to graphing, let’s change the name of the aggregated pnl column to something more meaningful.\n\ndf_monthly.rename(columns={'sum':'monthly_pnl'}, inplace=True)"
  },
  {
    "objectID": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#visualizing-the-data",
    "href": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#visualizing-the-data",
    "title": "10  Bar Charts with pandas",
    "section": "10.5 Visualizing the Data",
    "text": "10.5 Visualizing the Data\nCreating a simple bar graph of the monthly_pnls in df_monthly can be done easily with a single line of code.\n\ndf_monthly.plot(x='month', y='monthly_pnl', kind='bar');\n\n\n\n\nWhile the above graph may be fine for EDA purposes, it still leaves much to be desired, especially if our intention is to share it with a broader audience.\nThe following code makes several of modifications to improve its appearance.\n\nax = \\\n    df_monthly.\\\n        plot(\n            x = 'month',\n            y = 'monthly_pnl',\n            kind='bar',\n            color='k', # color is grey\n            grid=True , # adding a grid\n            alpha=0.75, # translucence\n            width=0.8, # increasing the width of the bars\n            title='Monthly PNL for SPY Calls',\n            figsize=(8, 4), # modifying the figure size\n        );\n\nax.set_xlabel(\"Month\"); # x axis label\nax.set_ylabel(\"PNL\");   # y axis label\n\n\n\n\n\nCode Challenge: Google and try to find how you create a horizontal bar graph using pandas.\n\n\nSolution\nax = \\\n    df_monthly.\\\n        plot(\n            x = 'month',\n            y = 'monthly_pnl',\n            kind='barh', # changed to barh\n            color='k', # color is grey\n            grid=True , # adding a grid\n            alpha=0.75, # translucence\n            width=0.8, # increasing the width of the bars\n            title='Monthly PNL for SPY Calls',\n            figsize=(8, 4), # modifying the figure size\n        );\n\nax.set_xlabel(\"Month\"); # x axis label\nax.set_ylabel(\"PNL\");"
  },
  {
    "objectID": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#a-few-words-about-visualization",
    "href": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#a-few-words-about-visualization",
    "title": "10  Bar Charts with pandas",
    "section": "10.6 A Few Words About Visualization",
    "text": "10.6 A Few Words About Visualization\nVisualizing data can be an effective way of communicating results to others, or exploring data on your own. The benefit of visualization comes into focus when we can convey a particular result more quickly and more viscerally with a graph rather than a table of numbers.\nThis is nicely illustrated by comparing our bar graph to the original DataFrame of data. Consider the following question:\nWhat were the two worst PNL months for these SPY calls?\nDo you find it easier to answer the question using the bar graph or the table? Explain why.\n\nax = \\\n    df_monthly.\\\n        plot(\n            x = 'month',\n            y = 'monthly_pnl',\n            kind = 'bar', \n            color='k', # color is grey\n            grid=True, # adding a grid\n            alpha=0.75, # translucence\n            width=0.8, # increasing the width of the bars\n            title='Monthly PNL for SPY Calls',\n            figsize=(8, 4), # modifying the figure size\n        );\n\nax.set_xlabel(\"Month\"); # x axis label\nax.set_ylabel(\"PNL\");   # y axis label\n\n\n\n\n\ndf_monthly\n\n\n\n\n\n\n\n\nyear\nmonth\nmonthly_pnl\n\n\n\n\n0\n2018\n1\n0.091963\n\n\n1\n2018\n2\n-2.759090\n\n\n2\n2018\n3\n-0.340270\n\n\n3\n2018\n4\n-1.174222\n\n\n4\n2018\n5\n1.487206\n\n\n5\n2018\n6\n0.644469\n\n\n6\n2018\n7\n0.516556\n\n\n7\n2018\n8\n0.195526\n\n\n8\n2018\n9\n0.753701\n\n\n9\n2018\n10\n-0.133537\n\n\n10\n2018\n11\n-0.979537\n\n\n11\n2018\n12\n-2.085526"
  },
  {
    "objectID": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#related-reading",
    "href": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#related-reading",
    "title": "10  Bar Charts with pandas",
    "section": "10.7 Related Reading",
    "text": "10.7 Related Reading\nPython for Data Analysis (McKinney) - Section 8.2 - Plotting Functions in pandas"
  },
  {
    "objectID": "chapters/11_pandas_scatter_plots/pandas_scatter_plots.html#loading-packages",
    "href": "chapters/11_pandas_scatter_plots/pandas_scatter_plots.html#loading-packages",
    "title": "11  Scatter Plots with pandas",
    "section": "11.1 Loading Packages",
    "text": "11.1 Loading Packages\nLet’s begin by loading the packages we will need.\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr\n%matplotlib inline"
  },
  {
    "objectID": "chapters/11_pandas_scatter_plots/pandas_scatter_plots.html#reading-in-data",
    "href": "chapters/11_pandas_scatter_plots/pandas_scatter_plots.html#reading-in-data",
    "title": "11  Scatter Plots with pandas",
    "section": "11.2 Reading-In Data",
    "text": "11.2 Reading-In Data\nNext, let’s use pandas_datareader to read in the SPY and VIX data.\n\ndf_spy = pdr.get_data_yahoo(['SPY', '^VIX'], start='2016-01-01', end='2021-06-30')\ndf_spy = df_spy.round(2)\ndf_spy.head()\n\n[*********************100%***********************]  2 of 2 completed\n\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\n\nSPY\n^VIX\nSPY\n^VIX\nSPY\n^VIX\nSPY\n^VIX\nSPY\n^VIX\nSPY\n^VIX\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-01-04\n175.91\n20.70\n201.02\n20.70\n201.03\n23.36\n198.59\n20.67\n200.49\n22.48\n222353500\n0\n\n\n2016-01-05\n176.21\n19.34\n201.36\n19.34\n201.90\n21.06\n200.05\n19.25\n201.40\n20.75\n110845800\n0\n\n\n2016-01-06\n173.98\n20.59\n198.82\n20.59\n200.06\n21.86\n197.60\n19.80\n198.34\n21.67\n152112600\n0\n\n\n2016-01-07\n169.81\n24.99\n194.05\n24.99\n197.44\n25.86\n193.59\n22.40\n195.33\n23.22\n213436100\n0\n\n\n2016-01-08\n167.95\n27.01\n191.92\n27.01\n195.85\n27.08\n191.58\n22.48\n195.19\n22.96\n209817200\n0\n\n\n\n\n\n\n\nThe following code cleans up the data by isolating the the Close prices, resetting the index, and then changing the column names\n\ndf_spy = df_spy['Close'].reset_index()\ndf_spy.rename(columns={'Date':'date','SPY':'spy','^VIX':'vix'}, inplace=True)\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nspy\nvix\n\n\n\n\n0\n2016-01-04\n201.02\n20.70\n\n\n1\n2016-01-05\n201.36\n19.34\n\n\n2\n2016-01-06\n198.82\n20.59\n\n\n3\n2016-01-07\n194.05\n24.99\n\n\n4\n2016-01-08\n191.92\n27.01"
  },
  {
    "objectID": "chapters/11_pandas_scatter_plots/pandas_scatter_plots.html#adding-returns-and-vix-changes-to-df_spy",
    "href": "chapters/11_pandas_scatter_plots/pandas_scatter_plots.html#adding-returns-and-vix-changes-to-df_spy",
    "title": "11  Scatter Plots with pandas",
    "section": "11.3 Adding Returns and VIX Changes to df_spy",
    "text": "11.3 Adding Returns and VIX Changes to df_spy\nLet’s add a return column to df_spy.\n\ndf_spy['return'] = df_spy['spy'] / df_spy['spy'].shift(1) - 1\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nspy\nvix\nreturn\n\n\n\n\n0\n2016-01-04\n201.02\n20.70\nNaN\n\n\n1\n2016-01-05\n201.36\n19.34\n0.001691\n\n\n2\n2016-01-06\n198.82\n20.59\n-0.012614\n\n\n3\n2016-01-07\n194.05\n24.99\n-0.023992\n\n\n4\n2016-01-08\n191.92\n27.01\n-0.010977\n\n\n\n\n\n\n\nNext, let’s calculate the daily change in the VIX, and put it in a new column called vix_chg.\n\ndf_spy['vix_chng'] = df_spy['vix'].diff()\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nspy\nvix\nreturn\nvix_chng\n\n\n\n\n0\n2016-01-04\n201.02\n20.70\nNaN\nNaN\n\n\n1\n2016-01-05\n201.36\n19.34\n0.001691\n-1.36\n\n\n2\n2016-01-06\n198.82\n20.59\n-0.012614\n1.25\n\n\n3\n2016-01-07\n194.05\n24.99\n-0.023992\n4.40\n\n\n4\n2016-01-08\n191.92\n27.01\n-0.010977\n2.02\n\n\n\n\n\n\n\nThe return column in df_spy is expressed as a decimal, so let’s change the vix and vix_chng columns of df_vix to also be expressed as decimals.\n\ndf_spy['vix'] = df_spy['vix'] / 100\ndf_spy['vix_chng'] = df_spy['vix_chng'] / 100\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nspy\nvix\nreturn\nvix_chng\n\n\n\n\n0\n2016-01-04\n201.02\n0.2070\nNaN\nNaN\n\n\n1\n2016-01-05\n201.36\n0.1934\n0.001691\n-0.0136\n\n\n2\n2016-01-06\n198.82\n0.2059\n-0.012614\n0.0125\n\n\n3\n2016-01-07\n194.05\n0.2499\n-0.023992\n0.0440\n\n\n4\n2016-01-08\n191.92\n0.2701\n-0.010977\n0.0202"
  },
  {
    "objectID": "chapters/11_pandas_scatter_plots/pandas_scatter_plots.html#scatter-plot",
    "href": "chapters/11_pandas_scatter_plots/pandas_scatter_plots.html#scatter-plot",
    "title": "11  Scatter Plots with pandas",
    "section": "11.4 Scatter Plot",
    "text": "11.4 Scatter Plot\nNow that we have our data wrangled, we are in position to use the DataFrame.plot.scatter() method to plot daily SPY return against daily changes in the VIX.\n\ndf_spy.plot.scatter('return', 'vix_chng');\n\n\n\n\nThe following code improves the aesthetics of our plot:\n\ndf_spy.plot.scatter(\n    x = 'return',\n    y = 'vix_chng',\n    grid=True ,  \n    c='k',\n    alpha=0.75,\n    s=10,  # changing the size of the dots\n    figsize=(8, 6),\n    title='SPY Return vs VIX Changes (2016Q1-2021Q2: daily)',\n);"
  },
  {
    "objectID": "chapters/12_seaborn/seaborn.html#loading-packages",
    "href": "chapters/12_seaborn/seaborn.html#loading-packages",
    "title": "12  Visualization with seaborn",
    "section": "12.1 Loading Packages",
    "text": "12.1 Loading Packages\nLet’s load the packages that we will be using.\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline"
  },
  {
    "objectID": "chapters/12_seaborn/seaborn.html#line-graph---prices-returns-realized-vol-vix",
    "href": "chapters/12_seaborn/seaborn.html#line-graph---prices-returns-realized-vol-vix",
    "title": "12  Visualization with seaborn",
    "section": "12.2 Line Graph - Prices, Returns, Realized Vol, VIX",
    "text": "12.2 Line Graph - Prices, Returns, Realized Vol, VIX\nLet’s read-in and wrangle some data for SPY and VIX during 2016Q1-2021Q2.\n\ndf_spy = pdr.get_data_yahoo(['SPY', '^VIX'], start='2016-01-01', end='2021-06-30')\ndf_spy = df_spy.round(2)\ndf_spy = df_spy['Close'].reset_index()\ndf_spy.rename(columns={'Date':'date','SPY':'spy','^VIX':'vix'}, inplace=True)\ndf_spy.head()\n\n[*********************100%***********************]  2 of 2 completed\n\n\n\n\n\n\n\n\n\ndate\nspy\nvix\n\n\n\n\n0\n2016-01-04\n201.02\n20.70\n\n\n1\n2016-01-05\n201.36\n19.34\n\n\n2\n2016-01-06\n198.82\n20.59\n\n\n3\n2016-01-07\n194.05\n24.99\n\n\n4\n2016-01-08\n191.92\n27.01\n\n\n\n\n\n\n\nNext, let’s add returns and realized_vol to the df_spy.\n\ndf_spy['return'] = df_spy['spy'] / df_spy['spy'].shift(1) - 1\ndf_spy['realized_vol'] = df_spy['return'].rolling(42).std() * np.sqrt(252)\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nspy\nvix\nreturn\nrealized_vol\n\n\n\n\n0\n2016-01-04\n201.02\n20.70\nNaN\nNaN\n\n\n1\n2016-01-05\n201.36\n19.34\n0.001691\nNaN\n\n\n2\n2016-01-06\n198.82\n20.59\n-0.012614\nNaN\n\n\n3\n2016-01-07\n194.05\n24.99\n-0.023992\nNaN\n\n\n4\n2016-01-08\n191.92\n27.01\n-0.010977\nNaN\n\n\n\n\n\n\n\n\n12.2.1 Graphing with pandas\nRecall that pandas allows us to quickly graph these four time-series in a single figure.\n\ndf_spy. \\\n    plot(\n        x = 'date', \n        y = ['spy', 'return', 'realized_vol', 'vix',],\n        subplots = True,\n        figsize=(8, 12),\n        title='SPY 2016Q1-2021Q2',\n    );\nplt.subplots_adjust(top=0.96); # this adjusts the location of the title\n\n\n\n\n\n\n12.2.2 Graphing with seaborn\nLet’s create similar graphs with seaborn. Creating all four plots in a single graph is not as easy with seaborn and the code is a little confusing. To keep things simple we will recreate two of the plots separately.\nHere is the code that generates the graph of the daily prices.\n\nwith sns.axes_style('darkgrid'):\n    g = sns.relplot(x='date', y='spy', kind='line', data=df_spy, aspect=1.5)\n    g.fig.autofmt_xdate()\n    # creating and tweaking the title\n    g.fig.suptitle('SPY Close Price: 2016Q1-2021Q2')\n    plt.subplots_adjust(top=0.93);\n\n\n\n\nAnd here is the code produces the graph of the daily returns.\n\nwith sns.axes_style('darkgrid'):\n    g = sns.relplot(x='date', y='return', kind='line', data=df_spy, aspect=1.5)\n    g.fig.autofmt_xdate()\n    # creating and tweaking the title\n    g.fig.suptitle('SPY Close Price: 2016Q1-2021Q2');\n    plt.subplots_adjust(top=0.93);"
  },
  {
    "objectID": "chapters/12_seaborn/seaborn.html#bar-graph---monthly-spy-call-pnls",
    "href": "chapters/12_seaborn/seaborn.html#bar-graph---monthly-spy-call-pnls",
    "title": "12  Visualization with seaborn",
    "section": "12.3 Bar Graph - Monthly SPY Call PNLs",
    "text": "12.3 Bar Graph - Monthly SPY Call PNLs\nOur next data set consists of monthly pnls from the call trades detailed in seaborn_monthly_pnl_bar.csv.\n\ndf_monthly_bar = pd.read_csv('seaborn_monthly_pnl_bar.csv')\ndf_monthly_bar.head()\n\n\n\n\n\n\n\n\nmonth\nyear\nmonthly_pnl\n\n\n\n\n0\n1\n2018\n0.091963\n\n\n1\n2\n2018\n-2.759090\n\n\n2\n3\n2018\n-0.340270\n\n\n3\n4\n2018\n-1.174222\n\n\n4\n5\n2018\n1.487206\n\n\n\n\n\n\n\n\n12.3.1 Graphing with pandas\nRecall that this code creates the barplot of the pnls by month using pandas.\n\nax = \\\n    (df_monthly_bar\n        .plot(\n            x = 'month',\n            y = ['monthly_pnl'],\n            kind ='bar',\n            color='k', # color is grey\n            grid=True, # adding a grid\n            alpha=0.75, # translucence\n            width=0.8, # increasing the width of the bars\n            title='Monthly PNL for SPY Calls',\n            figsize=(8, 5), # modifying the figure size\n        ));\n\nax.set_xlabel(\"Month\"); # x-axis label\nax.set_ylabel(\"PNL\");   # y-axis label\n\n\n\n\n\n\n12.3.2 Graphing with seaborn\nHere is the code that produces a similar graph in using seaborn.\n\nwith sns.axes_style('darkgrid'):\n    g = sns.catplot(\n        x='month'\n        , y='monthly_pnl'\n        , kind='bar'\n        , color='black'\n        , alpha=0.75\n        , height=5\n        , aspect = 1.5\n        , data=df_monthly_bar\n    );\n    plt.subplots_adjust(top=0.93);\n    g.fig.suptitle('Monthly PNL for SPY Calls');"
  },
  {
    "objectID": "chapters/12_seaborn/seaborn.html#scatter-plot---spy-returns-vs-vix-change-implied-leverage",
    "href": "chapters/12_seaborn/seaborn.html#scatter-plot---spy-returns-vs-vix-change-implied-leverage",
    "title": "12  Visualization with seaborn",
    "section": "12.4 Scatter Plot - SPY Returns vs VIX Change (implied leverage)",
    "text": "12.4 Scatter Plot - SPY Returns vs VIX Change (implied leverage)\nLet’s add vix_chng to df_spy and change the units to decimals.\n\ndf_spy['vix_chng'] = df_spy['vix'].diff()\ndf_spy['vix'] = df_spy['vix'] / 100\ndf_spy['vix_chng'] = df_spy['vix_chng'] / 100\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nspy\nvix\nreturn\nrealized_vol\nvix_chng\n\n\n\n\n0\n2016-01-04\n201.02\n0.2070\nNaN\nNaN\nNaN\n\n\n1\n2016-01-05\n201.36\n0.1934\n0.001691\nNaN\n-0.0136\n\n\n2\n2016-01-06\n198.82\n0.2059\n-0.012614\nNaN\n0.0125\n\n\n3\n2016-01-07\n194.05\n0.2499\n-0.023992\nNaN\n0.0440\n\n\n4\n2016-01-08\n191.92\n0.2701\n-0.010977\nNaN\n0.0202\n\n\n\n\n\n\n\n\n12.4.1 Graphing with pandas\nHere is the pandas code that creates scatter plot of returns vs VIX changes.\n\ndf_spy.plot.scatter(\n    x = 'return',\n    y = 'vix_chng',\n    grid=True ,  \n    c='k',\n    alpha=0.75,\n    s=10, # changing the size of the dots\n    figsize=(7, 5),\n    title='SPY Return vs VIX Changes: 2016Q1-2021Q2',\n);\n\n\n\n\n\n\n12.4.2 Graphing with seaborn\nHere is the code for a similar graph using seaborn.\n\nwith sns.axes_style('darkgrid'):\n    g = sns.relplot(\n            x = 'return',\n            y = 'vix_chng',\n            data = df_spy,\n            color = 'black',\n            alpha = 0.75,\n            height = 5.5,\n            aspect = 1.3,\n        );\n    plt.subplots_adjust(top=0.93);\n    g.fig.suptitle('SPY Return vs VIX Changes: 2016Q1-2021Q2)');"
  },
  {
    "objectID": "chapters/12_seaborn/seaborn.html#further-reading",
    "href": "chapters/12_seaborn/seaborn.html#further-reading",
    "title": "12  Visualization with seaborn",
    "section": "12.5 Further Reading",
    "text": "12.5 Further Reading\nPython Data Science Handbook - 4.14 - Visualization with Seaborn\nSeaborn Official Tutorials - https://seaborn.pydata.org/tutorial.html (very good, but long)"
  },
  {
    "objectID": "chapters/13_bokeh_basic/bokeh_basic.html#data",
    "href": "chapters/13_bokeh_basic/bokeh_basic.html#data",
    "title": "13  Basic Plotting with bokeh",
    "section": "13.1 Data",
    "text": "13.1 Data\nLet’s begin by loading the data that we will need for our visualizations.\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr\n\n# SPY Close, Returns, Realized Volatility, and VIX\ndf_spy = pdr.get_data_yahoo(['SPY', '^VIX'], start='2016-01-01', end='2021-09-30')\ndf_spy = df_spy.round(2)\ndf_spy = df_spy['Close'].reset_index()\ndf_spy.rename(columns={'Date':'date','SPY':'close','^VIX':'vix'}, inplace=True)\ndf_spy['return'] = np.log(df_spy['close'] / df_spy['close'].shift(1))\ndf_spy['realized_vol'] = df_spy['return'].rolling(42).std() * np.sqrt(252)\n\n# SPY Monthly Returns\ndf_spy['year'] = df_spy['date'].dt.year\ndf_spy['month'] = df_spy['date'].dt.month\ndf_monthly = df_spy.groupby(['year', 'month'], as_index=False)[['return']].sum()\ndf_monthly['year_month'] = (df_monthly['year'] * 100) + df_monthly['month']\ndf_monthly['year_month'] = df_monthly['year_month'].astype(str)\n\n# Implied Leverage Effect\ndf_spy['vix_change'] = df_spy['vix'].diff()\ndf_spy['vix'] = df_spy['vix'] / 100\ndf_spy['vix_change'] = df_spy['vix_change'] / 100\n\n# Asset Allocation - hypothetical allocation through time\ndf_asset_allocation = pd.read_csv('asset_allocation.csv', parse_dates=['trade_date'])\n\n[*********************100%***********************]  2 of 2 completed"
  },
  {
    "objectID": "chapters/13_bokeh_basic/bokeh_basic.html#two-interfaces",
    "href": "chapters/13_bokeh_basic/bokeh_basic.html#two-interfaces",
    "title": "13  Basic Plotting with bokeh",
    "section": "13.2 Two Interfaces",
    "text": "13.2 Two Interfaces\nThere are two interfaces for working with bokeh:\nbokeh.models: a low level interface that gives you complete control over how bokeh creates all elements of your visualization.\nbokeh.plotting: a high level, general-purpose interface that is similar to plotting interfaces of libraries such as Matplotlib or Matlab. It automatically assembles plots with default elements such as axes, grids, and tools for you.\nOur focus will be on the bokeh.plotting interface."
  },
  {
    "objectID": "chapters/13_bokeh_basic/bokeh_basic.html#blank-graph",
    "href": "chapters/13_bokeh_basic/bokeh_basic.html#blank-graph",
    "title": "13  Basic Plotting with bokeh",
    "section": "13.3 Blank Graph",
    "text": "13.3 Blank Graph\nLet’s begin by creating a blank graph, and examine the elements of the code:\nbokeh.io.output_notebook() - output the graph inline in the notebook.\nbokeh.plotting.figure() - creates the basic plot object, called a Figure model.\nbokeh.plotting.show() - outputs the Figure model.\n\n# import bokeh functions\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure, show\n\n# output inline\noutput_notebook()\n\n# create figure() object with a title\nfig = figure(title='Blank Figure')\n\n# output to notebook\nshow(fig)\n\n\n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\nWARNING:bokeh.core.validation.check:W-1000 (MISSING_RENDERERS): Plot has no renderers: figure(id='p1001', ...)\n\n\n\n  \n\n\n\n\n\nNotice the toolbar on the right of the figure above which has a variety of interactive tools. These aren’t that interesting at the moment because our graph is blank."
  },
  {
    "objectID": "chapters/13_bokeh_basic/bokeh_basic.html#basic-line-plot",
    "href": "chapters/13_bokeh_basic/bokeh_basic.html#basic-line-plot",
    "title": "13  Basic Plotting with bokeh",
    "section": "13.4 Basic Line Plot",
    "text": "13.4 Basic Line Plot\nLet’s create our first proper graph by adding a line glyph that plots the SPY close prices.\nNotice that we also modify our Figure object by adding arguments to the figure() function.\nNow the interactive tools are a bit more interesting.\n\n# import bokeh functions\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\n\n# output inline\noutput_notebook()\n\n# create figure() object with a title\np = figure(width=600, height=400, x_axis_type='datetime', title='SPY Close Price',\n           x_axis_label='date', y_axis_label='close price')\n\n# adding a line glyph to display close prices by passing data Series directly\np.line(x=df_spy['date'], y=df_spy['close'])\n\n# output to notebook\nshow(p)\n\n\n    \n        \n        Loading BokehJS ..."
  },
  {
    "objectID": "chapters/13_bokeh_basic/bokeh_basic.html#columndatasource",
    "href": "chapters/13_bokeh_basic/bokeh_basic.html#columndatasource",
    "title": "13  Basic Plotting with bokeh",
    "section": "13.5 ColumnDataSource",
    "text": "13.5 ColumnDataSource\nIn the example above, we passed our data to be graphed directly into the p.line() glyph by specifying particular columns of df_spy, which is a pandas.DataFrame. If you recall, the columns of a DataFrame are pandas.Series objects. We can also pass lists and numpy.arrays directly into a glyph.\nHowever, it is more fruitful to use a ColumnDataSource object. We can easily create one from a DataFrame called df with the following syntax: ColumnDataSource(df).\n\n# import bokeh functions\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource\n\n# output inline\noutput_notebook()\n\n# creating ColumnDataSource from DataFrame\ncds = ColumnDataSource(df_spy)\n\n# creating figure() object\np = figure(width=600, height=400, x_axis_type='datetime', title='SPY Close Price',\n           x_axis_label='date', y_axis_label='close price')\n\n# adding a line glyph to display close prices, passing data via ColumnDataSource\np.line(x='date', y='close',source=cds)\n\n# output to notebook\nshow(p)\n\n\n    \n        \n        Loading BokehJS ..."
  },
  {
    "objectID": "chapters/13_bokeh_basic/bokeh_basic.html#basic-scatter",
    "href": "chapters/13_bokeh_basic/bokeh_basic.html#basic-scatter",
    "title": "13  Basic Plotting with bokeh",
    "section": "13.6 Basic Scatter",
    "text": "13.6 Basic Scatter\nNext, let’s create a basic scatter plot using the .circle() glyph. Notice that we have reformatted the x-axis and y-axis to be expressed as percents.\n\n# import bokeh functions\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.models import NumeralTickFormatter\n\n# output inline\noutput_notebook()\n\n# creating a ColumnDataSource from DataFrame\ncds = ColumnDataSource(df_spy)\n\n# creating figure() object with a title\n# p = figure(plot_width=500, plot_height=500, title='SPY Implied Leverage Effect',\n#            x_axis_label='return', y_axis_label='vix change')\np = figure(width=500, height=500, title='SPY Implied Leverage Effect',\n           x_axis_label='return', y_axis_label='vix change')\n\n# adding circle glyph, passing data via ColumnDataSource\np.circle('return', 'vix_change', source=cds)\n\n# formatting the x-axis and y-axis to percents\np.xaxis.formatter = NumeralTickFormatter(format='0%') \np.yaxis.formatter = NumeralTickFormatter(format='0%')\n\n# output to notebook           \nshow(p)\n\n\n    \n        \n        Loading BokehJS ..."
  },
  {
    "objectID": "chapters/13_bokeh_basic/bokeh_basic.html#other-glyphs",
    "href": "chapters/13_bokeh_basic/bokeh_basic.html#other-glyphs",
    "title": "13  Basic Plotting with bokeh",
    "section": "13.7 Other Glyphs",
    "text": "13.7 Other Glyphs\nWe can recreate this scatter plot with a larger hexes instead of circles. We also modified the outline color and fill color, both of which were affected by arguments to p.hex().\n\n# import bokeh functions\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource\n\n# outpute inline\noutput_notebook()\n\n# creating ColumnDataSource from DataFrame\ncds = ColumnDataSource(df_spy)\n\n# creating figure() object\np = figure(width=600, height=400, title='SPY Implied Leverage Effect',\n           x_axis_label='return', y_axis_label='vix change')\n\n# adding hex glyph, while changing some visual paramenters\np.hex('return', 'vix_change', line_color=\"navy\", fill_color=\"orange\", size=15, source=cds)\n\n# formatting the x-axis and y-axis to percents\np.xaxis.formatter = NumeralTickFormatter(format='0%') \np.yaxis.formatter = NumeralTickFormatter(format='0%')\n\nshow(p)\n\n\n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n13.7.1 Glyphs Gallery\nThere are a wide variety of glyphs that can be used for scatter plots. They are demonstrated below in a visual that I borrowed from the bokeh documentation.\n\nfrom numpy.random import random\nfrom bokeh.io import output_notebook\nfrom bokeh.core.enums import MarkerType\nfrom bokeh.plotting import figure, show\n\n\noutput_notebook()\n\np = figure(title=\"Bokeh Markers\", toolbar_location=None)\np.grid.grid_line_color = None\np.background_fill_color = \"#eeeeee\"\np.axis.visible = False\np.y_range.flipped = True\n\nN = 10\n\nfor i, marker in enumerate(MarkerType):\n    x = i % 4\n    y = (i // 4) * 4 + 1\n\n    p.scatter(random(N)+2*x, random(N)+y, marker=marker, size=14,\n              line_color=\"navy\", fill_color=\"orange\", alpha=0.5)\n\n    p.text(2*x+0.5, y+2.5, text=[marker],\n           text_color=\"firebrick\", text_align=\"center\", text_font_size=\"13px\")\n\nshow(p)\n\n\n    \n        \n        Loading BokehJS ..."
  },
  {
    "objectID": "chapters/13_bokeh_basic/bokeh_basic.html#barchart-with-monthly-returns",
    "href": "chapters/13_bokeh_basic/bokeh_basic.html#barchart-with-monthly-returns",
    "title": "13  Basic Plotting with bokeh",
    "section": "13.8 Barchart with Monthly Returns",
    "text": "13.8 Barchart with Monthly Returns\nHere is a barchart of SPY monthly returns using the p.vbar() renderer.\n\n# import bokeh functions\nfrom bokeh.io import output_notebook\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.models import NumeralTickFormatter\nfrom bokeh.plotting import figure, show\nfrom bokeh.plotting import reset_output\nimport math\n\n# output inline\noutput_notebook()\n\n# creating ColumnDataSource from DataFrame\nsource = ColumnDataSource(df_monthly)\n\n# initializing the figure\np = figure(width=1000, height=400, x_range=df_monthly['year_month'], title='SPY Monthly Returns 2016Q1 to 2021Q3',\n           x_axis_label='month', y_axis_label='monthly return')\n\n# adding vbar glyphs, passing data with ColumnDataSource\np.vbar(x='year_month', bottom=0, top='return', color='blue', width=0.75, source=source)\n\n# formatting the y-axis to percents\np.yaxis.formatter = NumeralTickFormatter(format='0%')\n\n# rotating x-axis labels\np.xaxis.major_label_orientation = math.pi/4\n\n# output graph\nshow(p)\n\n\n    \n        \n        Loading BokehJS ..."
  },
  {
    "objectID": "chapters/13_bokeh_basic/bokeh_basic.html#stacked-areas-with-varying-asset-allocations",
    "href": "chapters/13_bokeh_basic/bokeh_basic.html#stacked-areas-with-varying-asset-allocations",
    "title": "13  Basic Plotting with bokeh",
    "section": "13.9 Stacked Areas with Varying Asset Allocations",
    "text": "13.9 Stacked Areas with Varying Asset Allocations\nNext, we create a stacked area chart to visualize our hypotheticl asset allocation that varies through time. In order to get a useful set of colors, we need to use a palette from the bokeh.palettes module.\n\n# import bokeh functions\nimport bokeh\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.models import NumeralTickFormatter\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.io import output_notebook\nimport pandas as pd\n\n# output inline\noutput_notebook()\n\n# putting data into ColumnDataSource object\nsource = ColumnDataSource(df_asset_allocation)\n\n# initializing the figure\np = figure(width=900, height=500, x_axis_type='datetime', title='Asset Allocation 2019-2020',\n           x_axis_label='month', y_axis_label='allocation')\n\n# choosing assets and colors to graph\nassets = df_asset_allocation.drop(columns='trade_date').columns\nnum_assets = len(assets)\ncolors = bokeh.palettes.magma(num_assets) # choosing color palette\n\n# adding glyph with legend\np.varea_stack(assets, x='trade_date', color=colors, source=source, legend_label=assets.to_list())\n\n# reversing order of legend to match order of graph\np.legend[0].items.reverse()\n\n# reformatting y-axis as percent \np.yaxis.formatter = NumeralTickFormatter(format='0%')\n\n# moving legend off of the graph\np.add_layout(p.legend[0], 'right')\n\n# output graph\nshow(p)\n\n\n    \n        \n        Loading BokehJS ..."
  },
  {
    "objectID": "chapters/13_bokeh_basic/bokeh_basic.html#layouts",
    "href": "chapters/13_bokeh_basic/bokeh_basic.html#layouts",
    "title": "13  Basic Plotting with bokeh",
    "section": "13.10 Layouts",
    "text": "13.10 Layouts\nLayout functions let you build a grid of plots and widgets. You can have as many rows, columns, or grids of plots in one layout as you like.\nWe will use the column() function to vertically stack our leverage effect line graphs. Notice that each graph has its own set of interactive tools because they are not linked together. We will remedy this in the next notebook.\n\n# import bokeh functions\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.layouts import column\n\n# output inline\noutput_notebook()\n\n# defining multiple plots\np1 = figure(width=600, height=200, x_axis_type='datetime', title='SPY Leverage Effecct')\np1.line(x=df_spy['date'], y=df_spy['close'])\n\np2 = figure(width=600, height=200, x_axis_type='datetime')\np2.line(x=df_spy['date'], y=df_spy['return'])\np2.yaxis.formatter = NumeralTickFormatter(format='0%')\n\np3 = figure(width=600, height=200, x_axis_type='datetime')\np3.line(x=df_spy['date'], y=df_spy['realized_vol'])\np3.yaxis.formatter = NumeralTickFormatter(format='0%')\n\np4 = figure(width=600, height=200, x_axis_type='datetime')\np4.line(x=df_spy['date'], y=df_spy['vix'])\np4.yaxis.formatter = NumeralTickFormatter(format='0%')\n\n# putting them together using column()\nshow(column(p1, p2, p3, p4))\n\n\n    \n        \n        Loading BokehJS ..."
  },
  {
    "objectID": "chapters/14_bokeh_advanced/bokeh_advanced.html#data",
    "href": "chapters/14_bokeh_advanced/bokeh_advanced.html#data",
    "title": "14  Advanced Plotting with bokeh",
    "section": "14.1 Data",
    "text": "14.1 Data\nLet’s begin by loading the data that we will need for our visualizations.\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr\n\n# SPY Close, Returns, Realized Volatility, and VIX\ndf_spy = pdr.get_data_yahoo(['SPY', '^VIX'], start='2016-01-01', end='2021-09-30')\ndf_spy = df_spy.round(2)\ndf_spy = df_spy['Close'].reset_index()\ndf_spy.rename(columns={'Date':'date','SPY':'close','^VIX':'vix'}, inplace=True)\ndf_spy['return'] = np.log(df_spy['close'] / df_spy['close'].shift(1))\ndf_spy['realized_vol'] = df_spy['return'].rolling(42).std() * np.sqrt(252)\n\n# SPY Monthly Returns\ndf_spy['year'] = df_spy['date'].dt.year\ndf_spy['month'] = df_spy['date'].dt.month\ndf_monthly = df_spy.groupby(['year', 'month'], as_index=False)[['return']].sum()\ndf_monthly['year_month'] = (df_monthly['year'] * 100) + df_monthly['month']\ndf_monthly['year_month'] = df_monthly['year_month'].astype(str)\n\n# Implied Leverage Effect\ndf_spy['vix_change'] = df_spy['vix'].diff()\ndf_spy['vix'] = df_spy['vix'] / 100\ndf_spy['vix_change'] = df_spy['vix_change'] / 100\n\n# Asset Allocation - hypothetical allocation through time\ndf_asset_allocation = pd.read_csv('asset_allocation.csv', parse_dates=['trade_date'])\n\n[*********************100%***********************]  2 of 2 completed"
  },
  {
    "objectID": "chapters/14_bokeh_advanced/bokeh_advanced.html#linked-panning-with-gridplot",
    "href": "chapters/14_bokeh_advanced/bokeh_advanced.html#linked-panning-with-gridplot",
    "title": "14  Advanced Plotting with bokeh",
    "section": "14.2 Linked Panning with gridplot()",
    "text": "14.2 Linked Panning with gridplot()\nIn a previous chapter, we created a column() of plots to visualize the leverage effect. However, the plots were all independent of one another, which may not be desireable.\nWe remedy that here using the gridplot() function. We also modify the x_range inputs of all of our figures so that they link properly.\nNotice that we now have a single toolbar for all four plots.\n\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.layouts import gridplot\nfrom bokeh.models import NumeralTickFormatter\n\noutput_notebook()\n\n# defining plot options all at once\nplot_options = dict(width=600, height=200, x_axis_type='datetime')\n\n# defining multiple plots - notice the change in the x_range for p2, p3, p3\np1 = figure(**plot_options, title='SPY Leverage Effect')\np1.line(x=df_spy['date'], y=df_spy['close'], legend_label='close')\np1.legend.location = 'top_left'\n\np2 = figure(x_range=p1.x_range,  **plot_options)\np2.line(x=df_spy['date'], y=df_spy['return'], legend_label='return')\np2.yaxis.formatter = NumeralTickFormatter(format='0%')\np2.legend.location = 'top_left'\n\np3 = figure(x_range=p1.x_range, **plot_options)\np3.line(x=df_spy['date'], y=df_spy['realized_vol'], legend_label='realized volatility')\np3.yaxis.formatter = NumeralTickFormatter(format='0%')\np3.legend.location = 'top_left'\n\np4 = figure(x_range=p1.x_range, **plot_options)\np4.line(x=df_spy['date'], y=df_spy['vix'], legend_label='vix')\np4.yaxis.formatter = NumeralTickFormatter(format='0%')\np4.legend.location = 'top_left'\n\n# putting all plots into a grid plot\np = gridplot([[p1], \n              [p2], \n              [p3],\n              [p4]]\n            )\n\nshow(p)\n\n\n    \n        \n        Loading BokehJS ..."
  },
  {
    "objectID": "chapters/14_bokeh_advanced/bokeh_advanced.html#specifying-tools",
    "href": "chapters/14_bokeh_advanced/bokeh_advanced.html#specifying-tools",
    "title": "14  Advanced Plotting with bokeh",
    "section": "14.3 Specifying Tools",
    "text": "14.3 Specifying Tools\nWe can also specify the interactive tools that we want in our graph.\nThe easiest way to do this is the tools argument of the figure() function which takes values that are comma delimited strings such as 'reset,hover,save'.\n\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.models import NumeralTickFormatter\n\noutput_notebook()\n\ncds = ColumnDataSource(df_spy)\n\n# creating custom tool list\ntools = 'box_select,box_zoom,lasso_select,pan,wheel_zoom,reset,hover,save'\n\n# adding custom tool list to figurw\np = figure(width=600, height=400, tools=tools, title='SPY Implied Leverage Effect',\n           x_axis_label='return', y_axis_label='vix change')\n\n# adding glyph\np.hex('return', 'vix_change', line_color=\"navy\", fill_color=\"orange\", size=15, source=cds)\n\n# formatting the x-axis and y-axis to percents\np.xaxis.formatter = NumeralTickFormatter(format='0%') \np.yaxis.formatter = NumeralTickFormatter(format='0%')\n\nshow(p)\n\n\n    \n        \n        Loading BokehJS ..."
  },
  {
    "objectID": "chapters/14_bokeh_advanced/bokeh_advanced.html#linked-properties",
    "href": "chapters/14_bokeh_advanced/bokeh_advanced.html#linked-properties",
    "title": "14  Advanced Plotting with bokeh",
    "section": "14.4 Linked Properties",
    "text": "14.4 Linked Properties\nWe can also link properities of the graph to widgets.\nThe example below allows us to modify the size of the .hex glyphs so we can dial in the appearance of our graph.\n\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.models import Slider\nfrom bokeh.layouts import column\nfrom bokeh.models import NumeralTickFormatter\n\noutput_notebook()\n\ncds = ColumnDataSource(df_spy)\n\np = figure(width=600, height=400, title='SPY Implied Leverage Effect',\n           x_axis_label='return', y_axis_label='vix change')\n\n# adding glyph\nr = p.hex('return', 'vix_change', line_color=\"navy\", fill_color=\"orange\", size=10, source=cds)\n\n# linking slider to  the size of the hex glyph \nslider = Slider(start=1, end=20, step=1, value=10)\nslider.js_link('value', r.glyph, 'size')\n\n# formatting the x-axis and y-axis to percents\np.xaxis.formatter = NumeralTickFormatter(format='0%') \np.yaxis.formatter = NumeralTickFormatter(format='0%')\n\nshow(column(p, slider))\n\n\n    \n        \n        Loading BokehJS ..."
  },
  {
    "objectID": "chapters/14_bokeh_advanced/bokeh_advanced.html#linked-brushing",
    "href": "chapters/14_bokeh_advanced/bokeh_advanced.html#linked-brushing",
    "title": "14  Advanced Plotting with bokeh",
    "section": "14.5 Linked Brushing",
    "text": "14.5 Linked Brushing\nLinked brushing allows us to highlight related data between two graphs.\n\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.layouts import gridplot\nfrom bokeh.models import NumeralTickFormatter\n\noutput_notebook()\n\ncds = ColumnDataSource(df_spy)\n\n# defining plot options all at once, passing custom tool list directly\nplot_options = dict(width=500, height=350, tools='box_zoom,lasso_select,reset, box_select')\n\np1 = figure(**plot_options, title='SPY Leverage Effect', x_axis_label='return', y_axis_label='vix change')\np1.hex('return', 'vix_change', line_color=\"navy\", fill_color=\"orange\", size=10, source=cds)\n# formatting the x-axis and y-axis to percents\np1.xaxis.formatter = NumeralTickFormatter(format='0%') \np1.yaxis.formatter = NumeralTickFormatter(format='0%')\n\np2 = figure(**plot_options, x_range=p1.x_range, title='Return vs Realized Vol',\n            x_axis_label='return', y_axis_label='realize vol')\np2.hex('return', 'realized_vol', line_color=\"navy\", fill_color=\"red\", size=10, source=cds)\n# formatting the x-axis and y-axis to percents\np2.xaxis.formatter = NumeralTickFormatter(format='0%') \np2.yaxis.formatter = NumeralTickFormatter(format='0%')\n\np = gridplot([[p1,p2]])\nshow(p)\n\n\n    \n        \n        Loading BokehJS ..."
  },
  {
    "objectID": "chapters/14_bokeh_advanced/bokeh_advanced.html#hover-tool",
    "href": "chapters/14_bokeh_advanced/bokeh_advanced.html#hover-tool",
    "title": "14  Advanced Plotting with bokeh",
    "section": "14.6 Hover Tool",
    "text": "14.6 Hover Tool\nHover tools can be helpful for making large visualizations more readable. Here we add one to our monthly returns bar chart.\n\nfrom bokeh.io import output_notebook\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.plotting import figure, show\nfrom bokeh.plotting import reset_output\nfrom bokeh.models import NumeralTickFormatter\nimport math\nreset_output()\n\noutput_notebook()\n\nsource = ColumnDataSource(df_monthly)\n\n# defining custom tool list\ntools = 'box_zoom, reset'\n\n# defining tool tip\ntooltips = [\n    ('month', '@year_month'),\n    ('return', '@return{0.0%}'),\n]\n\np = figure(width=1000, height=400, x_range=df_monthly['year_month'], tools=[tools], tooltips=tooltips,\n           title='SPY Monthly Returns 2016Q1 to 2021Q3', x_axis_label='month', y_axis_label='monthly return')\np.vbar(x='year_month', bottom=0, top='return', color='blue', width=0.75, source=source)\n\n# formatting the y-axis to percents\np.yaxis.formatter = NumeralTickFormatter(format='0%')\n\np.xaxis.major_label_orientation = math.pi/4\n\nshow(p)\n\n\n    \n        \n        Loading BokehJS ..."
  },
  {
    "objectID": "chapters/14_bokeh_advanced/bokeh_advanced.html#hover-tool-interactive-legend",
    "href": "chapters/14_bokeh_advanced/bokeh_advanced.html#hover-tool-interactive-legend",
    "title": "14  Advanced Plotting with bokeh",
    "section": "14.7 Hover Tool + Interactive Legend",
    "text": "14.7 Hover Tool + Interactive Legend\nHere we add a hover tool to our asset allocation stacked area plot.\nWe also make the legend interactive to be able to focus in on specific allocation buckets.\n\nimport bokeh\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.models import NumeralTickFormatter\nfrom bokeh.models import HoverTool\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.io import output_notebook\n\noutput_notebook()\n\nsource = ColumnDataSource(df_asset_allocation)\n\np = figure(width=900, height=500, x_axis_type='datetime', title='Asset Allocation 2019-2020',\n           x_axis_label='month', y_axis_label='allocation')\n\n# choosing assets and colors to graph\nassets = df_asset_allocation.drop(columns='trade_date').columns\nnum_assets = len(assets)\ncolors = bokeh.palettes.magma(num_assets) # choosing color palette\n\n# adding glyphs\np.varea_stack(assets, x='trade_date', color=colors, source=source, legend_label=assets.to_list())\np.vline_stack(assets.to_list(), x='trade_date', color=colors, source=source,)\n\n# defining tool tip\np.add_tools(HoverTool(\n    tooltips = [\n        (\"Trade Date\", \"@trade_date{%F}\"),\n        (\"VXX\", \"@VXX{0%}\"),\n        (\"DBA\", \"@DBA{0%}\"),\n        (\"USO\", \"@USO{0%}\"),\n        (\"HYG\", \"@HYG{0%}\"),\n        (\"TLT\", \"@TLT{0%}\"),\n        (\"IWM\", \"@IWM{0%}\"),\n        (\"SPY\", \"@SPY{0%}\"),\n    ],\n    formatters={\n        '@trade_date':'datetime', # use 'datetime' formatter for 'date' field\n    },\n\n))\n\n# interactive legend\np.legend.click_policy='hide'\n\n# reformatting \np.yaxis.formatter = NumeralTickFormatter(format='0%')\n\n# reversing ordering of legend to make it consistent with stacking order\np.legend[0].items.reverse()\n\n# moving legend off of the graph\np.add_layout(p.legend[0], 'right')\n\nshow(p)\n\n\n    \n        \n        Loading BokehJS ..."
  },
  {
    "objectID": "chapters/15_option_replication/option_replication.html#loading-packages",
    "href": "chapters/15_option_replication/option_replication.html#loading-packages",
    "title": "15  Black-Scholes-Merton Option Replication",
    "section": "15.1 Loading Packages",
    "text": "15.1 Loading Packages\nLet’s begin by loading the packages we will need.\n\nimport numpy as np\nimport pandas as pd\npd.options.display.max_rows = 10\n\nAdditionally, we will also require several functions from the py_vollib package for calculating option greeks.\n\nfrom py_vollib.black_scholes_merton import black_scholes_merton\nfrom py_vollib.black_scholes_merton.greeks.analytical import delta\nfrom py_vollib.black_scholes_merton.greeks.analytical import vega\nfrom py_vollib.black_scholes_merton.implied_volatility import implied_volatility"
  },
  {
    "objectID": "chapters/15_option_replication/option_replication.html#converting-py_vollib-functions",
    "href": "chapters/15_option_replication/option_replication.html#converting-py_vollib-functions",
    "title": "15  Black-Scholes-Merton Option Replication",
    "section": "15.2 Converting py_vollib Functions",
    "text": "15.2 Converting py_vollib Functions\nIn this section, we convert several of the py_vollib functions we imported above so that they can accept a row of a DataFrame as their argument. This will allow us to use these function with the DataFrame.apply() method, which we will use to write compact vectorized code. Notice that we assume a zero risk-free rate, and zero dividend yield throughout this tutorial, and thus those values are hardcoded into these functions..\n\ndef bsm_px(row):\n    cp = row['cp']\n    upx = row['upx']\n    strike = row['strike']\n    t2x = row['t2x']\n    rf = 0\n    volatility = row['volatility']\n    q = 0\n    px = black_scholes_merton(cp, upx, strike, t2x, rf, volatility, q)\n    px = np.round(px, 2)\n    return(px)\n\n\ndef bsm_delta(row):\n    cp = row['cp']\n    upx = row['upx']\n    strike = row['strike']\n    t2x = row['t2x']\n    rf = 0\n    volatility = row['volatility']\n    q = 0\n    if t2x == 0:\n        return(0)\n    diff = delta(cp, upx, strike, t2x, rf, volatility, q)\n    diff = np.round(diff, 3)\n    return(diff)\n\n\ndef bsm_vega(row):\n    cp = row['cp']\n    upx = row['upx']\n    strike = row['strike']\n    t2x = row['t2x']\n    rf = 0\n    volatility = row['volatility']\n    q = 0\n    if t2x == 0:\n        return(0)\n    vga = vega(cp, upx, strike, t2x, rf, volatility, q)\n    vga = np.round(vga, 3)\n    return(vga)"
  },
  {
    "objectID": "chapters/15_option_replication/option_replication.html#geometric-brownian-motion",
    "href": "chapters/15_option_replication/option_replication.html#geometric-brownian-motion",
    "title": "15  Black-Scholes-Merton Option Replication",
    "section": "15.3 Geometric Brownian Motion",
    "text": "15.3 Geometric Brownian Motion\nThe series of trade prices for a stock is often modeled as a series of random variables, which is also referred to as a stochastic process. There many types of stochastic processes; some of them resemble actual stock price movements better than others.\nThe Black-Scholes-Merton option pricing framework assumes that the price process of the underlying asset follows a geometric brownian motion (GBM). This means that:\n\nThe price process is continuous.\nThe log return over any period of time is normally distributed.\nThe returns during any two disjoint periods are independent.\n\nGBMs are one of the simplest types of processes that reasonably model asset price dynamics, so it’s often a good place to start when learning about simulating stock price data.\nThe price process of a geometric brownian motion is determined by the current risk-free rate \\(r\\) and the annualized volatility of the underlying \\(\\sigma\\). Prices that are separated by \\(\\Delta t\\) units of time are related by following equation:\n\\[S_{t} =  S_{t - \\Delta t} \\cdot \\exp\\bigg(\\bigg(r - \\frac{1}{2}\\sigma^2\\bigg)\\Delta t + \\sigma \\sqrt{\\Delta t} z_{t}\\bigg)\\]\nwhere \\(z_{t}\\) is a standard normal random variable.\nThis is called the Euler discretization of a GBM. It will serve as the recipe for our price-path simulation algorithm. Note that the expression in the parentheses is the log-return of the stock between time \\(t - \\Delta t\\) and \\(t\\).\nAlthough the GBM assumptions are often violated in actual prices, there is still enough truth in them that the Black-Scholes-Merton manufacturing process is practically useful. It prescribes a process that derivative dealers can use to construct the contracts that their customers are interested in. This manufacturing process is referred to as constructing a replicating portfolio, which in the case of vanilla option is accomplished via a dynamic trading strategy of the underlying asset. This dynamic strategy is called delta-hedging.\n\nDiscussion Question: Put-call parity is a pricing identity that emanates from a static manufacturing (replication) strategy of a certain type of derivative. Describe the strategy: what kind of derivative does this replication strategy manufacture, and what is the resulting pricing identity.\n\n\nSolution\n##&gt; A forward contract can be manufactured/replicated with a \n##&gt; portfolio consisting of long call position plus a short put \n##&gt; position, both with the same strike, call it K.  Thus, the \n##&gt; put-call parity identity is: c(K, T) - p(K, T) = S - Ke^(-rT)."
  },
  {
    "objectID": "chapters/15_option_replication/option_replication.html#the-option-we-will-analyze",
    "href": "chapters/15_option_replication/option_replication.html#the-option-we-will-analyze",
    "title": "15  Black-Scholes-Merton Option Replication",
    "section": "15.4 The Option We Will Analyze",
    "text": "15.4 The Option We Will Analyze\nWe want to analyze what it means for a derivatives dealer to trade an option and then delta-hedge the position. Let’s consider an option that actually traded in the market place:\n\nunderlying: QQQ\ncurrent date: 11/16/2018\nexpiration: 12/21/2018\ntype: put\nstrike: 160\nupx: 168\ndays-to-expiration (d2x): 24\nprice: 2.25\n\nFrom this trade price, we can calculate an implied volatility, which we will also refer to as the pricing volatility. (Note that this is the typical flow of events, the price of an option is observed and from that observed price, an implied volatility is calculated.)\n\npricing_vol = implied_volatility(price = 2.25, S = 168, K = 160, t = 24/252, r = 0, q = 0, flag = 'p')\npricing_vol = np.round(pricing_vol, 4)\npricing_vol\n\n0.2636"
  },
  {
    "objectID": "chapters/15_option_replication/option_replication.html#delta-hedging-a-single-simulated-underlying-price-path",
    "href": "chapters/15_option_replication/option_replication.html#delta-hedging-a-single-simulated-underlying-price-path",
    "title": "15  Black-Scholes-Merton Option Replication",
    "section": "15.5 Delta-Hedging: A Single Simulated Underlying Price Path",
    "text": "15.5 Delta-Hedging: A Single Simulated Underlying Price Path\nIt is typical that a derivatives dealer will trade the above option with a customer, and then hold that option option until expiration and delta hedge it on a daily basis.\nThe BSM manufacturing framework states that the dealer will break even if:\n\nthe underlying price follows a geometric brownian motion\nthe realized volatility during the life of the option is equal to the implied volatility used to price the option\nthe dealer delta-hedges with frequent rebalancing (in order for the result to be deterministic the delta-hedging must be continuous)\n\nIn this section we are explore what this manufacturing process looks like for a particular price path of the underlying. In order to do this, let’s simulate a single geometric brownian motion path whose realized volatility is equal to the pricing volatility of our QQQ option. This price path will consist of a series of daily prices that starts with 168, the spot price at the time of the trade. We will rebalance the delta-hedge daily.\nThe following code generates the price path.\n\n# setting the random seed\nnp.random.seed(1)\n\n# parameters of simulation\nr = 0\npath_vol = pricing_vol\ndt = 1./252\n\n# initializing paths\nsingle_path = np.zeros(25)\nsingle_path[0] = 168\n\n# looping through days and generating steps in the paths\nfor t in range(1, 25):\n    z = np.random.standard_normal(1)\n    single_path[t] = single_path[t - 1] * np.exp((r - 0.5 * path_vol ** 2) * dt + path_vol * np.sqrt(dt) * z) # memorize this line\n    single_path[t] = np.round(single_path[t], 2)\n\nLet’s take a look at the path we generated. (Obviously, in a real-world situation this price path would be realized over the course of the life of the option.)\n\nsingle_path\n\narray([168.  , 172.57, 170.8 , 169.29, 166.28, 168.66, 162.31, 167.06,\n       164.94, 165.79, 165.08, 169.11, 163.4 , 162.51, 161.45, 164.5 ,\n       161.5 , 161.02, 158.67, 158.76, 160.28, 157.36, 160.36, 162.76,\n       164.1 ])\n\n\nNext, let’s create a DataFrame that will track the PNL from delta-hedging the option; this DataFrame will contain all the information needed to calculate the price and greeks of the option on a daily basis.\n\ndf_path = \\\n    (\n    pd.DataFrame(\n        {'underlying':'QQQ',\n         'cp':'p',\n         'strike':160,\n         'volatility':0.2636,\n         'upx':single_path, \n         'd2x':list(range(24, -1, -1)),\n         'buy_sell':1,\n        }       \n    )\n    .assign(t2x = lambda df: df.d2x / 252)\n    )\ndf_path\n\n\n\n\n\n\n\n\nunderlying\ncp\nstrike\nvolatility\nupx\nd2x\nbuy_sell\nt2x\n\n\n\n\n0\nQQQ\np\n160\n0.2636\n168.00\n24\n1\n0.095238\n\n\n1\nQQQ\np\n160\n0.2636\n172.57\n23\n1\n0.091270\n\n\n2\nQQQ\np\n160\n0.2636\n170.80\n22\n1\n0.087302\n\n\n3\nQQQ\np\n160\n0.2636\n169.29\n21\n1\n0.083333\n\n\n4\nQQQ\np\n160\n0.2636\n166.28\n20\n1\n0.079365\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20\nQQQ\np\n160\n0.2636\n160.28\n4\n1\n0.015873\n\n\n21\nQQQ\np\n160\n0.2636\n157.36\n3\n1\n0.011905\n\n\n22\nQQQ\np\n160\n0.2636\n160.36\n2\n1\n0.007937\n\n\n23\nQQQ\np\n160\n0.2636\n162.76\n1\n1\n0.003968\n\n\n24\nQQQ\np\n160\n0.2636\n164.10\n0\n1\n0.000000\n\n\n\n\n25 rows × 8 columns\n\n\n\nWe can now use the bsm_px() function to calculate the prices of the option for each day in the simulation. At a derivatives dealer, the option value and greeks would be monitored in real-time in a position management system.\n\ndf_path['option_price'] = df_path[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_px, axis = 1)\ndf_path\n\n\n\n\n\n\n\n\nunderlying\ncp\nstrike\nvolatility\nupx\nd2x\nbuy_sell\nt2x\noption_price\n\n\n\n\n0\nQQQ\np\n160\n0.2636\n168.00\n24\n1\n0.095238\n2.25\n\n\n1\nQQQ\np\n160\n0.2636\n172.57\n23\n1\n0.091270\n1.21\n\n\n2\nQQQ\np\n160\n0.2636\n170.80\n22\n1\n0.087302\n1.44\n\n\n3\nQQQ\np\n160\n0.2636\n169.29\n21\n1\n0.083333\n1.67\n\n\n4\nQQQ\np\n160\n0.2636\n166.28\n20\n1\n0.079365\n2.33\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20\nQQQ\np\n160\n0.2636\n160.28\n4\n1\n0.015873\n1.98\n\n\n21\nQQQ\np\n160\n0.2636\n157.36\n3\n1\n0.011905\n3.44\n\n\n22\nQQQ\np\n160\n0.2636\n160.36\n2\n1\n0.007937\n1.33\n\n\n23\nQQQ\np\n160\n0.2636\n162.76\n1\n1\n0.003968\n0.21\n\n\n24\nQQQ\np\n160\n0.2636\n164.10\n0\n1\n0.000000\n0.00\n\n\n\n\n25 rows × 9 columns\n\n\n\nLet’s calculate the deltas through time. Notice that as the price of the underlying goes down the (absolute) delta of the put increases, and as the price of the underlying goes up the delta decreases.\n\ndf_path['delta'] = df_path[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_delta, axis = 1)\ndf_path\n\n\n\n\n\n\n\n\nunderlying\ncp\nstrike\nvolatility\nupx\nd2x\nbuy_sell\nt2x\noption_price\ndelta\n\n\n\n\n0\nQQQ\np\n160\n0.2636\n168.00\n24\n1\n0.095238\n2.25\n-0.261\n\n\n1\nQQQ\np\n160\n0.2636\n172.57\n23\n1\n0.091270\n1.21\n-0.161\n\n\n2\nQQQ\np\n160\n0.2636\n170.80\n22\n1\n0.087302\n1.44\n-0.190\n\n\n3\nQQQ\np\n160\n0.2636\n169.29\n21\n1\n0.083333\n1.67\n-0.218\n\n\n4\nQQQ\np\n160\n0.2636\n166.28\n20\n1\n0.079365\n2.33\n-0.289\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20\nQQQ\np\n160\n0.2636\n160.28\n4\n1\n0.015873\n1.98\n-0.472\n\n\n21\nQQQ\np\n160\n0.2636\n157.36\n3\n1\n0.011905\n3.44\n-0.714\n\n\n22\nQQQ\np\n160\n0.2636\n160.36\n2\n1\n0.007937\n1.33\n-0.457\n\n\n23\nQQQ\np\n160\n0.2636\n162.76\n1\n1\n0.003968\n0.21\n-0.150\n\n\n24\nQQQ\np\n160\n0.2636\n164.10\n0\n1\n0.000000\n0.00\n0.000\n\n\n\n\n25 rows × 10 columns\n\n\n\nNext, we calculate the option PNL.\n\ndf_path['option_pnl'] = df_path['buy_sell'] * df_path['option_price'].diff()\ndf_path\n\n\n\n\n\n\n\n\nunderlying\ncp\nstrike\nvolatility\nupx\nd2x\nbuy_sell\nt2x\noption_price\ndelta\noption_pnl\n\n\n\n\n0\nQQQ\np\n160\n0.2636\n168.00\n24\n1\n0.095238\n2.25\n-0.261\nNaN\n\n\n1\nQQQ\np\n160\n0.2636\n172.57\n23\n1\n0.091270\n1.21\n-0.161\n-1.04\n\n\n2\nQQQ\np\n160\n0.2636\n170.80\n22\n1\n0.087302\n1.44\n-0.190\n0.23\n\n\n3\nQQQ\np\n160\n0.2636\n169.29\n21\n1\n0.083333\n1.67\n-0.218\n0.23\n\n\n4\nQQQ\np\n160\n0.2636\n166.28\n20\n1\n0.079365\n2.33\n-0.289\n0.66\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20\nQQQ\np\n160\n0.2636\n160.28\n4\n1\n0.015873\n1.98\n-0.472\n-1.05\n\n\n21\nQQQ\np\n160\n0.2636\n157.36\n3\n1\n0.011905\n3.44\n-0.714\n1.46\n\n\n22\nQQQ\np\n160\n0.2636\n160.36\n2\n1\n0.007937\n1.33\n-0.457\n-2.11\n\n\n23\nQQQ\np\n160\n0.2636\n162.76\n1\n1\n0.003968\n0.21\n-0.150\n-1.12\n\n\n24\nQQQ\np\n160\n0.2636\n164.10\n0\n1\n0.000000\n0.00\n0.000\n-0.21\n\n\n\n\n25 rows × 11 columns\n\n\n\nDelta-hedging with daily rebalancing means at the end of each day we hold a position in the underlying whose size is equal to the negative of the delta of the option position. Thus, the daily delta-hedging PNL is calculated as follows:\n\ndf_path['delta_hedge_pnl'] = -df_path['buy_sell'] * df_path['delta'].shift(1) * df_path['upx'].diff() \ndf_path\n\n\n\n\n\n\n\n\nunderlying\ncp\nstrike\nvolatility\nupx\nd2x\nbuy_sell\nt2x\noption_price\ndelta\noption_pnl\ndelta_hedge_pnl\n\n\n\n\n0\nQQQ\np\n160\n0.2636\n168.00\n24\n1\n0.095238\n2.25\n-0.261\nNaN\nNaN\n\n\n1\nQQQ\np\n160\n0.2636\n172.57\n23\n1\n0.091270\n1.21\n-0.161\n-1.04\n1.19277\n\n\n2\nQQQ\np\n160\n0.2636\n170.80\n22\n1\n0.087302\n1.44\n-0.190\n0.23\n-0.28497\n\n\n3\nQQQ\np\n160\n0.2636\n169.29\n21\n1\n0.083333\n1.67\n-0.218\n0.23\n-0.28690\n\n\n4\nQQQ\np\n160\n0.2636\n166.28\n20\n1\n0.079365\n2.33\n-0.289\n0.66\n-0.65618\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20\nQQQ\np\n160\n0.2636\n160.28\n4\n1\n0.015873\n1.98\n-0.472\n-1.05\n0.87552\n\n\n21\nQQQ\np\n160\n0.2636\n157.36\n3\n1\n0.011905\n3.44\n-0.714\n1.46\n-1.37824\n\n\n22\nQQQ\np\n160\n0.2636\n160.36\n2\n1\n0.007937\n1.33\n-0.457\n-2.11\n2.14200\n\n\n23\nQQQ\np\n160\n0.2636\n162.76\n1\n1\n0.003968\n0.21\n-0.150\n-1.12\n1.09680\n\n\n24\nQQQ\np\n160\n0.2636\n164.10\n0\n1\n0.000000\n0.00\n0.000\n-0.21\n0.20100\n\n\n\n\n25 rows × 12 columns\n\n\n\n\nDiscussion Question: What is the delta-hedging position held at the end of d2x = 21. What is the trade executed at that time?\n\n\nSolution\n##&gt; The end-of-day delta hedge is long .714 contracts of the QQQ.  \n##&gt; In order to get to that position we would have to buy 0.242 contracts. \n\n\n\nThe total_pnl of the delta-hedged option position is the combination of the option_pnl and the delta_hedge_pnl.\n\ndf_path['total_pnl'] = df_path['option_pnl'] + df_path['delta_hedge_pnl']\ndf_path\n\n\n\n\n\n\n\n\nunderlying\ncp\nstrike\nvolatility\nupx\nd2x\nbuy_sell\nt2x\noption_price\ndelta\noption_pnl\ndelta_hedge_pnl\ntotal_pnl\n\n\n\n\n0\nQQQ\np\n160\n0.2636\n168.00\n24\n1\n0.095238\n2.25\n-0.261\nNaN\nNaN\nNaN\n\n\n1\nQQQ\np\n160\n0.2636\n172.57\n23\n1\n0.091270\n1.21\n-0.161\n-1.04\n1.19277\n0.15277\n\n\n2\nQQQ\np\n160\n0.2636\n170.80\n22\n1\n0.087302\n1.44\n-0.190\n0.23\n-0.28497\n-0.05497\n\n\n3\nQQQ\np\n160\n0.2636\n169.29\n21\n1\n0.083333\n1.67\n-0.218\n0.23\n-0.28690\n-0.05690\n\n\n4\nQQQ\np\n160\n0.2636\n166.28\n20\n1\n0.079365\n2.33\n-0.289\n0.66\n-0.65618\n0.00382\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20\nQQQ\np\n160\n0.2636\n160.28\n4\n1\n0.015873\n1.98\n-0.472\n-1.05\n0.87552\n-0.17448\n\n\n21\nQQQ\np\n160\n0.2636\n157.36\n3\n1\n0.011905\n3.44\n-0.714\n1.46\n-1.37824\n0.08176\n\n\n22\nQQQ\np\n160\n0.2636\n160.36\n2\n1\n0.007937\n1.33\n-0.457\n-2.11\n2.14200\n0.03200\n\n\n23\nQQQ\np\n160\n0.2636\n162.76\n1\n1\n0.003968\n0.21\n-0.150\n-1.12\n1.09680\n-0.02320\n\n\n24\nQQQ\np\n160\n0.2636\n164.10\n0\n1\n0.000000\n0.00\n0.000\n-0.21\n0.20100\n-0.00900\n\n\n\n\n25 rows × 13 columns\n\n\n\nAs we can see, the total PNL of the delta-hedged option position is close to zero, but not exactly zero.\n\ndf_path['total_pnl'].sum()\n\n0.18382999999999813\n\n\n\nCode Challenge: Copy and past the above code into the space below, and then modify it to calculate the PNL for selling this option for 2.25 and then delta-hedging it over this same scenario.\n\n\nSolution\n# creating the DataFrame\ndf_path_test = \\\n    (\n    pd.DataFrame(\n        {'underlying':'QQQ',\n         'cp':'p',\n         'strike':160,\n         'volatility':0.2636,\n         'upx':single_path, \n         'd2x':list(range(24, -1, -1)),\n         'buy_sell':-1,\n        }       \n    )\n    .assign(t2x = lambda df: df.d2x / 252)\n    )\n\n# calculating prices, greeks, and PNLs\ndf_path_test['option_price'] = df_path_test[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_px, axis = 1)\ndf_path_test['delta'] = df_path_test[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_delta, axis = 1)\ndf_path_test['option_pnl'] =  df_path_test['buy_sell'] * df_path_test['option_price'].diff()\ndf_path_test['delta_hedge_pnl'] = -df_path_test['buy_sell'] * df_path_test['delta'].shift(1) * df_path_test['upx'].diff()\ndf_path_test['total_pnl'] = df_path_test['option_pnl'] + df_path_test['delta_hedge_pnl']\n\n# calculating total PNL\ndf_path_test['total_pnl'].sum()\n\n\n-0.18382999999999813"
  },
  {
    "objectID": "chapters/15_option_replication/option_replication.html#delta-hedging-multiple-simulated-underlying-price-paths",
    "href": "chapters/15_option_replication/option_replication.html#delta-hedging-multiple-simulated-underlying-price-paths",
    "title": "15  Black-Scholes-Merton Option Replication",
    "section": "15.6 Delta-Hedging: Multiple Simulated Underlying Price Paths",
    "text": "15.6 Delta-Hedging: Multiple Simulated Underlying Price Paths\nAs we saw in the above example, we came fairly close to a zero PNL from delta hedging on a daily basis, which is what the BSM framework suggests. However, this was just for a single hypothetical path, so we may have just gotten lucky. In this section, we will generate PNL data for daily delta-hedging over a variety of paths, and analyze the resulting distribution.\nLet’s begin by initializing our option position and scenario generation parameters.\n\nbuy_sell = -1\nd2x = 24\ncp = 'p'\nspot = 168.\nstrike = 160.\ntenor = np.double(d2x)/252.\noption_price = 2.25\npricing_vol = implied_volatility(price = option_price, S = spot, K = strike, t = tenor, r = 0, q = 0, flag = cp)\npath_vol = pricing_vol\nhedge_frequency = d2x\ndt = tenor / hedge_frequency\nr = 0\nnum_paths = 1000\n\nNext, we initialize an array that will hold all of our paths.\n\nmultiple_paths = np.zeros((hedge_frequency + 1, num_paths))\nmultiple_paths\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\nThe first price in every path is the current spot price of the underlying.\n\nmultiple_paths[0] = spot\nmultiple_paths\n\narray([[168., 168., 168., ..., 168., 168., 168.],\n       [  0.,   0.,   0., ...,   0.,   0.,   0.],\n       [  0.,   0.,   0., ...,   0.,   0.,   0.],\n       ...,\n       [  0.,   0.,   0., ...,   0.,   0.,   0.],\n       [  0.,   0.,   0., ...,   0.,   0.,   0.],\n       [  0.,   0.,   0., ...,   0.,   0.,   0.]])\n\n\nUsing the convenience of broadcasting in numpy.arrays, we can easily calculate all of the required scenarios in a few lines of code.\n\n# setting the random seed\nnp.random.seed(1)\n\nfor t in range(1, hedge_frequency + 1):\n    z = np.random.standard_normal(num_paths) \n    multiple_paths[t] = multiple_paths[t - 1] * np.exp((r - 0.5 * path_vol ** 2) * dt + path_vol * np.sqrt(dt) * z)\n    multiple_paths[t] = np.round(multiple_paths[t], 2)\n    \nmultiple_paths\n\narray([[168.  , 168.  , 168.  , ..., 168.  , 168.  , 168.  ],\n       [172.57, 166.28, 166.51, ..., 167.78, 168.97, 167.46],\n       [172.11, 159.67, 167.9 , ..., 165.21, 170.77, 171.34],\n       ...,\n       [159.97, 156.87, 163.42, ..., 144.66, 174.18, 169.42],\n       [161.34, 160.23, 164.69, ..., 145.86, 170.28, 170.59],\n       [164.14, 157.3 , 162.92, ..., 145.92, 176.16, 169.25]])\n\n\nJust to make sure we understand our data structures, let’s pull out the first scenario and perform our delta-hedge calculations with it. After we do that, we will wrap this code in a for-loop in order to perform the delta-hedge calculations for all the scenarios.\n\n# creating the DataFrame\ndf_path = \\\n    pd.DataFrame(\n        {'cp':cp,\n         'strike':strike,\n         'volatility':path_vol,\n         'upx':multiple_paths[:, 0], \n         't2x':np.linspace(tenor, 0, hedge_frequency + 1),\n         'buy_sell': buy_sell,\n        }       \n    )\n\n# calculating prices, greeks, and PNLs\ndf_path['option_price'] = df_path[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_px, axis = 1)\ndf_path['delta'] = df_path[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_delta, axis = 1)\ndf_path['option_pnl'] = df_path['buy_sell'] * df_path['option_price'].diff()\ndf_path['delta_hedge_pnl'] = -df_path['buy_sell'] * df_path['delta'].shift(1) * df_path['upx'].diff()\ndf_path['total_pnl'] = df_path['option_pnl'] + df_path['delta_hedge_pnl']\n\n# viewing the path\ndf_path\n\n\n\n\n\n\n\n\ncp\nstrike\nvolatility\nupx\nt2x\nbuy_sell\noption_price\ndelta\noption_pnl\ndelta_hedge_pnl\ntotal_pnl\n\n\n\n\n0\np\n160.0\n0.263631\n168.00\n0.095238\n-1\n2.25\n-0.261\nNaN\nNaN\nNaN\n\n\n1\np\n160.0\n0.263631\n172.57\n0.091270\n-1\n1.21\n-0.161\n1.04\n-1.19277\n-0.15277\n\n\n2\np\n160.0\n0.263631\n172.11\n0.087302\n-1\n1.21\n-0.165\n-0.00\n0.07406\n0.07406\n\n\n3\np\n160.0\n0.263631\n173.49\n0.083333\n-1\n0.93\n-0.135\n0.28\n-0.22770\n0.05230\n\n\n4\np\n160.0\n0.263631\n173.24\n0.079365\n-1\n0.90\n-0.134\n0.03\n0.03375\n0.06375\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20\np\n160.0\n0.263631\n163.76\n0.015873\n-1\n0.77\n-0.237\n0.55\n-0.38913\n0.16087\n\n\n21\np\n160.0\n0.263631\n158.57\n0.011905\n-1\n2.63\n-0.617\n-1.86\n1.23003\n-0.62997\n\n\n22\np\n160.0\n0.263631\n159.97\n0.007937\n-1\n1.51\n-0.499\n1.12\n-0.86380\n0.25620\n\n\n23\np\n160.0\n0.263631\n161.34\n0.003968\n-1\n0.53\n-0.305\n0.98\n-0.68363\n0.29637\n\n\n24\np\n160.0\n0.263631\n164.14\n0.000000\n-1\n0.00\n0.000\n0.53\n-0.85400\n-0.32400\n\n\n\n\n25 rows × 11 columns\n\n\n\nLet’s check the cumulative PNL for this scenario; clearly the PNL outcome is much farther for this scenario than the one above.\n\ndf_path['total_pnl'].sum()\n\n-1.612160000000005\n\n\nWe can now generalize the above code and perform the delta-hedging calculations on each scenario. We will save the calculations for each scenario to analyze.\n\nlst_scenarios = []\nfor ix_path in range(0, num_paths):\n    \n    # creating dataframe\n    df_path = \\\n        pd.DataFrame(\n            {'cp':cp,\n             'strike':strike,\n             'volatility':pricing_vol,\n             'upx':multiple_paths[:, ix_path], \n             't2x':np.linspace(tenor, 0, hedge_frequency + 1),\n             'buy_sell':buy_sell\n            }\n        )\n    \n    # calculating prices, greeks, and PNLs\n    df_path['option_price'] = df_path[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_px, axis = 1)\n    df_path['delta'] = df_path[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_delta, axis = 1)\n    df_path['option_pnl'] = df_path['buy_sell'] * df_path['option_price'].diff()\n    df_path['delta_hedge_pnl'] = -df_path['buy_sell'] * df_path['delta'].shift(1) * df_path['upx'].diff()\n    df_path['total_pnl'] = df_path['option_pnl'] + df_path['delta_hedge_pnl']\n    df_path['scenario'] = ix_path\n    \n    # storing df_path into a list\n    lst_scenarios.append(df_path)\n\n# creating a single DataFrame that contains all scenarios\ndf_all_paths = pd.concat(lst_scenarios)\n\n# viewing the DataFrame\ndf_all_paths\n\n\n\n\n\n\n\n\ncp\nstrike\nvolatility\nupx\nt2x\nbuy_sell\noption_price\ndelta\noption_pnl\ndelta_hedge_pnl\ntotal_pnl\nscenario\n\n\n\n\n0\np\n160.0\n0.263631\n168.00\n0.095238\n-1\n2.25\n-0.261\nNaN\nNaN\nNaN\n0\n\n\n1\np\n160.0\n0.263631\n172.57\n0.091270\n-1\n1.21\n-0.161\n1.04\n-1.19277\n-0.15277\n0\n\n\n2\np\n160.0\n0.263631\n172.11\n0.087302\n-1\n1.21\n-0.165\n-0.00\n0.07406\n0.07406\n0\n\n\n3\np\n160.0\n0.263631\n173.49\n0.083333\n-1\n0.93\n-0.135\n0.28\n-0.22770\n0.05230\n0\n\n\n4\np\n160.0\n0.263631\n173.24\n0.079365\n-1\n0.90\n-0.134\n0.03\n0.03375\n0.06375\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20\np\n160.0\n0.263631\n171.42\n0.015873\n-1\n0.04\n-0.018\n0.03\n0.00837\n0.03837\n999\n\n\n21\np\n160.0\n0.263631\n173.24\n0.011905\n-1\n0.00\n-0.003\n0.04\n-0.03276\n0.00724\n999\n\n\n22\np\n160.0\n0.263631\n169.42\n0.007937\n-1\n0.01\n-0.007\n-0.01\n0.01146\n0.00146\n999\n\n\n23\np\n160.0\n0.263631\n170.59\n0.003968\n-1\n0.00\n-0.000\n0.01\n-0.00819\n0.00181\n999\n\n\n24\np\n160.0\n0.263631\n169.25\n0.000000\n-1\n0.00\n0.000\n-0.00\n0.00000\n0.00000\n999\n\n\n\n\n25000 rows × 12 columns\n\n\n\nLet’s use a .groupby() to calculate the cummulative PNL for each scenario\n\ndf_pnl = df_all_paths.groupby(['scenario'], as_index = False)[['total_pnl']].sum()\ndf_pnl\n\n\n\n\n\n\n\n\nscenario\ntotal_pnl\n\n\n\n\n0\n0\n-1.61216\n\n\n1\n1\n0.40480\n\n\n2\n2\n0.34073\n\n\n3\n3\n0.31858\n\n\n4\n4\n0.16595\n\n\n...\n...\n...\n\n\n995\n995\n1.75078\n\n\n996\n996\n-0.83710\n\n\n997\n997\n0.13385\n\n\n998\n998\n0.54689\n\n\n999\n999\n-1.77710\n\n\n\n\n1000 rows × 2 columns\n\n\n\nAs we can see, the average of the total_pnls is zero, which further demonstrates the manufacturing result of the Black-Scholes-Merton framework.\n\ndf_pnl['total_pnl'].mean()\n\n-0.0009835299999998704\n\n\n\nDiscussion Queston: If you sold this option for 0.25 more than fair-value, what would your average PNL be?\n\n\nSolution\n##&gt; Approximately $0.25\n\n\n\nCode Challenge: Is the delta-hedging reducing risk? Try to verify this with a bit of data analysis.\n\n\nSolution\nprint(df_all_paths.groupby('scenario')['option_pnl'].sum().std())\nprint(df_all_paths.groupby('scenario')['total_pnl'].sum().std())\n\n\n5.006480605770481\n0.7586453989501201\n\n\n\nCode Challenge: Calculate the standard deviation, minimum, and maximum of the cumulative PNLs.\n\n\nSolution\nprint(\"Std Dev:\", np.round(df_pnl['total_pnl'].std(), 2))\nprint(\"Min:   \", np.round(df_pnl['total_pnl'].min(), 2))\nprint(\"Max:    \",np.round(df_pnl['total_pnl'].max(), 2))\n\n\nStd Dev: 0.76\nMin:    -2.9\nMax:     2.49\n\n\n\nDiscussion Question: What are your thoughts on the range of possible PNL outcomes? Does option replication via discrete delta-hedging seem like a risk-free endeavor?\n\n\nSolution\n##&gt; There is a wide variation of PNLs, and it is possible to have a \n##&gt; gain or a loss that is greater than the value of the option itself.\n##&gt; Discrete delta-hedging is far from riskless."
  },
  {
    "objectID": "chapters/15_option_replication/option_replication.html#what-if-realized-volatility-is-different-that-pricing-volatility",
    "href": "chapters/15_option_replication/option_replication.html#what-if-realized-volatility-is-different-that-pricing-volatility",
    "title": "15  Black-Scholes-Merton Option Replication",
    "section": "15.7 What If Realized Volatility is Different that Pricing Volatility?",
    "text": "15.7 What If Realized Volatility is Different that Pricing Volatility?\nAs we can see above, if the realized volatility of the underlying during the life of the option is equal to the pricing volatility, then the daily delta-hedging trader will break even on average (however, the outcome can vary substantially depending on the particular scenario).\nIn this section, we explore what happens when realized volatility differs from pricing volatility. In particular, we will see what happens when a trader sells an option, delta-hedges daily, but the realized volatility is 5% higher than implied.\n\n# setting simulation parameters\nbuy_sell = -1\nd2x = 24\ncp = 'p'\nspot = 168.\nstrike = 160.\ntenor = np.double(d2x)/252.\noption_price = 2.25\npricing_vol = implied_volatility(price = option_price, S = spot, K = strike, t = tenor, r = 0, q = 0, flag = cp)\npath_vol = pricing_vol + 0.05\nhedge_frequency = d2x\ndt = tenor / hedge_frequency\nr = 0\nnum_paths = 1000\n\n\n# initializing paths\nmultiple_paths = np.zeros((hedge_frequency + 1, num_paths))\nmultiple_paths[0] = spot\n\n# setting the random seed\nnp.random.seed(1)\n\n# calculating paths\nfor t in range(1, hedge_frequency + 1):\n    z = np.random.standard_normal(num_paths) \n    multiple_paths[t] = multiple_paths[t - 1] * np.exp((r - 0.5 * path_vol ** 2) * dt + path_vol * np.sqrt(dt) * z)\n    multiple_paths[t] = np.round(multiple_paths[t], 2)\n\n# performing delta-hedge calculations on all the paths\nlst_scenarios = []\nfor ix_path in range(0, num_paths):\n    \n    # creating the DataFrame\n    df_path = \\\n        pd.DataFrame(\n            {'cp':cp,\n             'strike':strike,\n             'volatility':pricing_vol,\n             'upx':multiple_paths[:, ix_path], \n             't2x':np.linspace(tenor, 0, hedge_frequency + 1),\n             'buy_sell':buy_sell\n            }\n        )\n    \n    # calculating prices, greeks, and PNLs\n    df_path['option_price'] = df_path[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_px, axis = 1)\n    df_path['delta'] = df_path[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_delta, axis = 1)\n    df_path['option_pnl'] = df_path['buy_sell'] * df_path['option_price'].diff()\n    df_path['delta_hedge_pnl'] = -df_path['buy_sell'] * df_path['delta'].shift(1) * df_path['upx'].diff()\n    df_path['total_pnl'] = df_path['option_pnl'] + df_path['delta_hedge_pnl']\n    df_path['scenario'] = ix_path\n    \n    # storing df_path into a list\n    lst_scenarios.append(df_path)\n    \n# creating a single DataFrame that contains all scenarios    \ndf_all_paths = pd.concat(lst_scenarios)\n\n# calculating cumulative PNLs\ndf_pnl = df_all_paths.groupby(['scenario'], as_index = False)[['total_pnl']].sum()\n\nAs we can see, since realized is greater than implied, we lose money on average.\n\ndf_pnl['total_pnl'].mean()\n\n-0.8403840600000003\n\n\n\nCode Challenge: Use bsm_vega and verify that this PNL is consistent with the identity: \\(vega * (implied - realzed)\\).\n\n\nSolution\n-bsm_vega(df_all_paths[['cp', 'upx', 'strike', 't2x', 'volatility']].iloc[0,:]) * 5\n\n\n-0.8400000000000001"
  },
  {
    "objectID": "chapters/15_option_replication/option_replication.html#increasing-delta-hedge-frequency-reduces-pnl-variability",
    "href": "chapters/15_option_replication/option_replication.html#increasing-delta-hedge-frequency-reduces-pnl-variability",
    "title": "15  Black-Scholes-Merton Option Replication",
    "section": "15.8 Increasing Delta-Hedge Frequency Reduces PNL Variability",
    "text": "15.8 Increasing Delta-Hedge Frequency Reduces PNL Variability\nThe BSM manufacturing framework states that in the limit of continuous delta-hedging, that these results become deterministic. The means that the delta-hedging outcomes are always the same for each scenario. Your final code challenge is to explore this via data analysis.\n\nCode Challenge: Copy and paste the above code, and see what happens to the dispersion of the distribution of the delta-hedge PNL outcomes when you double and quadruple the hedge_frequency.\n\n\nSolution\n# setting simulation parameters\nbuy_sell = -1\nd2x = 24\ncp = 'p'\nspot = 168.\nstrike = 160.\ntenor = np.double(d2x)/252. # I want to generalize this\noption_price = 2.25\npricing_vol = implied_volatility(price = option_price, S = spot, K = strike, t = tenor, r = 0, q = 0, flag = cp)\npath_vol = pricing_vol\nhedge_frequency = d2x * 4\ndt = tenor / hedge_frequency  # I want to generalize this\nr = 0\nnum_paths = 1000\n\n\n# initializing paths\nmultiple_paths = np.zeros((hedge_frequency + 1, num_paths))\nmultiple_paths[0] = spot\n\n# setting the random seed\nnp.random.seed(1)\n\n# calculating paths\nfor t in range(1, hedge_frequency + 1):\n    z = np.random.standard_normal(num_paths) \n    multiple_paths[t] = multiple_paths[t - 1] * np.exp((r - 0.5 * path_vol ** 2) * dt + path_vol * np.sqrt(dt) * z)\n    multiple_paths[t] = np.round(multiple_paths[t], 2)\n\n# performing delta-hedge calculations on all the paths\nlst_scenarios = []\nfor ix_path in range(0, num_paths):\n    \n    # creating the DataFrame\n    df_path = \\\n        pd.DataFrame(\n            {'cp':cp,\n             'strike':strike,\n             'volatility':pricing_vol,\n             'upx':multiple_paths[:, ix_path], \n             't2x':np.linspace(tenor, 0, hedge_frequency + 1),\n             'buy_sell':buy_sell\n            }\n        )\n    \n    # calculating prices, greeks, and PNLs\n    df_path['option_price'] = df_path[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_px, axis = 1)\n    df_path['delta'] = df_path[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_delta, axis = 1)\n    df_path['option_pnl'] = df_path['buy_sell'] * df_path['option_price'].diff()\n    df_path['delta_hedge_pnl'] = -df_path['buy_sell'] * df_path['delta'].shift(1) * df_path['upx'].diff()\n    df_path['total_pnl'] = df_path['option_pnl'] + df_path['delta_hedge_pnl']\n    df_path['scenario'] = ix_path\n    \n    # storing df_path into a list\n    lst_scenarios.append(df_path)\n    \n# creating a single DataFrame that contains all scenarios    \ndf_all_paths = pd.concat(lst_scenarios)\n\n# calculating cumulative PNLs\ndf_pnl = df_all_paths.groupby(['scenario'], as_index = False)[['total_pnl']].sum()\n\n# calculating distrubution statistics\nprint(\"Std Dev:\", np.round(df_pnl['total_pnl'].std(), 2))\nprint(\"Min:   \", np.round(df_pnl['total_pnl'].min(), 2))\nprint(\"Max:    \",np.round(df_pnl['total_pnl'].max(), 2))\n\n\nStd Dev: 0.42\nMin:    -1.89\nMax:     1.92"
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#importing-packages",
    "href": "chapters/16_trend_following/trend_following.html#importing-packages",
    "title": "16  Trend Following",
    "section": "16.1 Importing Packages",
    "text": "16.1 Importing Packages\nLet’s begin by importing the packages that we will need.\n\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr"
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#setting-parameters",
    "href": "chapters/16_trend_following/trend_following.html#setting-parameters",
    "title": "16  Trend Following",
    "section": "16.2 Setting Parameters",
    "text": "16.2 Setting Parameters\nNext we set the parameters of our analysis. The parameter sma_days determines the number of days used in the trailing simple moving average.\n\nticker = '^SP500TR'\nstart_date = '1989-12-29'\nend_date = '2012-01-01'\nsma_days = 200"
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#reading-in-data",
    "href": "chapters/16_trend_following/trend_following.html#reading-in-data",
    "title": "16  Trend Following",
    "section": "16.3 Reading-In Data",
    "text": "16.3 Reading-In Data\nWe now read-in our data from Yahoo Finance.\n\ndf_asset = pdr.get_data_yahoo(ticker)\ndf_asset.reset_index(inplace=True)\ndf_asset.columns = df_asset.columns.str.lower().str.replace(' ','_')\ndf_asset = df_asset[['date', 'adj_close']].copy()\ndf_asset['sma'] = df_asset['adj_close'].rolling(sma_days).mean()\ndf_asset = df_asset.query('date &gt;= @start_date and date &lt;= @end_date').copy()\ndf_asset.plot(x='date', y=['adj_close', 'sma'], grid=True, title='Adjusted Close and Moving Average Through Time');\n\n[*********************100%***********************]  1 of 1 completed"
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#determining-position-based-on-trend",
    "href": "chapters/16_trend_following/trend_following.html#determining-position-based-on-trend",
    "title": "16  Trend Following",
    "section": "16.4 Determining Position Based on Trend",
    "text": "16.4 Determining Position Based on Trend\nHere we define the function that will help to determine our daily position in the asset.\n\ndef calc_position(row):\n    adj_close = row['adj_close']\n    sma = row['sma']\n\n    position = 0\n    if sma &lt; adj_close:\n        position = 1\n\n    return position\n\nLet’s now use the DataFrame.apply() method to calculate all the positions through time.\n\ndf_asset['trend_position'] = df_asset.apply(calc_position, axis = 1)\ndf_asset\n\n\n\n\n\n\n\n\ndate\nadj_close\nsma\ntrend_position\n\n\n\n\n504\n1989-12-29\n379.410004\n351.375200\n1\n\n\n505\n1990-01-02\n386.160004\n351.776050\n1\n\n\n506\n1990-01-03\n385.170013\n352.186400\n1\n\n\n507\n1990-01-04\n382.019989\n352.573600\n1\n\n\n508\n1990-01-05\n378.299988\n352.946600\n1\n\n\n...\n...\n...\n...\n...\n\n\n6047\n2011-12-23\n2171.500000\n2141.526601\n1\n\n\n6048\n2011-12-27\n2171.709961\n2141.442350\n1\n\n\n6049\n2011-12-28\n2145.090088\n2141.347501\n1\n\n\n6050\n2011-12-29\n2168.120117\n2141.578401\n1\n\n\n6051\n2011-12-30\n2158.939941\n2141.620952\n1\n\n\n\n\n5548 rows × 4 columns"
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#calculating-returns-equity-curves-and-drawdowns",
    "href": "chapters/16_trend_following/trend_following.html#calculating-returns-equity-curves-and-drawdowns",
    "title": "16  Trend Following",
    "section": "16.5 Calculating Returns, Equity Curves, and Drawdowns",
    "text": "16.5 Calculating Returns, Equity Curves, and Drawdowns\nWe now have all the data the we need to calculate daily returns, the equity curves, and drawdowns of the two strategies.\n\n# returns\ndf_asset['buy_hold_return'] = df_asset['adj_close'].pct_change()\ndf_asset['trend_return'] = df_asset['buy_hold_return'] * df_asset['trend_position'].shift(1)\n\n# growth factors\ndf_asset['buy_hold_factor'] = 1 + df_asset['buy_hold_return']\ndf_asset['trend_factor'] = 1 + df_asset['trend_return']\n\n# equity curves\ndf_asset['buy_hold_equity'] = df_asset['buy_hold_factor'].cumprod()\ndf_asset['trend_equity'] = df_asset['trend_factor'].cumprod()\n\n# maximum cumulative equity\ndf_asset['buy_hold_max_equity'] = df_asset['buy_hold_equity'].cummax()\ndf_asset['trend_max_equity'] = df_asset['trend_equity'].cummax()\n\n# draw-down\ndf_asset['buy_hold_drawdown'] = (df_asset['buy_hold_equity'] - df_asset['buy_hold_max_equity']) / df_asset['buy_hold_max_equity']\ndf_asset['trend_drawdown'] = (df_asset['trend_equity'] - df_asset['trend_max_equity']) / df_asset['trend_max_equity']\n\n# graphing equity curves\ndf_asset.plot(x='date', y=['buy_hold_equity','trend_equity'], grid=True, title='Equity Graph');"
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#return-characteristics",
    "href": "chapters/16_trend_following/trend_following.html#return-characteristics",
    "title": "16  Trend Following",
    "section": "16.6 Return Characteristics",
    "text": "16.6 Return Characteristics\nFinally, we calculate some basic performance metrics.\nAnnualized Return\n\nprint('buy-hold return: ', np.round(df_asset['buy_hold_equity'].iloc[-1] ** (252 / (len(df_asset) - 1)) - 1, 3) * 100, '%')\nprint('trend return:    ', np.round(df_asset['trend_equity'].iloc[-1] ** (252 / (len(df_asset) - 1)) - 1, 3) * 100, '%')\n\nbuy-hold return:  8.200000000000001 %\ntrend return:     7.199999999999999 %\n\n\nSharpe-Ratio\n\nprint('buy-hold sharpe-ratio: ', np.round((np.mean(df_asset['buy_hold_return']) / np.std(df_asset['buy_hold_return'])) * np.sqrt(252), 2))\nprint('trend sharpe-ratio:    ', np.round((np.mean(df_asset['trend_return']) / np.std(df_asset['trend_return'])) * np.sqrt(252), 2))\n\nbuy-hold sharpe-ratio:  0.51\ntrend sharpe-ratio:     0.66\n\n\nMaximum Drawdown\n\nprint('buy-hold max-drawdown: ', np.round(np.min(df_asset['buy_hold_drawdown']), 2))\nprint('trend max-drawdown:    ', np.round(np.min(df_asset['trend_drawdown']), 2))\n\nbuy-hold max-drawdown:  -0.55\ntrend max-drawdown:     -0.24"
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#case-study-sp500-1990-2012",
    "href": "chapters/16_trend_following/trend_following.html#case-study-sp500-1990-2012",
    "title": "16  Trend Following",
    "section": "16.7 Case Study: S&P500 1990-2012",
    "text": "16.7 Case Study: S&P500 1990-2012\nLet’s now take a look at a particular case study of the S&P500 in the period of 1990-2012. We will use the Total Returns futures to represent an investment in the S&P500. In order to get the result we describe, rerun this notebook with the following inputs:\nticker = '^SP500TR'\nstart_date = '1989-12-29'\nend_date = '2012-01-01'\nsma_days = 200\nWe first compare our equity graph above to Faber’s equity graph which we present below:\n\nAs we can see in Faber’s analysis, trend-following slightly outperforms the buy-and-hold strategy, while in our analysis the trend-following strategy underperforms. This difference is likely due to the difference in rebalance frequency. Our strategy rebalances daily (which is probably too much) while Faber’s rebalances monthly.\nAnalyzing our results more closely, we see that buy-and-hold has an annualized return of 8.2%, while trend-following has an annualized return of 7.2%. There is, however, significantly less downside risk with trend-following, as the strategy tends to sit-out bear markets (e.g. 2000 and 2008). This results in a max-drawdown of -24% for trend following, while buy-and-hold had a max-drawdown of -55% during the 2008 financial crisis. This reduced downside risk can also be seen in the Sharpe-ratio, a metric by which trend-following (0.66) outperforms buy-and-hold (0.51)."
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#loading-packages",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#loading-packages",
    "title": "17  Regression: Mutual Fund Analysis",
    "section": "17.1 Loading Packages",
    "text": "17.1 Loading Packages\nLet’s begin by importing the packages that we will need.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline"
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#reading-in-data",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#reading-in-data",
    "title": "17  Regression: Mutual Fund Analysis",
    "section": "17.2 Reading-In Data",
    "text": "17.2 Reading-In Data\nNext, we’ll read-in the data, rename the columns, and convert the trade_date column to datetime.\n\n# reading-in data\ndf_px_raw = pd.read_csv('mutual_fund_data.csv')\n# renaming columns\ndf_px_raw.rename(\n    columns={'Date':'trade_date', 'BM':'benchmark', 'Fund_1':'fund_1', 'Fund_2':'fund_2'}\n    , inplace=True)\n# converting data-type of trade_date\ndf_px_raw['trade_date'] = pd.to_datetime(df_px_raw['trade_date'])\n\ndf_px_raw.head()\n\n\n\n\n\n\n\n\ntrade_date\nbenchmark\nfund_1\nfund_2\n\n\n\n\n0\n2015-12-31\n203.8700\n108.29\n254.04\n\n\n1\n2016-01-01\n203.8700\n108.29\n254.04\n\n\n2\n2016-01-04\n201.0192\n106.61\n250.82\n\n\n3\n2016-01-05\n201.3600\n106.10\n250.90\n\n\n4\n2016-01-06\n198.8200\n104.66\n247.36"
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#visualizing-raw-price-data",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#visualizing-raw-price-data",
    "title": "17  Regression: Mutual Fund Analysis",
    "section": "17.3 Visualizing Raw Price Data",
    "text": "17.3 Visualizing Raw Price Data\nThe pandas package was created by Wes McKinney when he was a quant at the hedge fund AQR. Because of this, pandas was built with a lot of functionality that is particularly useful for working with financial timeseries. This includes some rudimentary plotting functionality, which is built on top of the package matplotlib.\nLet’s quickly plot the price series for our three funds using the DataFrame.plot() method.\n\ndf_px_raw.plot(\n    x='trade_date', \n    y=['benchmark', 'fund_1', 'fund_2'],\n    figsize=(8,5),\n    title='Daily Fund Prices: 2016Q1-2019Q3',\n    grid = True\n);\n\n\n\n\n\nCode Challenge: Copy and paste the code above and try setting subplots=True.\n\n\nSolution\ndf_px_raw.plot(\n    x='trade_date', \n    y=['benchmark', 'fund_1', 'fund_2'],\n    figsize=(8,8),\n    title='Fund Prices: 2016Q1-2019Q3',\n    grid = True,\n    subplots = True,\n);\nplt.subplots_adjust(top=0.94);"
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#tidying-the-data",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#tidying-the-data",
    "title": "17  Regression: Mutual Fund Analysis",
    "section": "17.4 Tidying the Data",
    "text": "17.4 Tidying the Data\nOur data, in its current form, is not tidy. In order for data to be tidy, each row should be a single observation. Currently, each row of df_px_raw is three price observations - one for each of the three funds - which will not allow for easy use of the .groupby() function.\nWe can use the pandas.melt() method to tidy our data.\n\ndf_px = \\\n    pd.melt(\n        df_px_raw, \n        id_vars = ['trade_date'],\n        value_vars = ['benchmark', 'fund_1', 'fund_2'], \n        var_name = 'symbol', \n        value_name = 'close',\n    )\n\ndf_px.head()\n\n\n\n\n\n\n\n\ntrade_date\nsymbol\nclose\n\n\n\n\n0\n2015-12-31\nbenchmark\n203.8700\n\n\n1\n2016-01-01\nbenchmark\n203.8700\n\n\n2\n2016-01-04\nbenchmark\n201.0192\n\n\n3\n2016-01-05\nbenchmark\n201.3600\n\n\n4\n2016-01-06\nbenchmark\n198.8200\n\n\n\n\n\n\n\nOur analysis will involve calculating quarterly statistics, so let’s add columns year, quarter, and month.\n\ndf_px['year'] = df_px['trade_date'].dt.year\ndf_px['quarter'] = df_px['trade_date'].dt.quarter\ndf_px['month'] = df_px['trade_date'].dt.month\ndf_px.head()\n\n\n\n\n\n\n\n\ntrade_date\nsymbol\nclose\nyear\nquarter\nmonth\n\n\n\n\n0\n2015-12-31\nbenchmark\n203.8700\n2015\n4\n12\n\n\n1\n2016-01-01\nbenchmark\n203.8700\n2016\n1\n1\n\n\n2\n2016-01-04\nbenchmark\n201.0192\n2016\n1\n1\n\n\n3\n2016-01-05\nbenchmark\n201.3600\n2016\n1\n1\n\n\n4\n2016-01-06\nbenchmark\n198.8200\n2016\n1\n1\n\n\n\n\n\n\n\nLet’s also rearrange columns to make our table a little more human readable.\n\ndf_px = df_px[['symbol','trade_date', 'year', 'quarter', 'month',  'close',]]\ndf_px.head()\n\n\n\n\n\n\n\n\nsymbol\ntrade_date\nyear\nquarter\nmonth\nclose\n\n\n\n\n0\nbenchmark\n2015-12-31\n2015\n4\n12\n203.8700\n\n\n1\nbenchmark\n2016-01-01\n2016\n1\n1\n203.8700\n\n\n2\nbenchmark\n2016-01-04\n2016\n1\n1\n201.0192\n\n\n3\nbenchmark\n2016-01-05\n2016\n1\n1\n201.3600\n\n\n4\nbenchmark\n2016-01-06\n2016\n1\n1\n198.8200"
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#visualizing-the-tidy-price-data-with-seaborn",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#visualizing-the-tidy-price-data-with-seaborn",
    "title": "17  Regression: Mutual Fund Analysis",
    "section": "17.5 Visualizing the Tidy Price Data with seaborn",
    "text": "17.5 Visualizing the Tidy Price Data with seaborn\nLet’s now import the seaborn package, which a popular visualization package that is also built on top of matplotlib.\n\nimport seaborn as sns\nsns.set()\n\nIn the code below, we use sns.relplot() to graph the three fund price series using our data in its tidy form. Notice that by setting hue='symbol', the sns.relplot() function knows to graph the close prices for each of the three symbols separately and with three different colors.\n\ng = \\\nsns.relplot(\n    data = df_px, kind = 'line',\n    x = 'trade_date', y = 'close', hue = 'symbol',\n    aspect = 1.5,\n);\n\n# creating and tweaking the title\ng.fig.suptitle('Daily Fund Close Prices: 2016Q1-2019Q3')\nplt.subplots_adjust(top=0.93);"
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#calculating-daily-returns-1",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#calculating-daily-returns-1",
    "title": "17  Regression: Mutual Fund Analysis",
    "section": "17.6 Calculating Daily Returns (#1)",
    "text": "17.6 Calculating Daily Returns (#1)\nNotice that our data does not contain daily returns.\n\ndf_px.head()\n\n\n\n\n\n\n\n\nsymbol\ntrade_date\nyear\nquarter\nmonth\nclose\n\n\n\n\n0\nbenchmark\n2015-12-31\n2015\n4\n12\n203.8700\n\n\n1\nbenchmark\n2016-01-01\n2016\n1\n1\n203.8700\n\n\n2\nbenchmark\n2016-01-04\n2016\n1\n1\n201.0192\n\n\n3\nbenchmark\n2016-01-05\n2016\n1\n1\n201.3600\n\n\n4\nbenchmark\n2016-01-06\n2016\n1\n1\n198.8200\n\n\n\n\n\n\n\nNow that our data is tidy, we can easily obtain daily log-returns using a grouped calculation.\nWe use log-returns because, they are very close in value to simple returns, and multi-day log-returns are easily calculated as sums of single-day returns.\n\ndf_px['daily_ret'] = \\\n    np.log(df_px['close']).groupby(df_px['symbol']).diff() #log returns\ndf_px.head()\n\n\n\n\n\n\n\n\nsymbol\ntrade_date\nyear\nquarter\nmonth\nclose\ndaily_ret\n\n\n\n\n0\nbenchmark\n2015-12-31\n2015\n4\n12\n203.8700\nNaN\n\n\n1\nbenchmark\n2016-01-01\n2016\n1\n1\n203.8700\n0.000000\n\n\n2\nbenchmark\n2016-01-04\n2016\n1\n1\n201.0192\n-0.014082\n\n\n3\nbenchmark\n2016-01-05\n2016\n1\n1\n201.3600\n0.001694\n\n\n4\nbenchmark\n2016-01-06\n2016\n1\n1\n198.8200\n-0.012694\n\n\n\n\n\n\n\n\nCode Challenge: Verify that all the log-returns on the first date in the data set are NaN.\n\n\nSolution\ndf_px[df_px['trade_date'] == np.min(df_px['trade_date'])]\n\n\n\n\n\n\n\n\n\nsymbol\ntrade_date\nyear\nquarter\nmonth\nclose\ndaily_ret\n\n\n\n\n0\nbenchmark\n2015-12-31\n2015\n4\n12\n203.87\nNaN\n\n\n978\nfund_1\n2015-12-31\n2015\n4\n12\n108.29\nNaN\n\n\n1956\nfund_2\n2015-12-31\n2015\n4\n12\n254.04\nNaN"
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#rolling-252-day-return-2",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#rolling-252-day-return-2",
    "title": "17  Regression: Mutual Fund Analysis",
    "section": "17.7 Rolling 252-day Return (#2)",
    "text": "17.7 Rolling 252-day Return (#2)\nNow that we have daily log-returns, we can easily calculate a rolling 252-day return by using the .rolling() method.\n\ndf_px['ret_252'] = \\\n    df_px['daily_ret'].groupby(df_px['symbol']).rolling(252).sum().values\ndf_px.head()\n\n\n\n\n\n\n\n\nsymbol\ntrade_date\nyear\nquarter\nmonth\nclose\ndaily_ret\nret_252\n\n\n\n\n0\nbenchmark\n2015-12-31\n2015\n4\n12\n203.8700\nNaN\nNaN\n\n\n1\nbenchmark\n2016-01-01\n2016\n1\n1\n203.8700\n0.000000\nNaN\n\n\n2\nbenchmark\n2016-01-04\n2016\n1\n1\n201.0192\n-0.014082\nNaN\n\n\n3\nbenchmark\n2016-01-05\n2016\n1\n1\n201.3600\n0.001694\nNaN\n\n\n4\nbenchmark\n2016-01-06\n2016\n1\n1\n198.8200\n-0.012694\nNaN\n\n\n\n\n\n\n\nNext we’ll use sns.FacetGrid() to graph three separate subplots of the rolling returns.\n\ng = sns.FacetGrid(df_px, col='symbol', aspect=1.25, height=3,)\ng.map(plt.plot, 'trade_date', 'ret_252', alpha=0.7,)\ng.add_legend();\ng.set_xticklabels(rotation=35, horizontalalignment='right');\n\n\n\n\n\nCode Challenge: Copy and paste the code above, and try changing col='symbol' to row='symbol'.\n\n\nSolution\ng = sns.FacetGrid(df_px, row='symbol', aspect=1.25, height=3)\ng.map(plt.plot, 'trade_date', 'ret_252', alpha=0.7, )\ng.add_legend();"
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#rolling-252-day-volatility-3",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#rolling-252-day-volatility-3",
    "title": "17  Regression: Mutual Fund Analysis",
    "section": "17.8 Rolling 252-day Volatility (#3)",
    "text": "17.8 Rolling 252-day Volatility (#3)\nWe can calculate a rolling 252-day volatility in a similar fashion as the rolling returns.\n\ndf_px['vol_252'] = \\\n    df_px['daily_ret'].groupby(df_px['symbol']).rolling(252).std().values * np.sqrt(252)\ndf_px.head()\n\n\n\n\n\n\n\n\nsymbol\ntrade_date\nyear\nquarter\nmonth\nclose\ndaily_ret\nret_252\nvol_252\n\n\n\n\n0\nbenchmark\n2015-12-31\n2015\n4\n12\n203.8700\nNaN\nNaN\nNaN\n\n\n1\nbenchmark\n2016-01-01\n2016\n1\n1\n203.8700\n0.000000\nNaN\nNaN\n\n\n2\nbenchmark\n2016-01-04\n2016\n1\n1\n201.0192\n-0.014082\nNaN\nNaN\n\n\n3\nbenchmark\n2016-01-05\n2016\n1\n1\n201.3600\n0.001694\nNaN\nNaN\n\n\n4\nbenchmark\n2016-01-06\n2016\n1\n1\n198.8200\n-0.012694\nNaN\nNaN\n\n\n\n\n\n\n\nThis code creates a seaborn.FacetGrid() of the rolling volatilities.\n\ng = sns.FacetGrid(df_px, col='symbol', aspect=1.25, height=3,)\ng.map(plt.plot, 'trade_date', 'vol_252', alpha=0.7)\ng.set_xticklabels(rotation=35, horizontalalignment='right');\ng.add_legend();\n\n\n\n\nThe following code uses pandas to graph the rolling returns and volatility on the same plot for fund_1.\n\n# filtering conditions\nbln_symbol = df_px['symbol'] == 'fund_1'\nbln_nans = ~(np.isnan(df_px['ret_252']))\nbln_filter = bln_symbol & bln_nans\n\n# graphing\ndf_px[bln_filter].plot(x='trade_date', y=['ret_252', 'vol_252']);\n\n\n\n\nCode Challenge Copy and paste the above code and modify it to graph only the data from the beginning of 2019 and onwards.\n\n\nSolution\nbln_symbol = df_px['symbol'] == 'fund_1'\nbln_date = df_px['trade_date'] &gt;= '2019-01-01'\nbln_filter = bln_symbol & bln_date\n\ndf_px[bln_filter].plot(x='trade_date', y=['ret_252', 'vol_252']);"
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#quarterly-returns-and-excess-return-4",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#quarterly-returns-and-excess-return-4",
    "title": "17  Regression: Mutual Fund Analysis",
    "section": "17.9 Quarterly Returns and Excess Return (#4)",
    "text": "17.9 Quarterly Returns and Excess Return (#4)\nOur next analysis objective is to calculate quarterly excess returns for fund_1 and fund_2 relative the benchmark.\nAs we can see, df_px consist of daily data.\n\ndf_px.head()\n\n\n\n\n\n\n\n\nsymbol\ntrade_date\nyear\nquarter\nmonth\nclose\ndaily_ret\nret_252\nvol_252\n\n\n\n\n0\nbenchmark\n2015-12-31\n2015\n4\n12\n203.8700\nNaN\nNaN\nNaN\n\n\n1\nbenchmark\n2016-01-01\n2016\n1\n1\n203.8700\n0.000000\nNaN\nNaN\n\n\n2\nbenchmark\n2016-01-04\n2016\n1\n1\n201.0192\n-0.014082\nNaN\nNaN\n\n\n3\nbenchmark\n2016-01-05\n2016\n1\n1\n201.3600\n0.001694\nNaN\nNaN\n\n\n4\nbenchmark\n2016-01-06\n2016\n1\n1\n198.8200\n-0.012694\nNaN\nNaN\n\n\n\n\n\n\n\nCalculating the quarterly returns is a straight-forward application of .groupby().agg().\n\ndf_quarter = \\\n    df_px[df_px.year &gt; 2015] \\\n        .groupby(['symbol', 'year', 'quarter'])['daily_ret'].agg([np.sum]).reset_index() \\\n        .rename(columns={'sum':'quarterly_ret'})\n\ndf_quarter.head()\n\n\n\n\n\n\n\n\nsymbol\nyear\nquarter\nquarterly_ret\n\n\n\n\n0\nbenchmark\n2016\n1\n0.008061\n\n\n1\nbenchmark\n2016\n2\n0.019061\n\n\n2\nbenchmark\n2016\n3\n0.032062\n\n\n3\nbenchmark\n2016\n4\n0.032879\n\n\n4\nbenchmark\n2017\n1\n0.053184\n\n\n\n\n\n\n\nSince we ultimately want to calculate excess return relative to the benchmark, it would be helpful to have the benchmark returns as a separate column in df_quarter.\nIn order to do this, let’s first separate out the benchmark quarterly returns into a separate DataFrame called df_bench.\n\ndf_bench = \\\n    df_quarter[df_quarter['symbol'] == 'benchmark'] \\\n    [['year', 'quarter', 'quarterly_ret']] \\\n    .rename(columns={'quarterly_ret':'bench_ret'})\n\ndf_bench.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nbench_ret\n\n\n\n\n0\n2016\n1\n0.008061\n\n\n1\n2016\n2\n0.019061\n\n\n2\n2016\n3\n0.032062\n\n\n3\n2016\n4\n0.032879\n\n\n4\n2017\n1\n0.053184\n\n\n\n\n\n\n\nIn order to add the bench_ret column to df_quarter we will utilize a left-join.\n\ndf_excess = \\\n    pd.merge(\n        df_quarter, df_bench, how='left',\n        left_on=['year','quarter'], right_on=['year', 'quarter']\n    )\n\ndf_excess.head()\n\n\n\n\n\n\n\n\nsymbol\nyear\nquarter\nquarterly_ret\nbench_ret\n\n\n\n\n0\nbenchmark\n2016\n1\n0.008061\n0.008061\n\n\n1\nbenchmark\n2016\n2\n0.019061\n0.019061\n\n\n2\nbenchmark\n2016\n3\n0.032062\n0.032062\n\n\n3\nbenchmark\n2016\n4\n0.032879\n0.032879\n\n\n4\nbenchmark\n2017\n1\n0.053184\n0.053184\n\n\n\n\n\n\n\nFinally, we can caluclate excess return for each of the funds.\n\ndf_excess['excess_ret'] = df_excess['quarterly_ret'] - df_excess['bench_ret']\ndf_excess['year_quarter'] = df_excess['year'] * 100 + df_excess['quarter']\ndf_excess = df_excess[['symbol', 'year', 'quarter', 'year_quarter', 'quarterly_ret', 'bench_ret', 'excess_ret']]\ndf_excess.head()\n\n\n\n\n\n\n\n\nsymbol\nyear\nquarter\nyear_quarter\nquarterly_ret\nbench_ret\nexcess_ret\n\n\n\n\n0\nbenchmark\n2016\n1\n201601\n0.008061\n0.008061\n0.0\n\n\n1\nbenchmark\n2016\n2\n201602\n0.019061\n0.019061\n0.0\n\n\n2\nbenchmark\n2016\n3\n201603\n0.032062\n0.032062\n0.0\n\n\n3\nbenchmark\n2016\n4\n201604\n0.032879\n0.032879\n0.0\n\n\n4\nbenchmark\n2017\n1\n201701\n0.053184\n0.053184\n0.0\n\n\n\n\n\n\n\nNext, we create a bar-plot with of the three funds quarterly returns.\n\ng = \\\n    sns.catplot(\n        data=df_excess, x=\"year_quarter\", y=\"quarterly_ret\",\n        hue=\"symbol\", kind=\"bar\",\n        aspect= 1.65, \n    )\ng.fig.suptitle('Fund Quarterly Returns')\ng.set_xticklabels(rotation=35, horizontalalignment='right')\nplt.subplots_adjust(top=0.93);\n\n\n\n\nAnd finally, we create a bar-plot of the excess returns of fund_1 and fund_2.\n\ng = \\\n    sns.catplot(\n        data=df_excess[df_excess.symbol != 'benchmark'],\n        x=\"year_quarter\", y=\"excess_ret\", \n        hue=\"symbol\", kind=\"bar\",\n        aspect=1.5\n    );\ng.fig.suptitle('Quarterly Excess Returns: fund_1 & fund_2')\ng.set_xticklabels(rotation=35, horizontalalignment='right')\nplt.subplots_adjust(top=0.93);"
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#wrangling-returns-data",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#wrangling-returns-data",
    "title": "17  Regression: Mutual Fund Analysis",
    "section": "17.10 Wrangling Returns Data",
    "text": "17.10 Wrangling Returns Data\nFor the remainder of the tutorial, will be helpful to have an untidy DataFrame with the daily returns for the three funds in separate columns.\nLet’s first separate out the returns for each fund into their own DataFrames.\n\ncols = ['trade_date', 'daily_ret']\ndf_bench = \\\n    df_px[df_px.symbol == 'benchmark'][cols].copy().rename(columns={'daily_ret':'benchmark'})\ndf_fund1 = \\\n    df_px[df_px.symbol == 'fund_1'][cols].copy().rename(columns={'daily_ret':'fund_1'})\ndf_fund2 = \\\n    df_px[df_px.symbol == 'fund_2'][cols].copy().rename(columns={'daily_ret':'fund_2'})\n\nNext, we left-join these three DataFrames together into a single variable called df_ret.\n\ndf_ret = \\\n    df_bench \\\n        .merge(right=df_fund1, how='left', left_on='trade_date', right_on='trade_date') \\\n        .merge(right=df_fund2, how='left', left_on='trade_date', right_on='trade_date') \\\n        .query('trade_date &gt; \"2015-12-31\"')\ndf_ret.head()\n\n\n\n\n\n\n\n\ntrade_date\nbenchmark\nfund_1\nfund_2\n\n\n\n\n1\n2016-01-01\n0.000000\n0.000000\n0.000000\n\n\n2\n2016-01-04\n-0.014082\n-0.015635\n-0.012756\n\n\n3\n2016-01-05\n0.001694\n-0.004795\n0.000319\n\n\n4\n2016-01-06\n-0.012694\n-0.013665\n-0.014210\n\n\n5\n2016-01-07\n-0.024284\n-0.032729\n-0.025882"
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#pair-plots-and-correlation-matrix-5",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#pair-plots-and-correlation-matrix-5",
    "title": "17  Regression: Mutual Fund Analysis",
    "section": "17.11 Pair-Plots and Correlation Matrix (#5)",
    "text": "17.11 Pair-Plots and Correlation Matrix (#5)\nIn a subsequent section, we will perform regressions on the two most recent years of data.\nPrior to modeling, I find it useful to look at scatter plots and correlations.\nAs a first step, let’s programmatically calculate the start date for our analysis. We will utilize the numpy.timedelta data structure for this purpose.\n\ndt_curr = df_ret.sort_values(['trade_date'])[-1:]['trade_date'].values[0]\ndt_start = dt_curr - np.timedelta64(730, 'D')\n\nprint(dt_curr)\nprint(dt_start)\n\n2019-09-30T00:00:00.000000000\n2017-09-30T00:00:00.000000000\n\n\nNext, we use the seaborn.pairplot() function to quickly graph all the pairwise correlatior for two years of fund data.\n\ncols = ['benchmark', 'fund_1', 'fund_2']\nsns.pairplot(\n    df_ret[df_ret['trade_date'] &gt; dt_start][cols]\n);\n\n\n\n\nClearly all three of our returns are highly correlated.\nLet’s use the DataFrame.corr() method to calculate the correlations explicitly.\n\ncols = ['benchmark', 'fund_1', 'fund_2']\ndf_ret[df_ret['trade_date'] &gt; dt_start][cols].corr()\n\n\n\n\n\n\n\n\nbenchmark\nfund_1\nfund_2\n\n\n\n\nbenchmark\n1.000000\n0.927068\n0.919859\n\n\nfund_1\n0.927068\n1.000000\n0.836144\n\n\nfund_2\n0.919859\n0.836144\n1.000000\n\n\n\n\n\n\n\n\nCode Challenge: Copy and paste the above correlation code and modify it calculate the correlations for the entirety of the data set.\n\n\nSolution\ncols = ['benchmark', 'fund_1', 'fund_2']\ndf_ret[cols].corr()\n\n\n\n\n\n\n\n\n\nbenchmark\nfund_1\nfund_2\n\n\n\n\nbenchmark\n1.000000\n0.914292\n0.914148\n\n\nfund_1\n0.914292\n1.000000\n0.812403\n\n\nfund_2\n0.914148\n0.812403\n1.000000"
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#regression-of-funds-against-benchmark-6-and-7",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#regression-of-funds-against-benchmark-6-and-7",
    "title": "17  Regression: Mutual Fund Analysis",
    "section": "17.12 Regression of Funds against Benchmark (#6 and #7)",
    "text": "17.12 Regression of Funds against Benchmark (#6 and #7)\nFinally, we will fit regressions of fund_1 and fund_2 against the benchmark.\nIn order to do this, we will utilize the sklearn package which is extremely useful for implementing a variety of machine learning techniques.\nLet’s begin by importing the LinearRegression() constructor function from sklearn.\n\nfrom sklearn.linear_model import LinearRegression\n\nWe again want to restrict our analysis to the most recent two years, so let’s recalculate dt_start just for good measure.\n\ndt_curr = df_ret.sort_values(['trade_date'])[-1:]['trade_date'].values[0]\ndt_start = dt_curr - np.timedelta64(730, 'D')\n\n\n17.12.1 Fund 1\nLet’s begin by regressing fund_1 against the benchmark.\nAs a preliminary step, let’s graph the scatter plot of the returns series.\n\ndf_ret \\\n    [df_ret.trade_date &gt; dt_start] \\\n    .plot.scatter('benchmark', 'fund_1', c='k', figsize=(6, 4));\n\n\n\n\nThe first step in using sklearn for a regression analysis is to instantiate a regression model object as assign it to a variable, which we will call reg_fund1.\n\nreg_fund1 = LinearRegression(fit_intercept=True)\n\nNext, let’s separate out the benchmark returns and the fund_1 returns into their own DataFrames.\n\ndf_bm = df_ret[['benchmark']][df_ret.trade_date &gt; dt_start]\ndf_fund1 = df_ret[['fund_1']][df_ret.trade_date &gt; dt_start]\n\nTo fit the regression we call the .fit() method of our model object reg_fund1.\n\nreg_fund1.fit(X = df_bm, y = df_fund1)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nThe coefficients \\((\\beta)\\) and the y-intercept \\((\\alpha)\\) are attributes of reg_fund1 which we can access as follows:\n\nprint(\"beta:  \" + str(np.round(reg_fund1.coef_[0, 0], 4)))\nprint(\"alpha: \" + str(np.round(reg_fund1.intercept_[0], 4)))\n\nbeta:  1.2982\nalpha: 0.0003\n\n\nEvery class of machine learning model has a .score() method, which gives some kind of accuracy measure. For LinearRegression the .score() gives the \\(R^2\\).\n\nreg_fund1.score(df_ret[['benchmark']], df_ret[['fund_1']])\n\n0.8349465372078845\n\n\nLet’s use pandas plotting along with matplotlib.pyplot to graph our regression line along with the scatter plot of the data:\n\nxfit = np.linspace(-0.05, 0.05, 100)           # range of line\nyfit = reg_fund1.predict(xfit[:, np.newaxis])  # model values in range\n\n\ndf_ret \\\n    [df_ret.trade_date &gt; dt_start] \\\n    .plot.scatter('benchmark', 'fund_1', c='k', figsize=(6, 4));\nplt.plot(xfit, yfit);\n\n/home/pritam/.local/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\n\n\n\n\n\n17.12.2 Fund 2\nLet’s implement the same kind of regression analysis for fund_2.\nScatter Plot of fund_2 vs benchmark\n\ndf_ret\\\n    [df_ret.trade_date &gt; dt_start]\\\n    .plot.scatter('benchmark', 'fund_2', c='k', figsize=(6, 4));\n\n\n\n\nInstantiate a LinearRegression Object\n\nreg_fund2 = LinearRegression(fit_intercept=True)\n\nSeparate out Features and Labels for Regression Analysis\n\ndf_bm = df_ret[['benchmark']][df_ret.trade_date &gt; dt_start]\ndf_fund2 = df_ret[['fund_2']][df_ret.trade_date &gt; dt_start]\n\nFit the Regression Model\n\nreg_fund2.fit(X = df_bm, y = df_fund2)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nPrint Coefficients\n\nprint(reg_fund2.coef_)\nprint(reg_fund2.intercept_)\n\n[[0.94373491]]\n[-0.00015386]\n\n\nCheck the \\(R^2\\)\n\nreg_fund2.score(df_ret[['benchmark']], df_ret[['fund_1']])\n\n0.7823936517887883\n\n\nPlot the Fitted Regression Line\n\nxfit = np.linspace(-0.05, 0.05, 100)           # range of line\nyfit = reg_fund2.predict(xfit[:, np.newaxis])  # model values in range\n\n\ndf_ret \\\n    [df_ret.trade_date &gt; dt_start] \\\n    .plot.scatter('benchmark', 'fund_2', c='k', figsize=(6, 4));\nplt.plot(xfit, yfit);\n\n/home/pritam/.local/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn("
  },
  {
    "objectID": "chapters/18_close_to_close/close_to_close.html#loading-packages",
    "href": "chapters/18_close_to_close/close_to_close.html#loading-packages",
    "title": "18  Volatility Forecasting: Close-to-Close Estimator",
    "section": "18.1 Loading Packages",
    "text": "18.1 Loading Packages\nLet’s begin by loading the packages that we will need.\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr\nimport sklearn\npd.options.display.max_rows = 10"
  },
  {
    "objectID": "chapters/18_close_to_close/close_to_close.html#reading-in-spy-data-from-yahoo-finance",
    "href": "chapters/18_close_to_close/close_to_close.html#reading-in-spy-data-from-yahoo-finance",
    "title": "18  Volatility Forecasting: Close-to-Close Estimator",
    "section": "18.2 Reading-In SPY Data From Yahoo Finance",
    "text": "18.2 Reading-In SPY Data From Yahoo Finance\nSepp’s analysis covers data starting from 1/1/2005 and ending on 4/2/2016. Let’s grab these SPY prices from Yahoo Finance using pandas_datareader.\n\ndf_spy = pdr.get_data_yahoo('SPY', start = '2004-12-31', end = '2016-04-02').reset_index()\ndf_spy.columns = df_spy.columns.str.lower().str.replace(' ', '_')\ndf_spy.rename(columns = {'date':'trade_date'}, inplace = True)\ndf_spy.insert(0, 'ticker', 'SPY')\ndf_spy\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nticker\ntrade_date\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n0\nSPY\n2004-12-31\n121.300003\n121.660004\n120.800003\n120.870003\n84.657806\n28648800\n\n\n1\nSPY\n2005-01-03\n121.559998\n121.760002\n119.900002\n120.300003\n84.258568\n55748000\n\n\n2\nSPY\n2005-01-04\n120.459999\n120.540001\n118.440002\n118.830002\n83.228973\n69167600\n\n\n3\nSPY\n2005-01-05\n118.739998\n119.250000\n118.000000\n118.010002\n82.654648\n65667300\n\n\n4\nSPY\n2005-01-06\n118.440002\n119.150002\n118.260002\n118.610001\n83.074875\n47814700\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2827\nSPY\n2016-03-28\n203.610001\n203.860001\n202.710007\n203.240005\n178.770126\n62408200\n\n\n2828\nSPY\n2016-03-29\n202.759995\n205.250000\n202.399994\n205.119995\n180.423782\n92922900\n\n\n2829\nSPY\n2016-03-30\n206.300003\n206.869995\n205.589996\n206.020004\n181.215378\n86365300\n\n\n2830\nSPY\n2016-03-31\n205.910004\n206.410004\n205.330002\n205.520004\n180.775589\n94584100\n\n\n2831\nSPY\n2016-04-01\n204.350006\n207.139999\n203.979996\n206.919998\n182.007065\n114423500\n\n\n\n\n2832 rows × 8 columns"
  },
  {
    "objectID": "chapters/18_close_to_close/close_to_close.html#calculating-daily-returns-realized-volatility",
    "href": "chapters/18_close_to_close/close_to_close.html#calculating-daily-returns-realized-volatility",
    "title": "18  Volatility Forecasting: Close-to-Close Estimator",
    "section": "18.3 Calculating Daily Returns & Realized Volatility",
    "text": "18.3 Calculating Daily Returns & Realized Volatility\nThe close-to-close estimator is a function of daily returns so let’s calculate those now. In particular, we will use log-returns.\n\ndf_spy['dly_ret'] = np.log(df_spy['close']).diff()\ndf_spy = df_spy[1:].reset_index(drop = True)\ndf_spy\n\n\n\n\n\n\n\n\nticker\ntrade_date\nopen\nhigh\nlow\nclose\nadj_close\nvolume\ndly_ret\n\n\n\n\n0\nSPY\n2005-01-03\n121.559998\n121.760002\n119.900002\n120.300003\n84.258568\n55748000\n-0.004727\n\n\n1\nSPY\n2005-01-04\n120.459999\n120.540001\n118.440002\n118.830002\n83.228973\n69167600\n-0.012295\n\n\n2\nSPY\n2005-01-05\n118.739998\n119.250000\n118.000000\n118.010002\n82.654648\n65667300\n-0.006925\n\n\n3\nSPY\n2005-01-06\n118.440002\n119.150002\n118.260002\n118.610001\n83.074875\n47814700\n0.005071\n\n\n4\nSPY\n2005-01-07\n118.970001\n119.230003\n118.129997\n118.440002\n82.955849\n55847700\n-0.001434\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2826\nSPY\n2016-03-28\n203.610001\n203.860001\n202.710007\n203.240005\n178.770126\n62408200\n0.000591\n\n\n2827\nSPY\n2016-03-29\n202.759995\n205.250000\n202.399994\n205.119995\n180.423782\n92922900\n0.009208\n\n\n2828\nSPY\n2016-03-30\n206.300003\n206.869995\n205.589996\n206.020004\n181.215378\n86365300\n0.004378\n\n\n2829\nSPY\n2016-03-31\n205.910004\n206.410004\n205.330002\n205.520004\n180.775589\n94584100\n-0.002430\n\n\n2830\nSPY\n2016-04-01\n204.350006\n207.139999\n203.979996\n206.919998\n182.007065\n114423500\n0.006789\n\n\n\n\n2831 rows × 9 columns"
  },
  {
    "objectID": "chapters/18_close_to_close/close_to_close.html#organizing-dates-for-backtest",
    "href": "chapters/18_close_to_close/close_to_close.html#organizing-dates-for-backtest",
    "title": "18  Volatility Forecasting: Close-to-Close Estimator",
    "section": "18.4 Organizing Dates for Backtest",
    "text": "18.4 Organizing Dates for Backtest\nOrganizing dates is an important step in a historical analysis.\nWe are performing a weekly analysis, which means that in later steps we will performing aggregation calculations of daily calculations grouped into weeks. Therefore, we will need to add a column to df_spy that will allow us to group by weeks.\nThe key to our approach will be to use the .dt.weekday attribute of the trade_date columns. In the following code, the variable weekday is a Series that contains the weekday associated with each date. Notice that Monday is encoded by 0 and Friday is encoded by 4.\n\nweekday = df_spy['trade_date'].dt.weekday\nweekday\n\n0       0\n1       1\n2       2\n3       3\n4       4\n       ..\n2826    0\n2827    1\n2828    2\n2829    3\n2830    4\nName: trade_date, Length: 2831, dtype: int32\n\n\nThe following code is a simple for-loop that has the effect of creating a week-number for each week.\n\nweek_num = []\nix_week = 0\nweek_num.append(ix_week)\nfor ix in range(0, len(weekday) - 1):\n    prev_day = weekday[ix]\n    curr_day = weekday[ix + 1]\n    if curr_day &lt; prev_day:\n        ix_week = ix_week + 1\n    week_num.append(ix_week)\nnp.array(week_num) # I use the array function simply because it looks better when it prints\n\narray([  0,   0,   0, ..., 586, 586, 586])\n\n\nLet’s now insert the week numbers into df_spy.\n\ndf_spy.insert(2, 'week_num', week_num)\ndf_spy\n\n\n\n\n\n\n\n\nticker\ntrade_date\nweek_num\nopen\nhigh\nlow\nclose\nadj_close\nvolume\ndly_ret\n\n\n\n\n0\nSPY\n2005-01-03\n0\n121.559998\n121.760002\n119.900002\n120.300003\n84.258568\n55748000\n-0.004727\n\n\n1\nSPY\n2005-01-04\n0\n120.459999\n120.540001\n118.440002\n118.830002\n83.228973\n69167600\n-0.012295\n\n\n2\nSPY\n2005-01-05\n0\n118.739998\n119.250000\n118.000000\n118.010002\n82.654648\n65667300\n-0.006925\n\n\n3\nSPY\n2005-01-06\n0\n118.440002\n119.150002\n118.260002\n118.610001\n83.074875\n47814700\n0.005071\n\n\n4\nSPY\n2005-01-07\n0\n118.970001\n119.230003\n118.129997\n118.440002\n82.955849\n55847700\n-0.001434\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2826\nSPY\n2016-03-28\n586\n203.610001\n203.860001\n202.710007\n203.240005\n178.770126\n62408200\n0.000591\n\n\n2827\nSPY\n2016-03-29\n586\n202.759995\n205.250000\n202.399994\n205.119995\n180.423782\n92922900\n0.009208\n\n\n2828\nSPY\n2016-03-30\n586\n206.300003\n206.869995\n205.589996\n206.020004\n181.215378\n86365300\n0.004378\n\n\n2829\nSPY\n2016-03-31\n586\n205.910004\n206.410004\n205.330002\n205.520004\n180.775589\n94584100\n-0.002430\n\n\n2830\nSPY\n2016-04-01\n586\n204.350006\n207.139999\n203.979996\n206.919998\n182.007065\n114423500\n0.006789\n\n\n\n\n2831 rows × 10 columns\n\n\n\n\nDiscussion Question: The pandas.Series.dt.week attribute gives the week-of-the-year for a give trade-date. My initial idea was to use .dt.week and dt.year for my grouping, but I ran into an issue. Can you think what the issue was?\n\n\nSolution\n##&gt; Weeks at the beginning and end of the year may be partial weeks.\n\n\n\nWe can now use .groupby() to calculate the starting and ending dates for each week.\n\ndf_start_end = \\\n    (\n    df_spy.groupby(['week_num'], as_index = False)[['trade_date']].agg([min, max])['trade_date']\n    .rename(columns = {'min':'week_start', 'max':'week_end'})\n    .reset_index()\n    .rename(columns = {'index':'week_num'})\n    )\ndf_start_end\n\n\n\n\n\n\n\n\nweek_num\nweek_start\nweek_end\n\n\n\n\n0\n0\n2005-01-03\n2005-01-07\n\n\n1\n1\n2005-01-10\n2005-01-14\n\n\n2\n2\n2005-01-18\n2005-01-21\n\n\n3\n3\n2005-01-24\n2005-01-28\n\n\n4\n4\n2005-01-31\n2005-02-04\n\n\n...\n...\n...\n...\n\n\n582\n582\n2016-02-29\n2016-03-04\n\n\n583\n583\n2016-03-07\n2016-03-11\n\n\n584\n584\n2016-03-14\n2016-03-18\n\n\n585\n585\n2016-03-21\n2016-03-24\n\n\n586\n586\n2016-03-28\n2016-04-01\n\n\n\n\n587 rows × 3 columns\n\n\n\nLet’s merge these columns into df_spy.\n\ndf_spy = df_spy.merge(df_start_end)\ndf_spy\n\n\n\n\n\n\n\n\nticker\ntrade_date\nweek_num\nopen\nhigh\nlow\nclose\nadj_close\nvolume\ndly_ret\nweek_start\nweek_end\n\n\n\n\n0\nSPY\n2005-01-03\n0\n121.559998\n121.760002\n119.900002\n120.300003\n84.258568\n55748000\n-0.004727\n2005-01-03\n2005-01-07\n\n\n1\nSPY\n2005-01-04\n0\n120.459999\n120.540001\n118.440002\n118.830002\n83.228973\n69167600\n-0.012295\n2005-01-03\n2005-01-07\n\n\n2\nSPY\n2005-01-05\n0\n118.739998\n119.250000\n118.000000\n118.010002\n82.654648\n65667300\n-0.006925\n2005-01-03\n2005-01-07\n\n\n3\nSPY\n2005-01-06\n0\n118.440002\n119.150002\n118.260002\n118.610001\n83.074875\n47814700\n0.005071\n2005-01-03\n2005-01-07\n\n\n4\nSPY\n2005-01-07\n0\n118.970001\n119.230003\n118.129997\n118.440002\n82.955849\n55847700\n-0.001434\n2005-01-03\n2005-01-07\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2826\nSPY\n2016-03-28\n586\n203.610001\n203.860001\n202.710007\n203.240005\n178.770126\n62408200\n0.000591\n2016-03-28\n2016-04-01\n\n\n2827\nSPY\n2016-03-29\n586\n202.759995\n205.250000\n202.399994\n205.119995\n180.423782\n92922900\n0.009208\n2016-03-28\n2016-04-01\n\n\n2828\nSPY\n2016-03-30\n586\n206.300003\n206.869995\n205.589996\n206.020004\n181.215378\n86365300\n0.004378\n2016-03-28\n2016-04-01\n\n\n2829\nSPY\n2016-03-31\n586\n205.910004\n206.410004\n205.330002\n205.520004\n180.775589\n94584100\n-0.002430\n2016-03-28\n2016-04-01\n\n\n2830\nSPY\n2016-04-01\n586\n204.350006\n207.139999\n203.979996\n206.919998\n182.007065\n114423500\n0.006789\n2016-03-28\n2016-04-01\n\n\n\n\n2831 rows × 12 columns"
  },
  {
    "objectID": "chapters/18_close_to_close/close_to_close.html#calculating-weekly-realized-volatility",
    "href": "chapters/18_close_to_close/close_to_close.html#calculating-weekly-realized-volatility",
    "title": "18  Volatility Forecasting: Close-to-Close Estimator",
    "section": "18.5 Calculating Weekly Realized Volatility",
    "text": "18.5 Calculating Weekly Realized Volatility\nNow that we have a week_num associated with each trade_date, we can use .groupby() to calculate the realized volatility.\nThese weekly realized volatilities are the labels that we will be predicting later in our analysis.\n\ndf_realized = \\\n    (\n    df_spy\n        .groupby(['week_num', 'week_start', 'week_end'], as_index = False)[['dly_ret']].agg(lambda x: np.std(x) * np.sqrt(252))\n        .rename(columns = {'dly_ret':'realized_vol'})\n    )\ndf_realized = df_realized[1:]\ndf_realized\n\n\n\n\n\n\n\n\nweek_num\nweek_start\nweek_end\nrealized_vol\n\n\n\n\n1\n1\n2005-01-10\n2005-01-14\n0.093295\n\n\n2\n2\n2005-01-18\n2005-01-21\n0.126557\n\n\n3\n3\n2005-01-24\n2005-01-28\n0.029753\n\n\n4\n4\n2005-01-31\n2005-02-04\n0.069583\n\n\n5\n5\n2005-02-07\n2005-02-11\n0.084567\n\n\n...\n...\n...\n...\n...\n\n\n582\n582\n2016-02-29\n2016-03-04\n0.159055\n\n\n583\n583\n2016-03-07\n2016-03-11\n0.137591\n\n\n584\n584\n2016-03-14\n2016-03-18\n0.057861\n\n\n585\n585\n2016-03-21\n2016-03-24\n0.048135\n\n\n586\n586\n2016-03-28\n2016-04-01\n0.066437\n\n\n\n\n586 rows × 4 columns"
  },
  {
    "objectID": "chapters/18_close_to_close/close_to_close.html#close-to-close-estimator",
    "href": "chapters/18_close_to_close/close_to_close.html#close-to-close-estimator",
    "title": "18  Volatility Forecasting: Close-to-Close Estimator",
    "section": "18.6 Close-to-Close Estimator",
    "text": "18.6 Close-to-Close Estimator\nLet’s now implement the close-to-close estimator.\n\ndef close_to_close(r):\n    T = r.shape[0]\n    r_bar = r.mean()\n    vol = np.sqrt((1 / (T - 1)) * ((r - r_bar) ** 2).sum()) * np.sqrt(252)\n    return(vol)\n\nNotice that close_to_close() is an aggregation function that takes in an array of daily returns and returns back a number. In order to calculate weekly estimates we use close_to_close() as the aggregation function applied to a .groupby().\n\ndf_close_to_close = \\\n    (\n    df_spy\n        .groupby(['week_num', 'week_start', 'week_end'], as_index = False)[['dly_ret']]\n        .agg(close_to_close)\n        .rename(columns = {'dly_ret':'close_to_close'})\n    )\ndf_close_to_close = df_close_to_close[0:-1]\ndf_close_to_close\n\n\n\n\n\n\n\n\nweek_num\nweek_start\nweek_end\nclose_to_close\n\n\n\n\n0\n0\n2005-01-03\n2005-01-07\n0.102492\n\n\n1\n1\n2005-01-10\n2005-01-14\n0.104307\n\n\n2\n2\n2005-01-18\n2005-01-21\n0.146136\n\n\n3\n3\n2005-01-24\n2005-01-28\n0.033265\n\n\n4\n4\n2005-01-31\n2005-02-04\n0.077796\n\n\n...\n...\n...\n...\n...\n\n\n581\n581\n2016-02-22\n2016-02-26\n0.175394\n\n\n582\n582\n2016-02-29\n2016-03-04\n0.177829\n\n\n583\n583\n2016-03-07\n2016-03-11\n0.153831\n\n\n584\n584\n2016-03-14\n2016-03-18\n0.064691\n\n\n585\n585\n2016-03-21\n2016-03-24\n0.055581\n\n\n\n\n586 rows × 4 columns\n\n\n\n\nDiscussion Question: Verify that the .groupby() above works just fine with out including week_start and week_end. If that is the case, then why did I include it?\n\n\nSolution\n(\ndf_spy\n    .groupby(['week_num', 'week_start', 'week_end'], as_index = False)[['dly_ret']]\n    .agg(close_to_close)\n    .rename(columns = {'dly_ret':'close_to_close'})\n)\n\n# It makes the code and the dataframe more readable.\n\n\n\n\n\n\n\n\n\nweek_num\nweek_start\nweek_end\nclose_to_close\n\n\n\n\n0\n0\n2005-01-03\n2005-01-07\n0.102492\n\n\n1\n1\n2005-01-10\n2005-01-14\n0.104307\n\n\n2\n2\n2005-01-18\n2005-01-21\n0.146136\n\n\n3\n3\n2005-01-24\n2005-01-28\n0.033265\n\n\n4\n4\n2005-01-31\n2005-02-04\n0.077796\n\n\n...\n...\n...\n...\n...\n\n\n582\n582\n2016-02-29\n2016-03-04\n0.177829\n\n\n583\n583\n2016-03-07\n2016-03-11\n0.153831\n\n\n584\n584\n2016-03-14\n2016-03-18\n0.064691\n\n\n585\n585\n2016-03-21\n2016-03-24\n0.055581\n\n\n586\n586\n2016-03-28\n2016-04-01\n0.074279\n\n\n\n\n587 rows × 4 columns\n\n\n\n\nCode Challenge: Create an alternative version of our close-to-close function using np.std(). Call the new function close_to_close_std(). Verify that your values match.\n\n\nSolution\ndef close_to_close_std(r):\n    vol = np.std(r, ddof = 1) * np.sqrt(252)\n    return(vol)\n\ndf_std = \\\n    (\n    df_spy\n        .groupby(['week_num', 'week_start', 'week_end'], as_index = False)[['dly_ret']]\n        .agg(close_to_close_std)\n        .rename(columns = {'dly_ret':'close_to_close'})\n    )\n    \ndf_std = df_std[:-1]\nprint(df_std['close_to_close'].sum())\nprint(df_close_to_close['close_to_close'].sum())\n\n\n90.6980642016463\n90.6980642016463\n\n\nIn Sepp 2016, the author uses the \\(R^2\\) between the forecasts and the realized labels as a means of assessing the quality of a particular estimator. Let’s utilize sklearn to do the same.\nWe being by importing the LinearRegression() constructor and instantiating a model.\n\nfrom sklearn.linear_model import LinearRegression\nmdl_reg = LinearRegression(fit_intercept = True)\n\nNext, let’s organize our features and labels.\n\nX = df_close_to_close[['close_to_close']]\ny = df_realized['realized_vol']\n\nWe can now fit the model.\n\nmdl_reg.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nThe .score() method of a LinearRegression model returns the \\(R^2\\).\n\nmdl_reg.score(X, y)\n\n0.4093645253435927\n\n\nAnd we can examine the slope and intercept of our model as follows:\n\nprint(\"Intercept:\", mdl_reg.intercept_)\nprint(\"Slope:   \", mdl_reg.coef_)\n\nIntercept: 0.04933384095053199\nSlope:    [0.57068844]\n\n\n\nDiscussion Question: How do our results compare to Sepp’s?\n\n\nSolution\n# They seem close enough that there probably isn't some error in my calculations.\n# The differences probably come down to differences in data.\n# I do wish the results were a bit closer to feel totally comfortable.\n\n\n\nLet’s also measure the bias and efficiency of the the close-to-close estimator.\n\n# bias\nprint(\"Bias:      \", np.mean(df_close_to_close['close_to_close'] - df_realized['realized_vol']))\n\n# efficiency\nprint(\"Efficiency:\", np.std(df_realized['realized_vol']) / np.std(df_close_to_close['close_to_close']))\n\nBias:       0.01708041424884867\nEfficiency: 0.8919571135609281"
  },
  {
    "objectID": "chapters/19_garch/garch.html#loading-packages",
    "href": "chapters/19_garch/garch.html#loading-packages",
    "title": "19  Volatility Forecasting: GARCH",
    "section": "19.1 Loading Packages",
    "text": "19.1 Loading Packages\nWe begin by loading the packages and functions that we will need.\n\nimport numpy as np\nimport pandas as pd\nfrom arch import arch_model\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr\nimport sys\npd.options.display.max_rows = 10"
  },
  {
    "objectID": "chapters/19_garch/garch.html#spy-data-for-backtest-period",
    "href": "chapters/19_garch/garch.html#spy-data-for-backtest-period",
    "title": "19  Volatility Forecasting: GARCH",
    "section": "19.2 SPY Data for Backtest Period",
    "text": "19.2 SPY Data for Backtest Period\nNext, let’s read-in the data for the backtest period.\n\ndf_spy = pdr.get_data_yahoo('SPY', start = '2004-12-31', end = '2016-04-02').reset_index()\ndf_spy.columns = df_spy.columns.str.lower().str.replace(' ', '_')\ndf_spy.rename(columns = {'date':'trade_date'}, inplace = True)\ndf_spy.insert(0, 'ticker', 'SPY')\ndf_spy\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nticker\ntrade_date\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n0\nSPY\n2004-12-31\n121.300003\n121.660004\n120.800003\n120.870003\n84.657822\n28648800\n\n\n1\nSPY\n2005-01-03\n121.559998\n121.760002\n119.900002\n120.300003\n84.258591\n55748000\n\n\n2\nSPY\n2005-01-04\n120.459999\n120.540001\n118.440002\n118.830002\n83.229019\n69167600\n\n\n3\nSPY\n2005-01-05\n118.739998\n119.250000\n118.000000\n118.010002\n82.654678\n65667300\n\n\n4\nSPY\n2005-01-06\n118.440002\n119.150002\n118.260002\n118.610001\n83.074913\n47814700\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2827\nSPY\n2016-03-28\n203.610001\n203.860001\n202.710007\n203.240005\n178.770111\n62408200\n\n\n2828\nSPY\n2016-03-29\n202.759995\n205.250000\n202.399994\n205.119995\n180.423782\n92922900\n\n\n2829\nSPY\n2016-03-30\n206.300003\n206.869995\n205.589996\n206.020004\n181.215408\n86365300\n\n\n2830\nSPY\n2016-03-31\n205.910004\n206.410004\n205.330002\n205.520004\n180.775589\n94584100\n\n\n2831\nSPY\n2016-04-01\n204.350006\n207.139999\n203.979996\n206.919998\n182.007034\n114423500\n\n\n\n\n2832 rows × 8 columns\n\n\n\nWe will need the daily log returns, so let’s calculate those now.\n\ndf_spy['dly_ret'] = np.log(df_spy['adj_close']).diff()\ndf_spy = df_spy.dropna().reset_index(drop = True)\ndf_spy\n\n\n\n\n\n\n\n\nticker\ntrade_date\nopen\nhigh\nlow\nclose\nadj_close\nvolume\ndly_ret\n\n\n\n\n0\nSPY\n2005-01-03\n121.559998\n121.760002\n119.900002\n120.300003\n84.258591\n55748000\n-0.004727\n\n\n1\nSPY\n2005-01-04\n120.459999\n120.540001\n118.440002\n118.830002\n83.229019\n69167600\n-0.012294\n\n\n2\nSPY\n2005-01-05\n118.739998\n119.250000\n118.000000\n118.010002\n82.654678\n65667300\n-0.006925\n\n\n3\nSPY\n2005-01-06\n118.440002\n119.150002\n118.260002\n118.610001\n83.074913\n47814700\n0.005071\n\n\n4\nSPY\n2005-01-07\n118.970001\n119.230003\n118.129997\n118.440002\n82.955826\n55847700\n-0.001435\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2826\nSPY\n2016-03-28\n203.610001\n203.860001\n202.710007\n203.240005\n178.770111\n62408200\n0.000591\n\n\n2827\nSPY\n2016-03-29\n202.759995\n205.250000\n202.399994\n205.119995\n180.423782\n92922900\n0.009208\n\n\n2828\nSPY\n2016-03-30\n206.300003\n206.869995\n205.589996\n206.020004\n181.215408\n86365300\n0.004378\n\n\n2829\nSPY\n2016-03-31\n205.910004\n206.410004\n205.330002\n205.520004\n180.775589\n94584100\n-0.002430\n\n\n2830\nSPY\n2016-04-01\n204.350006\n207.139999\n203.979996\n206.919998\n182.007034\n114423500\n0.006789\n\n\n\n\n2831 rows × 9 columns"
  },
  {
    "objectID": "chapters/19_garch/garch.html#organizing-dates",
    "href": "chapters/19_garch/garch.html#organizing-dates",
    "title": "19  Volatility Forecasting: GARCH",
    "section": "19.3 Organizing Dates",
    "text": "19.3 Organizing Dates\nLet’s organize our backtest period into weeks (Monday - Friday), since our analysis is of weekly volatility predictions.\n\n# adding a new columns consisting of the weekday number for each trade_date \nweekday = df_spy['trade_date'].dt.weekday\n\n# assigning a week number to each trade-date; M-F is all the same week\nweek_num = []\nix_week = 0\nweek_num.append(ix_week)\nfor ix in range(0, len(weekday) - 1):\n    prev_day = weekday.iloc[ix]\n    curr_day = weekday.iloc[ix + 1]\n    if curr_day &lt; prev_day:\n        ix_week = ix_week + 1\n    week_num.append(ix_week)\n\n# inserting the week-number column into df_spy\ndf_spy.insert(2, 'week_num', week_num)\ndf_spy\n\n\n\n\n\n\n\n\nticker\ntrade_date\nweek_num\nopen\nhigh\nlow\nclose\nadj_close\nvolume\ndly_ret\n\n\n\n\n0\nSPY\n2005-01-03\n0\n121.559998\n121.760002\n119.900002\n120.300003\n84.258591\n55748000\n-0.004727\n\n\n1\nSPY\n2005-01-04\n0\n120.459999\n120.540001\n118.440002\n118.830002\n83.229019\n69167600\n-0.012294\n\n\n2\nSPY\n2005-01-05\n0\n118.739998\n119.250000\n118.000000\n118.010002\n82.654678\n65667300\n-0.006925\n\n\n3\nSPY\n2005-01-06\n0\n118.440002\n119.150002\n118.260002\n118.610001\n83.074913\n47814700\n0.005071\n\n\n4\nSPY\n2005-01-07\n0\n118.970001\n119.230003\n118.129997\n118.440002\n82.955826\n55847700\n-0.001435\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2826\nSPY\n2016-03-28\n586\n203.610001\n203.860001\n202.710007\n203.240005\n178.770111\n62408200\n0.000591\n\n\n2827\nSPY\n2016-03-29\n586\n202.759995\n205.250000\n202.399994\n205.119995\n180.423782\n92922900\n0.009208\n\n\n2828\nSPY\n2016-03-30\n586\n206.300003\n206.869995\n205.589996\n206.020004\n181.215408\n86365300\n0.004378\n\n\n2829\nSPY\n2016-03-31\n586\n205.910004\n206.410004\n205.330002\n205.520004\n180.775589\n94584100\n-0.002430\n\n\n2830\nSPY\n2016-04-01\n586\n204.350006\n207.139999\n203.979996\n206.919998\n182.007034\n114423500\n0.006789\n\n\n\n\n2831 rows × 10 columns\n\n\n\nThe following code generates a DataFrame that contains the start-date and end-date for each week.\n\ndf_start_end = \\\n    (\n    df_spy.groupby(['week_num'], as_index = False)[['trade_date']].agg([min, max])['trade_date']\n    .rename(columns = {'min':'week_start', 'max':'week_end'})\n    .reset_index()\n    .rename(columns = {'index':'week_num'})\n    )\ndf_start_end\n\n\n\n\n\n\n\n\nweek_num\nweek_start\nweek_end\n\n\n\n\n0\n0\n2005-01-03\n2005-01-07\n\n\n1\n1\n2005-01-10\n2005-01-14\n\n\n2\n2\n2005-01-18\n2005-01-21\n\n\n3\n3\n2005-01-24\n2005-01-28\n\n\n4\n4\n2005-01-31\n2005-02-04\n\n\n...\n...\n...\n...\n\n\n582\n582\n2016-02-29\n2016-03-04\n\n\n583\n583\n2016-03-07\n2016-03-11\n\n\n584\n584\n2016-03-14\n2016-03-18\n\n\n585\n585\n2016-03-21\n2016-03-24\n\n\n586\n586\n2016-03-28\n2016-04-01\n\n\n\n\n587 rows × 3 columns\n\n\n\nAnd finally let’s join the weekly start/end dates into df_spy.\n\ndf_spy = df_spy.merge(df_start_end)\ndf_spy\n\n\n\n\n\n\n\n\nticker\ntrade_date\nweek_num\nopen\nhigh\nlow\nclose\nadj_close\nvolume\ndly_ret\nweek_start\nweek_end\n\n\n\n\n0\nSPY\n2005-01-03\n0\n121.559998\n121.760002\n119.900002\n120.300003\n84.258591\n55748000\n-0.004727\n2005-01-03\n2005-01-07\n\n\n1\nSPY\n2005-01-04\n0\n120.459999\n120.540001\n118.440002\n118.830002\n83.229019\n69167600\n-0.012294\n2005-01-03\n2005-01-07\n\n\n2\nSPY\n2005-01-05\n0\n118.739998\n119.250000\n118.000000\n118.010002\n82.654678\n65667300\n-0.006925\n2005-01-03\n2005-01-07\n\n\n3\nSPY\n2005-01-06\n0\n118.440002\n119.150002\n118.260002\n118.610001\n83.074913\n47814700\n0.005071\n2005-01-03\n2005-01-07\n\n\n4\nSPY\n2005-01-07\n0\n118.970001\n119.230003\n118.129997\n118.440002\n82.955826\n55847700\n-0.001435\n2005-01-03\n2005-01-07\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2826\nSPY\n2016-03-28\n586\n203.610001\n203.860001\n202.710007\n203.240005\n178.770111\n62408200\n0.000591\n2016-03-28\n2016-04-01\n\n\n2827\nSPY\n2016-03-29\n586\n202.759995\n205.250000\n202.399994\n205.119995\n180.423782\n92922900\n0.009208\n2016-03-28\n2016-04-01\n\n\n2828\nSPY\n2016-03-30\n586\n206.300003\n206.869995\n205.589996\n206.020004\n181.215408\n86365300\n0.004378\n2016-03-28\n2016-04-01\n\n\n2829\nSPY\n2016-03-31\n586\n205.910004\n206.410004\n205.330002\n205.520004\n180.775589\n94584100\n-0.002430\n2016-03-28\n2016-04-01\n\n\n2830\nSPY\n2016-04-01\n586\n204.350006\n207.139999\n203.979996\n206.919998\n182.007034\n114423500\n0.006789\n2016-03-28\n2016-04-01\n\n\n\n\n2831 rows × 12 columns"
  },
  {
    "objectID": "chapters/19_garch/garch.html#calculating-realized-volatility",
    "href": "chapters/19_garch/garch.html#calculating-realized-volatility",
    "title": "19  Volatility Forecasting: GARCH",
    "section": "19.4 Calculating Realized Volatility",
    "text": "19.4 Calculating Realized Volatility\nThis code cell calculates the realized volatility for each week. In machine learning parlance, these are the labels that we are trying to predict.\n\ndf_realized = \\\n    (\n    df_spy\n        .groupby(['week_num', 'week_start', 'week_end'], as_index = False)[['dly_ret']].agg(lambda x: np.std(x) * np.sqrt(252))\n        .rename(columns = {'dly_ret':'realized_vol'})\n    )\ndf_realized = df_realized[1:]\ndf_realized\n\n\n\n\n\n\n\n\nweek_num\nweek_start\nweek_end\nrealized_vol\n\n\n\n\n1\n1\n2005-01-10\n2005-01-14\n0.093297\n\n\n2\n2\n2005-01-18\n2005-01-21\n0.126558\n\n\n3\n3\n2005-01-24\n2005-01-28\n0.029752\n\n\n4\n4\n2005-01-31\n2005-02-04\n0.069585\n\n\n5\n5\n2005-02-07\n2005-02-11\n0.084566\n\n\n...\n...\n...\n...\n...\n\n\n582\n582\n2016-02-29\n2016-03-04\n0.159052\n\n\n583\n583\n2016-03-07\n2016-03-11\n0.137589\n\n\n584\n584\n2016-03-14\n2016-03-18\n0.054367\n\n\n585\n585\n2016-03-21\n2016-03-24\n0.048135\n\n\n586\n586\n2016-03-28\n2016-04-01\n0.066439\n\n\n\n\n586 rows × 4 columns"
  },
  {
    "objectID": "chapters/19_garch/garch.html#garch-training-data",
    "href": "chapters/19_garch/garch.html#garch-training-data",
    "title": "19  Volatility Forecasting: GARCH",
    "section": "19.5 GARCH Training Data",
    "text": "19.5 GARCH Training Data\nA practical rule of thumb for fitting garch models to equity index returns is that you should use 5-10 years worth of data. In this tutorial we will use 10-years worth.\nLet’s begin by grabbing all the data that we will need for fitting.\n\ndf_train = pdr.get_data_yahoo('SPY', start = '1994-12-30', end = '2016-04-02').reset_index()\ndf_train.columns = df_train.columns.str.lower().str.replace(' ', '_')\ndf_train.rename(columns = {'date':'trade_date'}, inplace = True)\ndf_train.insert(0, 'ticker', 'SPY')\ndf_train\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nticker\ntrade_date\nopen\nhigh\nlow\nclose\nadj_close\nvolume\n\n\n\n\n0\nSPY\n1994-12-30\n46.203125\n46.250000\n45.562500\n45.562500\n27.317329\n2209500\n\n\n1\nSPY\n1995-01-03\n45.703125\n45.843750\n45.687500\n45.781250\n27.448469\n324300\n\n\n2\nSPY\n1995-01-04\n45.984375\n46.000000\n45.750000\n46.000000\n27.579622\n351800\n\n\n3\nSPY\n1995-01-05\n46.031250\n46.109375\n45.953125\n46.000000\n27.579622\n89800\n\n\n4\nSPY\n1995-01-06\n46.093750\n46.250000\n45.906250\n46.046875\n27.607731\n448400\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5346\nSPY\n2016-03-28\n203.610001\n203.860001\n202.710007\n203.240005\n178.770142\n62408200\n\n\n5347\nSPY\n2016-03-29\n202.759995\n205.250000\n202.399994\n205.119995\n180.423767\n92922900\n\n\n5348\nSPY\n2016-03-30\n206.300003\n206.869995\n205.589996\n206.020004\n181.215424\n86365300\n\n\n5349\nSPY\n2016-03-31\n205.910004\n206.410004\n205.330002\n205.520004\n180.775589\n94584100\n\n\n5350\nSPY\n2016-04-01\n204.350006\n207.139999\n203.979996\n206.919998\n182.007050\n114423500\n\n\n\n\n5351 rows × 8 columns\n\n\n\nNext we calculate the daily returns.\n\ndf_train['dly_ret'] = np.log(df_train['adj_close']).diff()\ndf_train.dropna(inplace = True)\ndf_train.reset_index(drop = True, inplace = True)\ndf_train\n\n\n\n\n\n\n\n\nticker\ntrade_date\nopen\nhigh\nlow\nclose\nadj_close\nvolume\ndly_ret\n\n\n\n\n0\nSPY\n1995-01-03\n45.703125\n45.843750\n45.687500\n45.781250\n27.448469\n324300\n0.004789\n\n\n1\nSPY\n1995-01-04\n45.984375\n46.000000\n45.750000\n46.000000\n27.579622\n351800\n0.004767\n\n\n2\nSPY\n1995-01-05\n46.031250\n46.109375\n45.953125\n46.000000\n27.579622\n89800\n0.000000\n\n\n3\nSPY\n1995-01-06\n46.093750\n46.250000\n45.906250\n46.046875\n27.607731\n448400\n0.001019\n\n\n4\nSPY\n1995-01-09\n46.031250\n46.093750\n46.000000\n46.093750\n27.635849\n36800\n0.001018\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5345\nSPY\n2016-03-28\n203.610001\n203.860001\n202.710007\n203.240005\n178.770142\n62408200\n0.000591\n\n\n5346\nSPY\n2016-03-29\n202.759995\n205.250000\n202.399994\n205.119995\n180.423767\n92922900\n0.009207\n\n\n5347\nSPY\n2016-03-30\n206.300003\n206.869995\n205.589996\n206.020004\n181.215424\n86365300\n0.004378\n\n\n5348\nSPY\n2016-03-31\n205.910004\n206.410004\n205.330002\n205.520004\n180.775589\n94584100\n-0.002430\n\n\n5349\nSPY\n2016-04-01\n204.350006\n207.139999\n203.979996\n206.919998\n182.007050\n114423500\n0.006789\n\n\n\n\n5350 rows × 9 columns\n\n\n\nThe GARCH fitting process has better convergence if we express the returns as percents rather than decimals.\n\nser_returns = df_train['dly_ret'] * 100\nser_returns.index = df_train['trade_date']\n\nNext, we instantiate the model.\n\nmodel = arch_model(ser_returns, vol = 'Garch', p = 1, o = 0, q = 1, dist = 'Normal')\nresult = model.fit(update_freq = 5)\n\nIteration:      5,   Func. Count:     37,   Neg. LLF: 7624.3614767949375\nIteration:     10,   Func. Count:     65,   Neg. LLF: 7598.333575044246\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: 7598.333533861198\n            Iterations: 12\n            Function evaluations: 75\n            Gradient evaluations: 12\n\n\nThe following code loops through and performs the fitting for each day. It takes a while to run, so I’ll leave it to you to run it and examine on your own time. See the package documentation linked below for details - you will probably need to read those in order to complete the associated project.\n\n# ix_start = df_train.query('trade_date == trade_date.min()').index[0]\n# ix_end = df_train.query('trade_date == \"2004-12-31\"').index[0]\n# forecasts = {}\n# for ix in range(2518, 5349):\n#     sys.stdout.write('.')\n#     sys.stdout.flush()\n#     result = model.fit(first_obs = (ix - 2518), last_obs = ix, disp = 'off')\n#     temp = result.forecast(horizon = 5, reindex = True).variance\n#     fcast = temp.iloc[ix - 1]\n#     forecasts[fcast.name] = fcast\n# print()\n# df_forecast = pd.DataFrame(pd.DataFrame(forecasts).T)\n# df_forecast = df_forecast.reset_index().rename(columns = {'index':'trade_date'})\n# df_forecast\n\n# # writing variance estimates to csv-file\n# df_forecast.to_csv('variance_forecast.csv', index = False)\n\nThe above code saves the variance estimates to a CSV file, and we will read those in now.\n\ndf_forecast = pd.read_csv('variance_forecast.csv')\ndf_forecast['trade_date'] = pd.to_datetime(df_forecast['trade_date'])\ndf_forecast\n\n\n\n\n\n\n\n\ntrade_date\nh.1\nh.2\nh.3\nh.4\nh.5\n\n\n\n\n0\n2004-12-30\n0.312152\n0.320800\n0.329415\n0.337999\n0.346551\n\n\n1\n2004-12-31\n0.303715\n0.312426\n0.321104\n0.329750\n0.338364\n\n\n2\n2005-01-03\n0.313479\n0.322231\n0.330949\n0.339635\n0.348287\n\n\n3\n2005-01-04\n0.433124\n0.441654\n0.450148\n0.458607\n0.467031\n\n\n4\n2005-01-05\n0.453912\n0.462365\n0.470782\n0.479165\n0.487513\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2826\n2016-03-23\n0.578104\n0.593060\n0.607729\n0.622115\n0.636224\n\n\n2827\n2016-03-24\n0.527753\n0.543716\n0.559369\n0.574720\n0.589773\n\n\n2828\n2016-03-28\n0.482408\n0.499278\n0.515820\n0.532041\n0.547947\n\n\n2829\n2016-03-29\n0.526633\n0.542651\n0.558358\n0.573760\n0.588863\n\n\n2830\n2016-03-30\n0.496876\n0.513525\n0.529849\n0.545856\n0.561551\n\n\n\n\n2831 rows × 6 columns\n\n\n\nThe columns of the df_forecast are the one day variance estimates for the next five days. In order to calculate a 5-day volatility forecasts we will add up these columns, divide by 100 (the variances are stated as percentages), and then take a square-root.\n\ndf_forecast['volatility_forecast'] = \\\n    np.sqrt((df_forecast['h.1'] + df_forecast['h.2'] + df_forecast['h.3'] + df_forecast['h.4'] + df_forecast['h.5']) / 100)\ndf_forecast\n\n\n\n\n\n\n\n\ntrade_date\nh.1\nh.2\nh.3\nh.4\nh.5\nvolatility_forecast\n\n\n\n\n0\n2004-12-30\n0.312152\n0.320800\n0.329415\n0.337999\n0.346551\n0.128332\n\n\n1\n2004-12-31\n0.303715\n0.312426\n0.321104\n0.329750\n0.338364\n0.126703\n\n\n2\n2005-01-03\n0.313479\n0.322231\n0.330949\n0.339635\n0.348287\n0.128630\n\n\n3\n2005-01-04\n0.433124\n0.441654\n0.450148\n0.458607\n0.467031\n0.150019\n\n\n4\n2005-01-05\n0.453912\n0.462365\n0.470782\n0.479165\n0.487513\n0.153419\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2826\n2016-03-23\n0.578104\n0.593060\n0.607729\n0.622115\n0.636224\n0.174277\n\n\n2827\n2016-03-24\n0.527753\n0.543716\n0.559369\n0.574720\n0.589773\n0.167192\n\n\n2828\n2016-03-28\n0.482408\n0.499278\n0.515820\n0.532041\n0.547947\n0.160546\n\n\n2829\n2016-03-29\n0.526633\n0.542651\n0.558358\n0.573760\n0.588863\n0.167041\n\n\n2830\n2016-03-30\n0.496876\n0.513525\n0.529849\n0.545856\n0.561551\n0.162716\n\n\n\n\n2831 rows × 7 columns\n\n\n\nLet’s plot these forecasts to make sure that they look reasonable.\n\ndf_forecast.plot(x = 'trade_date', y = 'volatility_forecast', figsize = (12, 6));\n\n\n\n\nWe actually have far more forecasts than we need, because we calculated forecasts for each day. We can isolate the ones in question by using a join.\n\ndf_start_end.merge(df_forecast[['trade_date', 'volatility_forecast']], left_on = 'week_end', right_on = 'trade_date')\n\n\n\n\n\n\n\n\nweek_num\nweek_start\nweek_end\ntrade_date\nvolatility_forecast\n\n\n\n\n0\n0\n2005-01-03\n2005-01-07\n2005-01-07\n0.147481\n\n\n1\n1\n2005-01-10\n2005-01-14\n2005-01-14\n0.150795\n\n\n2\n2\n2005-01-18\n2005-01-21\n2005-01-21\n0.170141\n\n\n3\n3\n2005-01-24\n2005-01-28\n2005-01-28\n0.147867\n\n\n4\n4\n2005-01-31\n2005-02-04\n2005-02-04\n0.151963\n\n\n...\n...\n...\n...\n...\n...\n\n\n581\n581\n2016-02-22\n2016-02-26\n2016-02-26\n0.250594\n\n\n582\n582\n2016-02-29\n2016-03-04\n2016-03-04\n0.242543\n\n\n583\n583\n2016-03-07\n2016-03-11\n2016-03-11\n0.230967\n\n\n584\n584\n2016-03-14\n2016-03-18\n2016-03-18\n0.188825\n\n\n585\n585\n2016-03-21\n2016-03-24\n2016-03-24\n0.167192\n\n\n\n\n586 rows × 5 columns\n\n\n\nLet’s grab the forecasts and put them into their own variable.\n\nvolatility_forecasts = df_start_end.merge(df_forecast, left_on = 'week_end', right_on = 'trade_date')['volatility_forecast']\nvolatility_forecasts\n\n0      0.147481\n1      0.150795\n2      0.170141\n3      0.147867\n4      0.151963\n         ...   \n581    0.250594\n582    0.242543\n583    0.230967\n584    0.188825\n585    0.167192\nName: volatility_forecast, Length: 586, dtype: float64\n\n\nNext, let’s use the \\(R^2\\) metric that Sepp uses in his studies.\n\nnp.corrcoef(volatility_forecasts, df_realized['realized_vol'])[0,1] ** 2\n\n0.5612564282964889\n\n\nAnd finally, let’s calculate the bias and efficiency.\n\n# bias\nprint(np.mean(volatility_forecasts - df_realized['realized_vol']))\n\n# efficiency\nprint(np.std(df_realized['realized_vol']) / np.std(volatility_forecasts))\n\n0.1028763057503257\n0.8233056504414178"
  },
  {
    "objectID": "chapters/19_garch/garch.html#references",
    "href": "chapters/19_garch/garch.html#references",
    "title": "19  Volatility Forecasting: GARCH",
    "section": "19.6 References",
    "text": "19.6 References\nDoes Anything Beat Garch(1, 1) - Hansen and Lunde 2004\nOptions, Futures, and Other Derivatives 9th Edition - John Hull (Chapter 23)\nVolatility Modeling and Trading - Artur Sepp 2016\nhttps://arch.readthedocs.io/en/latest/univariate/univariate_volatility_forecasting.html"
  },
  {
    "objectID": "chapters/20_feed_grains_database/feed_grains_database.html#import-pacakges",
    "href": "chapters/20_feed_grains_database/feed_grains_database.html#import-pacakges",
    "title": "20  Exploration of the Feed Grains Database",
    "section": "20.1 Import Pacakges",
    "text": "20.1 Import Pacakges\nLet’s begin by loading the packages that we will need.\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "chapters/20_feed_grains_database/feed_grains_database.html#read-in-data",
    "href": "chapters/20_feed_grains_database/feed_grains_database.html#read-in-data",
    "title": "20  Exploration of the Feed Grains Database",
    "section": "20.2 Read-In Data",
    "text": "20.2 Read-In Data\nNext, let’s read-in our data.\n\ndf_feed_grains = pd.read_csv('../data/FeedGrains.csv')\ndf_feed_grains\n\n\n\n\n\n\n\n\nSC_Group_ID\nSC_Group_Desc\nSC_GroupCommod_ID\nSC_GroupCommod_Desc\nSC_Geography_ID\nSortOrder\nSC_GeographyIndented_Desc\nSC_Commodity_ID\nSC_Commodity_Desc\nSC_Attribute_ID\nSC_Attribute_Desc\nSC_Unit_ID\nSC_Unit_Desc\nYear_ID\nSC_Frequency_ID\nSC_Frequency_Desc\nTimeperiod_ID\nTimeperiod_Desc\nAmount\n\n\n\n\n0\n2\nSupply and use\n9.0\nBarley\n1\n0.80\nUnited States\n1\nBarley\n1\nPlanted acreage\n2\nMillion acres\n1926\n3\nAnnual\n69\nCommodity Market Year\n8.796000\n\n\n1\n2\nSupply and use\n9.0\nBarley\n1\n0.80\nUnited States\n1\nBarley\n1\nPlanted acreage\n2\nMillion acres\n1927\n3\nAnnual\n69\nCommodity Market Year\n9.513000\n\n\n2\n2\nSupply and use\n9.0\nBarley\n1\n0.80\nUnited States\n1\nBarley\n1\nPlanted acreage\n2\nMillion acres\n1928\n3\nAnnual\n69\nCommodity Market Year\n12.828000\n\n\n3\n2\nSupply and use\n9.0\nBarley\n1\n0.80\nUnited States\n1\nBarley\n1\nPlanted acreage\n2\nMillion acres\n1929\n3\nAnnual\n69\nCommodity Market Year\n14.703000\n\n\n4\n2\nSupply and use\n9.0\nBarley\n1\n0.80\nUnited States\n1\nBarley\n1\nPlanted acreage\n2\nMillion acres\n1930\n3\nAnnual\n69\nCommodity Market Year\n13.581000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n496558\n3\nExports and imports\n17.0\nOats\n300\n1.02\nCaribbean Basin (CBERA)\n79\nOats products\n24\nExports, from U.S. to specified destination\n7\n1,000 metric tons\n2022\n1\nMonthly\n12\nDec\n0.063158\n\n\n496559\n3\nExports and imports\n17.0\nOats\n300\n1.02\nCaribbean Basin (CBERA)\n79\nOats products\n24\nExports, from U.S. to specified destination\n7\n1,000 metric tons\n2022\n3\nAnnual\n19\nMY Jun-May\n0.442165\n\n\n496560\n3\nExports and imports\n17.0\nOats\n300\n1.02\nCaribbean Basin (CBERA)\n79\nOats products\n24\nExports, from U.S. to specified destination\n7\n1,000 metric tons\n2023\n1\nMonthly\n1\nJan\n0.051325\n\n\n496561\n3\nExports and imports\n17.0\nOats\n300\n1.02\nCaribbean Basin (CBERA)\n79\nOats products\n24\nExports, from U.S. to specified destination\n7\n1,000 metric tons\n2023\n1\nMonthly\n2\nFeb\n0.036997\n\n\n496562\n3\nExports and imports\n17.0\nOats\n300\n1.02\nCaribbean Basin (CBERA)\n79\nOats products\n24\nExports, from U.S. to specified destination\n7\n1,000 metric tons\n2023\n1\nMonthly\n3\nMar\n0.022716\n\n\n\n\n496563 rows × 19 columns"
  },
  {
    "objectID": "chapters/20_feed_grains_database/feed_grains_database.html#production",
    "href": "chapters/20_feed_grains_database/feed_grains_database.html#production",
    "title": "20  Exploration of the Feed Grains Database",
    "section": "20.3 Production",
    "text": "20.3 Production\nLet’s grab all the attributes that are related to production side of the WASDE balance sheet analysis. We have to do this in two separate parts because the Timeperiod_Desc is different for some of the attributes.\n\nattributes = ['Harvested acreage', 'Yield per harvested acre', 'Planted acreage',\n       'Prices received by farmers', 'Production', 'Imports', 'Total Supply']\n\ndf_tidy_corn = \\\n(\ndf_feed_grains\n    .query('SC_Commodity_Desc == \"Corn\"')\n    .query('SC_GeographyIndented_Desc == \"United States\"')\n    .query('SC_Attribute_Desc == @attributes')\n    .query('Timeperiod_Desc == \"Commodity Market Year\"')\n    [['SC_Commodity_Desc', 'SC_GeographyIndented_Desc', 'SC_Attribute_Desc', 'Timeperiod_Desc','Year_ID', 'Amount', 'SC_Unit_Desc']]\n)\ndf_tidy_corn\n\n\n\n\n\n\n\n\nSC_Commodity_Desc\nSC_GeographyIndented_Desc\nSC_Attribute_Desc\nTimeperiod_Desc\nYear_ID\nAmount\nSC_Unit_Desc\n\n\n\n\n13840\nCorn\nUnited States\nHarvested acreage\nCommodity Market Year\n1866\n30.017\nMillion acres\n\n\n13841\nCorn\nUnited States\nHarvested acreage\nCommodity Market Year\n1867\n32.116\nMillion acres\n\n\n13842\nCorn\nUnited States\nHarvested acreage\nCommodity Market Year\n1868\n35.116\nMillion acres\n\n\n13843\nCorn\nUnited States\nHarvested acreage\nCommodity Market Year\n1869\n35.833\nMillion acres\n\n\n13844\nCorn\nUnited States\nHarvested acreage\nCommodity Market Year\n1870\n38.388\nMillion acres\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15962\nCorn\nUnited States\nPrices received by farmers\nCommodity Market Year\n2019\n3.560\nDollars per bushel\n\n\n15979\nCorn\nUnited States\nPrices received by farmers\nCommodity Market Year\n2020\n4.530\nDollars per bushel\n\n\n15996\nCorn\nUnited States\nPrices received by farmers\nCommodity Market Year\n2021\n6.000\nDollars per bushel\n\n\n16007\nCorn\nUnited States\nPrices received by farmers\nCommodity Market Year\n2022\n6.600\nDollars per bushel\n\n\n16014\nCorn\nUnited States\nPrices received by farmers\nCommodity Market Year\n2023\n4.800\nDollars per bushel\n\n\n\n\n730 rows × 7 columns\n\n\n\n\nattributes = ['Imports, market year', 'Beginning stocks']\n\ndf_imports_stocks = \\\n    (\n    df_feed_grains\n        .query('SC_Commodity_Desc == \"Corn\"')\n        .query('SC_GeographyIndented_Desc == \"United States\"')\n        .query('SC_Attribute_Desc == @attributes')\n        .query('Timeperiod_Desc.str.contains(\"MY\")')\n        [['SC_Commodity_Desc', 'SC_GeographyIndented_Desc', 'SC_Attribute_Desc', 'Timeperiod_Desc','Year_ID', 'Amount', 'SC_Unit_Desc']]\n    )\n\ndf_tidy_corn = pd.concat([df_tidy_corn, df_imports_stocks])\ndf_tidy_corn\n\n\n\n\n\n\n\n\nSC_Commodity_Desc\nSC_GeographyIndented_Desc\nSC_Attribute_Desc\nTimeperiod_Desc\nYear_ID\nAmount\nSC_Unit_Desc\n\n\n\n\n13840\nCorn\nUnited States\nHarvested acreage\nCommodity Market Year\n1866\n30.017\nMillion acres\n\n\n13841\nCorn\nUnited States\nHarvested acreage\nCommodity Market Year\n1867\n32.116\nMillion acres\n\n\n13842\nCorn\nUnited States\nHarvested acreage\nCommodity Market Year\n1868\n35.116\nMillion acres\n\n\n13843\nCorn\nUnited States\nHarvested acreage\nCommodity Market Year\n1869\n35.833\nMillion acres\n\n\n13844\nCorn\nUnited States\nHarvested acreage\nCommodity Market Year\n1870\n38.388\nMillion acres\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16416\nCorn\nUnited States\nImports, market year\nMY Sep-Aug\n2019\n41.885\nMillion bushels\n\n\n16421\nCorn\nUnited States\nImports, market year\nMY Sep-Aug\n2020\n24.233\nMillion bushels\n\n\n16516\nCorn\nUnited States\nImports, market year\nMY Sep-Aug\n2021\n24.227\nMillion bushels\n\n\n16519\nCorn\nUnited States\nImports, market year\nMY Sep-Aug\n2022\n40.000\nMillion bushels\n\n\n16520\nCorn\nUnited States\nImports, market year\nMY Sep-Aug\n2023\n25.000\nMillion bushels\n\n\n\n\n828 rows × 7 columns\n\n\n\nOur analysis will be easier if we pivot our tidy data.\n\ndf_supply = \\\n(\ndf_tidy_corn\n    .pivot(index='Year_ID', columns='SC_Attribute_Desc', values='Amount')\n    .reset_index()\n    [['Year_ID', 'Beginning stocks','Planted acreage', 'Harvested acreage', 'Yield per harvested acre', \n      'Production', 'Prices received by farmers', 'Imports, market year']]\n    .assign(total_supply = lambda df: df['Beginning stocks'] + df['Production'] + df['Imports, market year'])\n)\ndf_supply.columns.name = None\ndf_supply\n\n\n\n\n\n\n\n\nYear_ID\nBeginning stocks\nPlanted acreage\nHarvested acreage\nYield per harvested acre\nProduction\nPrices received by farmers\nImports, market year\ntotal_supply\n\n\n\n\n0\n1866\nNaN\nNaN\n30.017\n24.3000\n730.814\n0.657\nNaN\nNaN\n\n\n1\n1867\nNaN\nNaN\n32.116\n24.7000\n793.905\n0.781\nNaN\nNaN\n\n\n2\n1868\nNaN\nNaN\n35.116\n26.2000\n919.590\n0.617\nNaN\nNaN\n\n\n3\n1869\nNaN\nNaN\n35.833\n21.8000\n782.084\n0.725\nNaN\nNaN\n\n\n4\n1870\nNaN\nNaN\n38.388\n29.3000\n1124.775\n0.521\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n153\n2019\n2220.749\n89.745\n81.337\n167.5000\n13619.928\n3.560\n41.885\n15882.562\n\n\n154\n2020\n1919.462\n90.652\n82.313\n171.4000\n14111.449\n4.530\n24.233\n16055.144\n\n\n155\n2021\n1234.512\n93.252\n85.318\n176.7000\n15073.820\n6.000\n24.227\n16332.559\n\n\n156\n2022\n1376.890\n88.579\n79.207\n173.3397\n13729.719\n6.600\n40.000\n15146.609\n\n\n157\n2023\n1416.609\n91.996\n84.100\n181.5101\n15265.000\n4.800\n25.000\n16706.609\n\n\n\n\n158 rows × 9 columns\n\n\n\n\n# recreating graph in 12.1.1 Forecasting Harvested Acres\n(\ndf_supply\n    .assign(difference = lambda df: df['Planted acreage'] - df['Harvested acreage'])\n    .query('Year_ID &gt; 1999')\n).plot(x='Year_ID', y=['Harvested acreage', 'Planted acreage', 'difference'], grid=True);\n\n\n\n\n\n# recreating graph in 12.2 Forecasting Yield\ndf_supply.plot(x='Year_ID', y=['Yield per harvested acre'], grid=True);\n\n\n\n\n\n# recreating graph in 12.2 Forecasting Yield\ndf_supply.query('Year_ID &gt; 1950').plot(x='Year_ID', y=['Yield per harvested acre'], grid=True);\n\n\n\n\n\n# recreating graph in 12.2 Forecasting Yield\ndf_supply.query('Year_ID &gt; 1980').plot(x='Year_ID', y=['Yield per harvested acre'], grid=True);"
  },
  {
    "objectID": "chapters/20_feed_grains_database/feed_grains_database.html#consumption",
    "href": "chapters/20_feed_grains_database/feed_grains_database.html#consumption",
    "title": "20  Exploration of the Feed Grains Database",
    "section": "20.4 Consumption",
    "text": "20.4 Consumption\nLet’s now switch our attention to the demand side. We begin by grabbing all the demand related attributes.\n\nattributes = ['Food, alcohol, and industrial use', 'Feed and residual use', 'Seed use', 'Exports, market year']\n\ndf_demand_tidy = \\\n    (\n    df_feed_grains\n        .query('SC_Commodity_Desc == \"Corn\"')\n        .query('SC_GeographyIndented_Desc == \"United States\"')\n        .query('SC_Attribute_Desc == @attributes')\n        .query('Timeperiod_Desc.str.contains(\"MY\")')\n        [['SC_Commodity_Desc', 'SC_GeographyIndented_Desc', 'SC_Attribute_Desc', 'Timeperiod_Desc','Year_ID', 'Amount', 'SC_Unit_Desc']]\n    )\ndf_demand_tidy\n\n\n\n\n\n\n\n\nSC_Commodity_Desc\nSC_GeographyIndented_Desc\nSC_Attribute_Desc\nTimeperiod_Desc\nYear_ID\nAmount\nSC_Unit_Desc\n\n\n\n\n16525\nCorn\nUnited States\nExports, market year\nMY Sep-Aug\n1975\n1664.494\nMillion bushels\n\n\n16530\nCorn\nUnited States\nExports, market year\nMY Sep-Aug\n1976\n1645.119\nMillion bushels\n\n\n16535\nCorn\nUnited States\nExports, market year\nMY Sep-Aug\n1977\n1896.396\nMillion bushels\n\n\n16540\nCorn\nUnited States\nExports, market year\nMY Sep-Aug\n1978\n2113.117\nMillion bushels\n\n\n16633\nCorn\nUnited States\nExports, market year\nMY Sep-Aug\n1979\n2401.517\nMillion bushels\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n18601\nCorn\nUnited States\nFood, alcohol, and industrial use\nMY Sep-Aug\n2019\n6256.213\nMillion bushels\n\n\n18606\nCorn\nUnited States\nFood, alcohol, and industrial use\nMY Sep-Aug\n2020\n6435.942\nMillion bushels\n\n\n18611\nCorn\nUnited States\nFood, alcohol, and industrial use\nMY Sep-Aug\n2021\n6734.439\nMillion bushels\n\n\n18614\nCorn\nUnited States\nFood, alcohol, and industrial use\nMY Sep-Aug\n2022\n6649.000\nMillion bushels\n\n\n18615\nCorn\nUnited States\nFood, alcohol, and industrial use\nMY Sep-Aug\n2023\n6704.000\nMillion bushels\n\n\n\n\n196 rows × 7 columns\n\n\n\nNow let’s pivot our tidy data to make it a bit more usable.\n\ndf_demand = \\\n    (\n    df_demand_tidy\n        .pivot(index='Year_ID', columns='SC_Attribute_Desc', values='Amount')\n        .reset_index()\n        [['Year_ID', 'Food, alcohol, and industrial use', 'Feed and residual use', 'Seed use', 'Exports, market year']]\n        .assign(total_demand = lambda df: df['Food, alcohol, and industrial use'] + df['Feed and residual use'] + \n                df['Seed use'] + df['Exports, market year'])\n    )\ndf_demand.head()\n\n\n\n\n\n\n\nSC_Attribute_Desc\nYear_ID\nFood, alcohol, and industrial use\nFeed and residual use\nSeed use\nExports, market year\ntotal_demand\n\n\n\n\n0\n1975\n500.7\n3581.760\n20.1\n1664.494\n5767.054\n\n\n1\n1976\n522.1\n3601.881\n20.1\n1645.119\n5789.200\n\n\n2\n1977\n561.5\n3729.743\n19.5\n1896.396\n6207.139\n\n\n3\n1978\n588.5\n4274.362\n19.5\n2113.117\n6995.479\n\n\n4\n1979\n619.5\n4563.043\n20.0\n2401.517\n7604.060\n\n\n\n\n\n\n\n\n# 13.1 Food, alcohol, and industrial use\ndf_demand.plot(x='Year_ID', y='Food, alcohol, and industrial use', grid=True);\n\n\n\n\n\n# 13.1 Food, alcohol, and industrial use as proportion of total demand\n(\ndf_demand.\n    assign(prop = lambda df: df['Food, alcohol, and industrial use'] / df['total_demand'])\n).plot(x='Year_ID', y='prop', grid=True);\n\n\n\n\n\n# 13.2 Exports\ndf_demand.plot(x='Year_ID', y='Exports, market year', grid=True);\n\n\n\n\n\n# 13.2 Exports as a proportion of total demand\n(\ndf_demand.\n    assign(prop = lambda df: df['Exports, market year'] / df['total_demand'])\n).plot(x='Year_ID', y='prop', grid=True);\n\n\n\n\n\n# 13.3 Feed and residuals\ndf_demand.plot(x='Year_ID', y='Feed and residual use', grid=True);\n\n\n\n\n\n# 13.3 Feed and residuals as a proportion of total demand\n(\ndf_demand.\n    assign(prop = lambda df: df['Feed and residual use'] / df['total_demand'])\n).plot(x='Year_ID', y='prop', grid=True);"
  },
  {
    "objectID": "chapters/20_feed_grains_database/feed_grains_database.html#ending-stocks-and-price",
    "href": "chapters/20_feed_grains_database/feed_grains_database.html#ending-stocks-and-price",
    "title": "20  Exploration of the Feed Grains Database",
    "section": "20.5 Ending Stocks and Price",
    "text": "20.5 Ending Stocks and Price\nLet’s now analyze the relationship between price and ending stocks. This is done in Chapter 15 (Ending Stocks and Price).\nWe begin by examining a scatter plot of stocks-to-use vs prices received by farmers. I would guess there would be a negative relationship here and that is the case.\n\n# corn stocks-to-use and prices received by farmers\ndf_surplus = \\\n    (\n    df_supply[['Year_ID', 'total_supply', 'Prices received by farmers']]\n        .merge(df_demand[['Year_ID', 'total_demand']], how='inner')\n        .assign(stocks_ratio = lambda df: (df['total_supply'] - df['total_demand']) / df['total_demand'])\n    )\ndf_surplus.plot(x='stocks_ratio', y='Prices received by farmers', kind='scatter', grid=True);\n\n\n\n\nLet’s run a regression on this data set.\n\nfrom sklearn.linear_model import LinearRegression\ndf_X = df_surplus.dropna()[['stocks_ratio']]\ndf_y = df_surplus.dropna()[['Prices received by farmers']]\nmodel = LinearRegression()\nmodel.fit(df_X, df_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nAs we see we get an \\(R^2\\) of 0.22 which isn’t bad in the world of finance.\n\nmodel.score(df_X, df_y)\n\n0.22025208089867765\n\n\nFor every percent increase in stocks-to-use ratio there is about a $0.05 reduction in the price received by farmers.\n\nmodel.coef_ / 100\n\narray([[-0.04628747]])\n\n\n\n20.5.1 Analyzing Pre-2006\nMallory suggests breaking down the analysis into pre-2006 and post-2006 because that’s when the ethanol mandates came into play.\nWe begin with pre-2006.\n\ndf_surplus_pre_2006 = df_surplus.query('Year_ID &lt; 2006')\ndf_surplus_pre_2006.plot(x='stocks_ratio', y='Prices received by farmers', kind='scatter', grid=True);\n\n\n\n\nLet’s fit a regression to the pre-2006 data.\n\nfrom sklearn.linear_model import LinearRegression\ndf_X = df_surplus_pre_2006.dropna()[['stocks_ratio']]\ndf_y = df_surplus_pre_2006.dropna()[['Prices received by farmers']]\nmodel = LinearRegression()\nmodel.fit(df_X, df_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nIt doesn’t seem to help our \\(R^2\\), but it does significantly change our coefficient, which is good to know.\n\nmodel.score(df_X, df_y)\n\n0.1505653587487924\n\n\n\nmodel.coef_ / 100\n\narray([[-0.01018139]])\n\n\n\n\n20.5.2 Analyzing post-2006\nNow let’s do the post-2006 analysis.\n\ndf_surplus_post_2006 = df_surplus.query('Year_ID &gt;= 2006')\ndf_surplus_post_2006.plot(x='stocks_ratio', y='Prices received by farmers', kind='scatter', grid=True);\n\n\n\n\nLet’s now fit a regression to the post-2006 data.\n\nfrom sklearn.linear_model import LinearRegression\ndf_X = df_surplus_post_2006.dropna()[['stocks_ratio']]\ndf_y = df_surplus_post_2006.dropna()[['Prices received by farmers']]\nmodel = LinearRegression()\nmodel.fit(df_X, df_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nOur \\(R^2\\) improves and the negative relationship is much more pronounced.\n\nmodel.score(df_X, df_y)\n\n0.5193873474299935\n\n\n\nmodel.coef_ / 100\n\narray([[-0.31178772]])"
  },
  {
    "objectID": "chapters/21_simple_linear_regression/simple_linear_regression.html#import-packages",
    "href": "chapters/21_simple_linear_regression/simple_linear_regression.html#import-packages",
    "title": "21  Simple Linear Regression",
    "section": "21.1 Import Packages",
    "text": "21.1 Import Packages\nLet’s begin by loading the packages that we will need.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport pandas as pd\nimport numpy as np\nimport sklearn"
  },
  {
    "objectID": "chapters/21_simple_linear_regression/simple_linear_regression.html#reading-in-data",
    "href": "chapters/21_simple_linear_regression/simple_linear_regression.html#reading-in-data",
    "title": "21  Simple Linear Regression",
    "section": "21.2 Reading-In Data",
    "text": "21.2 Reading-In Data\nThe dataset that we will analyze in this tutorial consists of weekly volatility metrics for SPY during 2014-2018. Each row of the DataFrame is a set of observations from a specific week. In particular:\n\nrealized_vol - standard deviation of returns during period (annualized).\nret - simple return for the period.\nstart_iv - the implied vol (variance swap rate) at the start of the period.\n\nLet’s read-in the data set and have a look.\n\ndf_spy = pd.read_csv('spy_2014_2018_regression.csv')\ndf_spy.head()\n\n\n\n\n\n\n\n\nunderlying\nstart_date\nend_date\nrealized_vol\nret\nstart_iv\n\n\n\n\n0\nSPY\n2014-01-03\n2014-01-10\n0.052949\n0.006812\n0.104300\n\n\n1\nSPY\n2014-01-10\n2014-01-17\n0.147207\n-0.002719\n0.093948\n\n\n2\nSPY\n2014-01-17\n2014-01-24\n0.176336\n-0.026206\n0.103134\n\n\n3\nSPY\n2014-01-24\n2014-01-31\n0.136391\n-0.003977\n0.195719\n\n\n4\nSPY\n2014-01-31\n2014-02-07\n0.235160\n0.008383\n0.182371"
  },
  {
    "objectID": "chapters/21_simple_linear_regression/simple_linear_regression.html#exploratory-data-analysis-implied-volatility-as-a-fear-index",
    "href": "chapters/21_simple_linear_regression/simple_linear_regression.html#exploratory-data-analysis-implied-volatility-as-a-fear-index",
    "title": "21  Simple Linear Regression",
    "section": "21.3 Exploratory Data Analysis: Implied Volatility as a Fear Index",
    "text": "21.3 Exploratory Data Analysis: Implied Volatility as a Fear Index\nOptions are simple insurance contracts that are written on top of an underlying stock. They protect against large moves in the price of the underlying. Puts protect against downward moves, calls protect against upward moves.\nThe implied volatility of a stock is a measurement that gauges how much market participants are willing to pay for options on that stock. Thus, the implied volatility of a stock serves as a index of how fearful market participants are about large moves in the stock price.\nThe Leverage Effect: For many stocks, especially index-ETFs, the following two relationships hold:\n\nImplied volatility increases when the stock experiences losses (negative returns).\nImplied volatility decreases when the stock experiences gains (positive returns).\n\nLet’s try to see this relationship in our SPY weekly data by means of a simple scatter plot.\nFirst, let’s create a new column in df_spy - we’ll call it iv_change - to capture the week over week change of the implied volatility.\n\ndf_spy['iv_change'] = (df_spy['start_iv'] - df_spy['start_iv'].shift(1)).shift(-1)\ndf_spy.head()\n\n\n\n\n\n\n\n\nunderlying\nstart_date\nend_date\nrealized_vol\nret\nstart_iv\niv_change\n\n\n\n\n0\nSPY\n2014-01-03\n2014-01-10\n0.052949\n0.006812\n0.104300\n-0.010352\n\n\n1\nSPY\n2014-01-10\n2014-01-17\n0.147207\n-0.002719\n0.093948\n0.009187\n\n\n2\nSPY\n2014-01-17\n2014-01-24\n0.176336\n-0.026206\n0.103134\n0.092585\n\n\n3\nSPY\n2014-01-24\n2014-01-31\n0.136391\n-0.003977\n0.195719\n-0.013349\n\n\n4\nSPY\n2014-01-31\n2014-02-07\n0.235160\n0.008383\n0.182371\n-0.041966\n\n\n\n\n\n\n\nNext, let’s plot the weekly returns (ret) against the implied-vol changes (iv_change) using the pandas built-in plotting functionality.\n\ndf_spy.plot.scatter('ret', 'iv_change', c='k', figsize=(6, 4));\n\n\n\n\nClearly there is a negative relationship, which is what we would expect."
  },
  {
    "objectID": "chapters/21_simple_linear_regression/simple_linear_regression.html#regression-example-1-returns-vs-change-in-implied-volatility",
    "href": "chapters/21_simple_linear_regression/simple_linear_regression.html#regression-example-1-returns-vs-change-in-implied-volatility",
    "title": "21  Simple Linear Regression",
    "section": "21.4 Regression Example 1: Returns vs Change in Implied Volatility",
    "text": "21.4 Regression Example 1: Returns vs Change in Implied Volatility\nIn an exploratory data analysis situation, the visualization above may be all we would need to establish the existance of the leverage effect in SPY. On the other hand, we may want to make this analysis more precise by fitting a linear regression line to the data. A linear regression is a simple model that purports that iv_change is a linear function of the ret. Intuitively, when fitting a linear regression we are trying to find the straight line that has the minimium aggregate distance from all the points in our data.\nIn the language of statistics, the ret is the independent variable and the iv_change is the dependent variable. The field of machine learning uses different terminology: ret is called the feature and iv_change is called the label. In a generic machine learning problem, we seek to predict a label from one or more features.\nWe will use sklearn to fit a linear regression to our data. The first step in any learning task with sklearn is to instantiate the model object with a constructor function. In the case of linear regression, the constructor function is LinearRegression(). We’ll call our model variable iv_model.\n\nfrom sklearn.linear_model import LinearRegression\niv_model = LinearRegression(fit_intercept=False)\n\nBy setting fit_intercept=False we are forcing the line to go through the origin. This seems reasonable from a visual inspection of the data.\nNext, we’ll separate out the data that will be used to fit the model.\n\ndf_ret = df_spy[['ret']][0:-1] # features\ndf_iv = df_spy[['iv_change']][0:-1] # labels\n\nWe are now ready to fit the model by using the .fit() method of iv_model.\n\niv_model.fit(df_ret, df_iv)\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression(fit_intercept=False)\n\n\nNext, let’s check the intercept and coefficient of the line that was fit to our data.\n\nprint(iv_model.coef_)\nprint(iv_model.intercept_)\n\n[[-2.17471827]]\n0.0\n\n\nThis means that our linear regression model has determined that the best fitting line is of the form:\n\\[\\begin{align}\niv\\_change = -2.1747 \\cdot weekly\\_return.\n\\end{align}\\]\nThis can be interpreted to mean that every 1% of positive weekly price return leads to a drop in implied volatility of about 2.175%.\nWe can use the .predict() method of our fitted model iv_model to predict labels for a given set of features. In our example, we can predict implied volatility changes for a given set of weekly returns.\nLet’s try this for -5%, 0%, and 1%.\n\ntest_values = np.array([-0.05, 0, 0.01]).reshape(-1, 1)\niv_model.predict(test_values)\n\n/home/pritam/.local/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\narray([[ 0.10873591],\n       [ 0.        ],\n       [-0.02174718]])\n\n\nWe can also use the .predict() method to graph our fitted line along with our data.\n\nxfit = np.linspace(-0.08, 0.055, 100)         # range of line\nyfit = iv_model.predict(xfit[:, np.newaxis])  # model values in range\n\ndf_spy.plot.scatter('ret', 'iv_change', c='k', figsize=(6, 4));\nplt.plot(xfit, yfit);\n\n/home/pritam/.local/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\n\n\n\nIn sklearn, all learning models have a .score() method which calculates some kind of measure of accuracy or fit. For a LinearRegression model, .score() gives the \\(R^2\\).\nThe \\(R^{2}\\) measures gives a sense for the goodness of fit of a linear regression. It can be interpreted as the percent of variance in the label that is explained by the features.\n\niv_model.score(df_ret, df_iv)\n\n0.5900683519289271\n\n\nOur linear regression explains 59% of the variance in weekly implied volatility changes, from weekly returns.\nThere is no universal notion of what is a good or bad \\(R^2\\). That type of value judgement is context specific. Based on my experience of looking at financial data, this scatter plot looks pretty good, meaning that the relationship is strong."
  },
  {
    "objectID": "chapters/21_simple_linear_regression/simple_linear_regression.html#regression-example-2-realized-volatility-clustering",
    "href": "chapters/21_simple_linear_regression/simple_linear_regression.html#regression-example-2-realized-volatility-clustering",
    "title": "21  Simple Linear Regression",
    "section": "21.5 Regression Example 2: Realized Volatility Clustering",
    "text": "21.5 Regression Example 2: Realized Volatility Clustering\nA stylized fact about financial asset returns is that realized volatility exhibits clustering. This means that high volatility tends to be followed by high volatility, and low volatility tends to be followed by low volatility.\nLet’s try to observe realized voaltility clustering in our weekly SPY data, and then analyze it with linear regression. In particular, let’s observe the relationship between current-week realized volaltility and subsequent-week realized volatility.\nWe’ll begin by first creating new columns in df_spy to hold this data. Notice that real_vol_0 is just a copy of realized_vol.\n\ndf_spy['real_vol_0'] = df_spy['realized_vol']\ndf_spy['real_vol_1'] = df_spy['realized_vol'].shift(-1)\ndf_spy.head()\n\n\n\n\n\n\n\n\nunderlying\nstart_date\nend_date\nrealized_vol\nret\nstart_iv\niv_change\nreal_vol_0\nreal_vol_1\n\n\n\n\n0\nSPY\n2014-01-03\n2014-01-10\n0.052949\n0.006812\n0.104300\n-0.010352\n0.052949\n0.147207\n\n\n1\nSPY\n2014-01-10\n2014-01-17\n0.147207\n-0.002719\n0.093948\n0.009187\n0.147207\n0.176336\n\n\n2\nSPY\n2014-01-17\n2014-01-24\n0.176336\n-0.026206\n0.103134\n0.092585\n0.176336\n0.136391\n\n\n3\nSPY\n2014-01-24\n2014-01-31\n0.136391\n-0.003977\n0.195719\n-0.013349\n0.136391\n0.235160\n\n\n4\nSPY\n2014-01-31\n2014-02-07\n0.235160\n0.008383\n0.182371\n-0.041966\n0.235160\n0.063975\n\n\n\n\n\n\n\nNext, let’s take a look at a scatter plot of real_vol_0 vs real_vol_1.\n\ndf_spy.plot.scatter('real_vol_0', 'real_vol_1', c='k', figsize=(6, 4));\n\n\n\n\nAt first glance, I would say this scatter plot looks good. The relationship is clearly positive (although quite noisy), as we would expect from the stylized fact of volatility clustering.\nHowever, the data in data in it’s current form is not particularly well suited for linear regression. First of all, the volatilies are bunched near zero, with a few extremely large observations. Additionally, standard deviations are by definition always greater than zero.\nFor both of these reasons, let’s take the logs of both variables to make the relationship more clear. We’ll do so by simply repopulating the columns in df_spy with the logged values that we want.\n\ndf_spy['real_vol_0'] = np.log(df_spy['realized_vol'])\ndf_spy['real_vol_1'] = np.log(df_spy['realized_vol']).shift(-1)\ndf_spy.head()\n\n\n\n\n\n\n\n\nunderlying\nstart_date\nend_date\nrealized_vol\nret\nstart_iv\niv_change\nreal_vol_0\nreal_vol_1\n\n\n\n\n0\nSPY\n2014-01-03\n2014-01-10\n0.052949\n0.006812\n0.104300\n-0.010352\n-2.938431\n-1.915913\n\n\n1\nSPY\n2014-01-10\n2014-01-17\n0.147207\n-0.002719\n0.093948\n0.009187\n-1.915913\n-1.735366\n\n\n2\nSPY\n2014-01-17\n2014-01-24\n0.176336\n-0.026206\n0.103134\n0.092585\n-1.735366\n-1.992228\n\n\n3\nSPY\n2014-01-24\n2014-01-31\n0.136391\n-0.003977\n0.195719\n-0.013349\n-1.992228\n-1.447491\n\n\n4\nSPY\n2014-01-31\n2014-02-07\n0.235160\n0.008383\n0.182371\n-0.041966\n-1.447491\n-2.749269\n\n\n\n\n\n\n\nLet’s replot the logged data.\n\ndf_spy.plot.scatter('real_vol_0', 'real_vol_1', c='k', figsize=(6, 4));\n\n\n\n\nThe positive relationship looks more linear after taking logs of both the features and the labels.\nAs in the previous section, let’s fit a simple linear regression to this data by executing the following steps:\n\nInstantiate a model with the LinearRegression() constructor.\nIsolate the data for fitting.\nFit the model with .fit().\nCheck for goodness of fit with .score().\n\nFirst, let’s instantiate our model with the LinearRegression() constructor function. We will call our model rv_model.\n\nrv_model = LinearRegression(fit_intercept=True)\n\nNext, let’s isolate the data that we will use to fit the model.\n\ndf_rv_0 = df_spy[['real_vol_0']][:-1]\ndf_rv_1 = df_spy[['real_vol_1']][:-1]\n\nWe can now fit the model to the data using the .fit() method of rv_model.\n\nrv_model.fit(df_rv_0, df_rv_1)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nLastly, we can check the goodness of fit by first visually inspecting the data, and then also by calculating the \\(R^2\\).\n\nxfit = np.linspace(-4.5, -0.75, 100)          # range of line\nyfit = rv_model.predict(xfit[:, np.newaxis])  # model values in range\n\ndf_spy.plot.scatter('real_vol_0', 'real_vol_1', c='k', figsize=(6, 4));\nplt.plot(xfit, yfit);\n\n/home/pritam/.local/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\n\n\n\nAs we can see from the output code below, our \\(R^2\\) is lower for this regression than the previous one (ret vs iv_change). This is rather obvious from visual inspection of the two graphs - notice how much more spread out the data points are in this graph, versus the graph in the previous analysis.\n\nrv_model.score(df_rv_0, df_rv_1)\n\n0.21785126272072308"
  },
  {
    "objectID": "chapters/21_simple_linear_regression/simple_linear_regression.html#forecasting-realized-volatility",
    "href": "chapters/21_simple_linear_regression/simple_linear_regression.html#forecasting-realized-volatility",
    "title": "21  Simple Linear Regression",
    "section": "21.6 Forecasting Realized Volatility",
    "text": "21.6 Forecasting Realized Volatility\nThus far, our regression analysis involved using the entirety of the five years of SPY data that we have avaialable. This is typical if you are using regression for exploratory data analysis, or simply to confirm some kind of directional relationship between two variables.\nHowever, machine learning has aspirations beyond mere exploration - the ultimate goal is usually prediction or forecasting. If you’re serious about that objective, it’s appropriate to split your data into a training set and a testing set. These separate sets are used for two distinct purposes in a two-stage approach:\n\nTraing data - used to train/fit/learn the model. This phase is referred to as the learning or training phase.\nTesting data - fed into the trained model to produce predictions; we then analyze the predicted values vs true values in the test set to determine the accuracy of the model. This phase referred to as the testing or generalization phase.\n\nLet’s try using this two-stage approach with our realized volatility data.\nSpecifically, rather than fitting a linear regression to the entirety of our data set, let’s instead fit it to only the first four years of the data (2014-2017). We’ll then use the fitted/trained model to forecast realized volatility in 2018.\nLet’s begin by instantiating a new LinearRegression object and call it fcst_model.\n\nfcst_model = LinearRegression(fit_intercept=True)\n\nNext, let’s grab the training data from 2014-2018.\n\ndf_rv_0_train = df_spy[['real_vol_0']][0:208] # this weeks volatility (feature)\ndf_rv_1_train = df_spy[['real_vol_1']][0:208] # next weeks volatility (label)\n\nWe next fit our model to the training data using fcst_model.fit().\n\nfcst_model.fit(df_rv_0_train, df_rv_1_train) # fitting the model\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nLet’s print the coefficient, the intercept, and the \\(R^{2}\\) from the model.\n\nprint(\"Coefficent:      \", np.round(fcst_model.coef_[0, 0], 2))    # coefficient\nprint(\"Intercept:      \", np.round(fcst_model.intercept_[0], 2))  # intercept\nprint(\"R^2 (training):  \", np.round(fcst_model.score(df_rv_0_train, df_rv_1_train), 2)) # R^2 from training\n\nCoefficent:       0.46\nIntercept:       -1.34\nR^2 (training):   0.21\n\n\nSo our linear model is:\n\\[\\log(next\\_week\\_realized\\_vol) = 0.46 * \\log(this\\_week\\_realized\\_vol) - 1.34.\\]\nIt accounts for about 21% of the variability of the weekly \\(\\log(realized\\_vol)\\) in the training set.\nLet’s now apply our trained model to data from 2018. We begin be separating out the 2018 testing data into it’s own DataFrame.\n\ndf_rv_0_test = df_spy[['real_vol_0']][209:-1] # features\ndf_rv_1_test = df_spy[['real_vol_1']][209:-1] # labels\n\nWe can test how our model predictions compare to the real data by plugging our testing data into the score() method of the model.\n\nfcst_model.score(df_rv_0_test, df_rv_1_test)\n\n0.1484251353811019\n\n\nAlternatively, we can first calculate the predictions, and then calculate the \\(R^2\\) directly on the predicted values. In order to do this we would use the .predict() method of the LinearRegression object along the r2_score() function in the sklearn.metrics module.\n\nsklearn.metrics.r2_score(df_rv_1_test, fcst_model.predict(df_rv_0_test))\n\n0.1484251353811019\n\n\nNotice that the model is more accurate (i.e. has a higher \\(R^2\\)) on the training set than on the testing set - this is almost always the case."
  },
  {
    "objectID": "chapters/21_simple_linear_regression/simple_linear_regression.html#updating-our-forecasting-model-daily",
    "href": "chapters/21_simple_linear_regression/simple_linear_regression.html#updating-our-forecasting-model-daily",
    "title": "21  Simple Linear Regression",
    "section": "21.7 Updating our Forecasting Model Daily",
    "text": "21.7 Updating our Forecasting Model Daily\nIn our forcasting exercise above, our LinearRegression model was trained on data from 2014-2017, and all of our 2018 forecasts were based on that model. This is probably not what we would do in practice. Instead, we would fit a new model on a regular basis.\nIn the following code, the training period is updated every week to the most recent four years. We would hope to see slightly improved performance over just training the model once. (The code below is also indcative of patterns used when conducting a backtest.)\n\nix_start = 0\nix_end = 208\n\nforecasts = np.zeros(50)\nfcst_model_2 = LinearRegression(fit_intercept=True)\nfor ix_end in range(208, 258, 1):\n    # setting training period start date   \n    ix_start = ix_end - 208\n    \n    # selecting training data\n    df_rv_0_train = df_spy[['real_vol_0']][ix_start:ix_end]\n    df_rv_1_train = df_spy[['real_vol_1']][ix_start:ix_end]\n    \n    # fitting the model\n    fcst_model_2.fit(df_rv_0_train, df_rv_1_train)\n    \n    # forecasting with the newly fitted model\n    real_vol = df_spy['real_vol_0'].values[ix_end]\n    fcst_rv = fcst_model_2.coef_[0, 0] * real_vol  + fcst_model_2.intercept_[0]\n    \n    # saving the current forecast\n    forecasts[ix_start] = fcst_rv\n\nAs we can see, there is a slight improvement when updating the model daily - an \\(R^2\\) of 0.1817 vs 0.1484. I would not expect the improvement to be that significant given the simplistic nature of our forecasting mechanism.\n\nsklearn.metrics.r2_score(df_spy[['real_vol_1']][208:258], forecasts)\n\n0.18178835397754134"
  },
  {
    "objectID": "chapters/21_simple_linear_regression/simple_linear_regression.html#further-reading",
    "href": "chapters/21_simple_linear_regression/simple_linear_regression.html#further-reading",
    "title": "21  Simple Linear Regression",
    "section": "21.8 Further Reading",
    "text": "21.8 Further Reading\nPython Data Science Handbook (VanderPlas) - 5.1 - What Is Machine Learning?\nPython Data Science Handbook (VanderPlas) - 5.2 - Introducing Scikit-Learn\nPython Data Science Handbook (VanderPlas) - 5.3 - Hyperparameters and Model Validation\nPython Data Science Handbook (VanderPlas) - 5.4 - Feature Engineering\nPython Data Science Handbook (VanderPlas) - 5.6 - In Depth: Linear Regression"
  },
  {
    "objectID": "chapters/22_categorical_linear_regression/categorical_linear_regression.html#loading-packages",
    "href": "chapters/22_categorical_linear_regression/categorical_linear_regression.html#loading-packages",
    "title": "22  Linear Regression with Categorical Variables",
    "section": "22.1 Loading Packages",
    "text": "22.1 Loading Packages\nLet’s begin by loading the packages that we will need. Notice that in addition to sklearn we are also using statsmodels, which is a popular package that includes many classical statistical techniques.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline"
  },
  {
    "objectID": "chapters/22_categorical_linear_regression/categorical_linear_regression.html#reading-in-data",
    "href": "chapters/22_categorical_linear_regression/categorical_linear_regression.html#reading-in-data",
    "title": "22  Linear Regression with Categorical Variables",
    "section": "22.2 Reading-In Data",
    "text": "22.2 Reading-In Data\nNext, we’ll read-in the data that we will be working with; it is the Credit data set from the ISLR2 R package that can be downloaded from CRAN.\n\ndf_credit = pd.read_csv('credit.csv')\ndf_credit.columns = df_credit.columns.str.lower()\ndf_credit\n\n\n\n\n\n\n\n\nincome\nlimit\nrating\ncards\nage\neducation\nown\nstudent\nmarried\nregion\nbalance\n\n\n\n\n0\n14.891\n3606\n283\n2\n34\n11\nNo\nNo\nYes\nSouth\n333\n\n\n1\n106.025\n6645\n483\n3\n82\n15\nYes\nYes\nYes\nWest\n903\n\n\n2\n104.593\n7075\n514\n4\n71\n11\nNo\nNo\nNo\nWest\n580\n\n\n3\n148.924\n9504\n681\n3\n36\n11\nYes\nNo\nNo\nWest\n964\n\n\n4\n55.882\n4897\n357\n2\n68\n16\nNo\nNo\nYes\nSouth\n331\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n395\n12.096\n4100\n307\n3\n32\n13\nNo\nNo\nYes\nSouth\n560\n\n\n396\n13.364\n3838\n296\n5\n65\n17\nNo\nNo\nNo\nEast\n480\n\n\n397\n57.872\n4171\n321\n5\n67\n12\nYes\nNo\nYes\nSouth\n138\n\n\n398\n37.728\n2525\n192\n1\n44\n13\nNo\nNo\nYes\nSouth\n0\n\n\n399\n18.701\n5524\n415\n5\n64\n7\nYes\nNo\nNo\nWest\n966\n\n\n\n\n400 rows × 11 columns\n\n\n\nThe label is balance - average credit card debt for each individual\nThere are several quantitative features: - age - in years - cards - number of credit cards - education - years of education - income - in thousands of dollars - limit - credit limit - rating credit rating (FICO)\nIn addition to these quantitative variables, we also have four qualitative variables: - own - home ownership - student - are they a student or not - status - are they married or not - region - geographic location (East, West, or South)\nWe can examine the pairwise relationships between the quantitative variables with the seaborn.pairplot() function.\n\nsns.pairplot(df_credit, height=1.25, plot_kws={\"s\": 15});"
  },
  {
    "objectID": "chapters/22_categorical_linear_regression/categorical_linear_regression.html#categorical-feature-with-two-levels",
    "href": "chapters/22_categorical_linear_regression/categorical_linear_regression.html#categorical-feature-with-two-levels",
    "title": "22  Linear Regression with Categorical Variables",
    "section": "22.3 Categorical Feature with Two Levels",
    "text": "22.3 Categorical Feature with Two Levels\nSuppose that we wish to investigate diﬀerences in credit card balance between those who own a house and those who don’t, ignoring the other variables. If a qualitative feature only has two possible values (also called levels), then incorporating it into a regression model is simple. We simply create an indicator or dummy variable that takes on two possible numerical values. In particular, our dummy variable takes a value of zero for those who don’t own a house, and a value of one for those who do own a house. This technique is called one-hot encoding.\nWe can use pandas.get_dummies() to perform one-hot encoding.\n\ndf_X = df_credit[['own']]\ndf_X = pd.get_dummies(df_X, drop_first=True, dtype='float')\ndf_X.head()\n\n\n\n\n\n\n\n\nown_Yes\n\n\n\n\n0\n0.0\n\n\n1\n1.0\n\n\n2\n0.0\n\n\n3\n1.0\n\n\n4\n0.0\n\n\n\n\n\n\n\nLet’s isolate our labels into their own DataFrame.\n\ndf_y = df_credit[['balance']]\ndf_y.head()\n\n\n\n\n\n\n\n\nbalance\n\n\n\n\n0\n333\n\n\n1\n903\n\n\n2\n580\n\n\n3\n964\n\n\n4\n331\n\n\n\n\n\n\n\n\n22.3.1 Using sklearn\nWe’ll first perform a linear regression using sklearn.\n\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(df_X, df_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nWe can examine the intercept and coefficient as follows.\n\nprint(model.coef_)\nprint(model.intercept_)\n\n[[19.73312308]]\n[509.80310881]\n\n\nLet’s make these values easier to read by putting them into a DataFrame.\n\ndf_coef = pd.DataFrame(\n    data = {'coefficient': list(model.intercept_) + list(np.ravel(model.coef_))},\n    index = ['intercept'] + list(df_X.columns.values), \n)\ndf_coef\n\n\n\n\n\n\n\n\ncoefficient\n\n\n\n\nintercept\n509.803109\n\n\nown_Yes\n19.733123\n\n\n\n\n\n\n\n\n\n22.3.2 Using statsmodels\nNext, let’s perform the same the same linear regression using statsmodels which will allow us to check for statistical significance by examining \\(p\\)-values.\n\ndf_X_sm = sm.add_constant(df_X)\nls = sm.OLS(df_y, df_X_sm).fit()\nprint(ls.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                balance   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.002\nMethod:                 Least Squares   F-statistic:                    0.1836\nDate:                Tue, 29 Aug 2023   Prob (F-statistic):              0.669\nTime:                        13:35:04   Log-Likelihood:                -3019.3\nNo. Observations:                 400   AIC:                             6043.\nDf Residuals:                     398   BIC:                             6051.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        509.8031     33.128     15.389      0.000     444.675     574.931\nown_Yes       19.7331     46.051      0.429      0.669     -70.801     110.267\n==============================================================================\nOmnibus:                       28.438   Durbin-Watson:                   1.940\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               27.346\nSkew:                           0.583   Prob(JB):                     1.15e-06\nKurtosis:                       2.471   Cond. No.                         2.66\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nHere is how to interpret these results: - A non-home owner has an average balance of $509.80 - A home owner has an average balance of 509.80 + 19.73 = $529.53 - However, notice that the \\(p\\)-value of own_Yes is quite high which suggests that there is no statistical evidence of a diﬀerence in average credit card balance based on house ownership."
  },
  {
    "objectID": "chapters/22_categorical_linear_regression/categorical_linear_regression.html#categorical-feature-with-multiple-levels",
    "href": "chapters/22_categorical_linear_regression/categorical_linear_regression.html#categorical-feature-with-multiple-levels",
    "title": "22  Linear Regression with Categorical Variables",
    "section": "22.4 Categorical Feature with Multiple Levels",
    "text": "22.4 Categorical Feature with Multiple Levels\nIn this section we consider regression on a categorical feature that has multiple levels. When a qualitative feature has more than two levels, a single dummy variable cannot represent all possible values. In this situation we can create additional dummy variables. There will always be one fewer dummy variable than the number of levels.\nTo explore this technique, let’s use the region feature and create dummy variables using the pandas.get_dummies() function.\n\ndf_X = df_credit[['region']]\ndf_X = pd.get_dummies(df_X, drop_first=True, dtype='float')\ndf_X.head()\n\n\n\n\n\n\n\n\nregion_South\nregion_West\n\n\n\n\n0\n1.0\n0.0\n\n\n1\n0.0\n1.0\n\n\n2\n0.0\n1.0\n\n\n3\n0.0\n1.0\n\n\n4\n1.0\n0.0\n\n\n\n\n\n\n\nNext, let’s use sklearn to perform a linear regression with the dummy variables.\n\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(df_X, df_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nWe can examine the coefficients in a DataFrame as follows:\n\ndf_coef = pd.DataFrame(\n    data = {'coefficient': list(model.intercept_) + list(np.ravel(model.coef_))},\n    index = ['intercept'] + list(df_X.columns.values), \n)\ndf_coef\n\n\n\n\n\n\n\n\ncoefficient\n\n\n\n\nintercept\n531.000000\n\n\nregion_South\n-12.502513\n\n\nregion_West\n-18.686275\n\n\n\n\n\n\n\nFinally, we’ll use statsmodels to perform the same regression so that we can examine the \\(p\\)-values.\n\ndf_X_sm = sm.add_constant(df_X)\nls = sm.OLS(df_y, df_X_sm).fit()\nprint(ls.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                balance   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.005\nMethod:                 Least Squares   F-statistic:                   0.04344\nDate:                Tue, 29 Aug 2023   Prob (F-statistic):              0.957\nTime:                        13:37:01   Log-Likelihood:                -3019.3\nNo. Observations:                 400   AIC:                             6045.\nDf Residuals:                     397   BIC:                             6057.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst          531.0000     46.319     11.464      0.000     439.939     622.061\nregion_South   -12.5025     56.681     -0.221      0.826    -123.935      98.930\nregion_West    -18.6863     65.021     -0.287      0.774    -146.515     109.142\n==============================================================================\nOmnibus:                       28.829   Durbin-Watson:                   1.946\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               27.395\nSkew:                           0.581   Prob(JB):                     1.13e-06\nKurtosis:                       2.460   Cond. No.                         4.39\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nHere is how to interpret these results:\n\nThe average balance of a cardholder that lives in the East is $531.00.\nA cardholder in the South has an average balance of 531.00 - 12.50 = $518.50\nA cardholder in the West has an average balance of 531.00 - 18.67 = $512.33\nThe the \\(p\\)-values of both region_South and region_West are quite high which suggests that there is no statistical evidence of a diﬀerence in average credit card balance based on region."
  },
  {
    "objectID": "chapters/23_ridge_lasso_regression/ridge_lasso_regression.html#import-packages",
    "href": "chapters/23_ridge_lasso_regression/ridge_lasso_regression.html#import-packages",
    "title": "23  Ridge and Lasso Regression",
    "section": "23.1 Import Packages",
    "text": "23.1 Import Packages\nLet’s begin by importing the packages that we will need.\n\nimport pandas as pd\nimport numpy as np\n%matplotlib inline"
  },
  {
    "objectID": "chapters/23_ridge_lasso_regression/ridge_lasso_regression.html#read-in-data",
    "href": "chapters/23_ridge_lasso_regression/ridge_lasso_regression.html#read-in-data",
    "title": "23  Ridge and Lasso Regression",
    "section": "23.2 Read-In Data",
    "text": "23.2 Read-In Data\nNext, we’ll read-in the data that we will be working with; it is the Credit data set from the ISLR2 R package that can be downloaded from CRAN.\n\ndf_credit = pd.read_csv('credit.csv')\ndf_credit.columns = df_credit.columns.str.lower()\ndf_credit\n\n\n\n\n\n\n\n\nincome\nlimit\nrating\ncards\nage\neducation\nown\nstudent\nmarried\nregion\nbalance\n\n\n\n\n0\n14.891\n3606\n283\n2\n34\n11\nNo\nNo\nYes\nSouth\n333\n\n\n1\n106.025\n6645\n483\n3\n82\n15\nYes\nYes\nYes\nWest\n903\n\n\n2\n104.593\n7075\n514\n4\n71\n11\nNo\nNo\nNo\nWest\n580\n\n\n3\n148.924\n9504\n681\n3\n36\n11\nYes\nNo\nNo\nWest\n964\n\n\n4\n55.882\n4897\n357\n2\n68\n16\nNo\nNo\nYes\nSouth\n331\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n395\n12.096\n4100\n307\n3\n32\n13\nNo\nNo\nYes\nSouth\n560\n\n\n396\n13.364\n3838\n296\n5\n65\n17\nNo\nNo\nNo\nEast\n480\n\n\n397\n57.872\n4171\n321\n5\n67\n12\nYes\nNo\nYes\nSouth\n138\n\n\n398\n37.728\n2525\n192\n1\n44\n13\nNo\nNo\nYes\nSouth\n0\n\n\n399\n18.701\n5524\n415\n5\n64\n7\nYes\nNo\nNo\nWest\n966\n\n\n\n\n400 rows × 11 columns"
  },
  {
    "objectID": "chapters/23_ridge_lasso_regression/ridge_lasso_regression.html#wrangling-data",
    "href": "chapters/23_ridge_lasso_regression/ridge_lasso_regression.html#wrangling-data",
    "title": "23  Ridge and Lasso Regression",
    "section": "23.3 Wrangling Data",
    "text": "23.3 Wrangling Data\nOur next task is to preform some wrangling on our data.\nWe begin by creating dummy variables for all our categorial features.\n\ndf_X = pd.get_dummies(df_credit.drop('balance', axis=1), drop_first=True)\ndf_y = df_credit[['balance']]\n\nWhen performing shrinkage methods, it is best to normalize the features.\n\nfrom sklearn.preprocessing import scale\nX_s = scale(df_X, with_mean=False)"
  },
  {
    "objectID": "chapters/23_ridge_lasso_regression/ridge_lasso_regression.html#ridge-regression",
    "href": "chapters/23_ridge_lasso_regression/ridge_lasso_regression.html#ridge-regression",
    "title": "23  Ridge and Lasso Regression",
    "section": "23.4 Ridge Regression",
    "text": "23.4 Ridge Regression\nWe are now ready to fit a ridge regression so let’s import the Ridge constructor.\n\nfrom sklearn.linear_model import Ridge\n\n\n23.4.1 Fitting a Ridge for a Single Value of alpha\nWe begin by fitting a ridge with a single value of alpha, which is the parameter that controls the amount of shrinkage to zero. Higher values correspond to more shrinkage. An alpha value of zero corresponds to the standard least squares model.\nLet’s start with alpha = 1.\n\nmodel = Ridge(alpha=1)\nmodel.fit(X_s, df_y)\n\nRidge(alpha=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=1)\n\n\nNext, let’s display the coefficients in a DataFrame.\n\ndf_coef = pd.DataFrame(\n    data = {'coefficient': list(model.intercept_) + list(np.ravel(model.coef_))},\n    index = ['intercept'] + list(df_X.columns.values), \n)\ndf_coef\n\n\n\n\n\n\n\n\ncoefficient\n\n\n\n\nintercept\n-487.398207\n\n\nincome\n-271.271818\n\n\nlimit\n367.168796\n\n\nrating\n245.258668\n\n\ncards\n21.330598\n\n\nage\n-10.896509\n\n\neducation\n-3.013622\n\n\nown_Yes\n-5.222885\n\n\nstudent_Yes\n126.864176\n\n\nmarried_Yes\n-4.718335\n\n\nregion_South\n5.083115\n\n\nregion_West\n7.600739\n\n\n\n\n\n\n\nNotice that effect size of income, limit, rating, and student_Yes are by far the largest. So we will focus on analyzing those in our subsequet analysis when we vary alpha.\n\n\n23.4.2 Fitting A Ridge for Multiple Values of alpha\nWe are now going fit our ridge model for various values of alpha and examine the coefficients and \\(R^2\\).\nThis code loops through alpha values, stores various values in lists, and then puts it all together nicely into a DataFrame\n\nalpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000, 1000000]\nincome_coef = []\nlimit_coef = []\nrating_coef = []\nstudent_yes_coef = []\nr_squared = []\nfor ix in alpha:\n    model = Ridge(alpha=ix)\n    model.fit(X_s, df_y)\n    coef = np.ravel(model.coef_)\n    income_coef.append(coef[0].round(2))\n    limit_coef.append(coef[1].round(2))\n    rating_coef.append(coef[2].round(2))\n    student_yes_coef.append(coef[7].round(2))\n    r_squared.append(model.score(X_s, df_y))\ndf_coefficient = pd.DataFrame(\n    {\n        'alpha':alpha,\n        'income':income_coef,\n        'limit':limit_coef,\n        'rating':rating_coef,\n        'student_Yes':student_yes_coef,\n        'r_squared':r_squared,\n    }\n)\ndf_coefficient\n\n\n\n\n\n\n\n\nalpha\nincome\nlimit\nrating\nstudent_Yes\nr_squared\n\n\n\n\n0\n0.001\n-274.67\n439.94\n175.78\n127.72\n0.955102\n\n\n1\n0.010\n-274.64\n438.55\n177.14\n127.71\n0.955102\n\n\n2\n0.100\n-274.34\n426.09\n189.33\n127.60\n0.955097\n\n\n3\n1.000\n-271.27\n367.17\n245.26\n126.86\n0.954974\n\n\n4\n10.000\n-242.12\n301.99\n281.05\n122.99\n0.952659\n\n\n5\n100.000\n-94.60\n211.39\n209.60\n97.64\n0.882671\n\n\n6\n1000.000\n22.93\n84.08\n84.22\n34.01\n0.550125\n\n\n7\n10000.000\n7.31\n14.44\n14.47\n4.57\n0.123256\n\n\n8\n100000.000\n0.84\n1.57\n1.57\n0.47\n0.014014\n\n\n9\n1000000.000\n0.09\n0.16\n0.16\n0.05\n0.001421\n\n\n\n\n\n\n\nNotice that the \\(R^2\\) decreases monotonically.\nLet’s now plot the coefficient values for various values of alpha.\n\nax = df_coefficient.\\\n        plot(\n            x = 'alpha',\n            y = ['income', 'limit', 'rating', 'student_Yes'],\n            title = 'Coefficient Values for Various alpha',\n            grid = True,\n            alpha = 0.75,\n            figsize = (9, 5),\n            logx=True,\n            );\nax.set_xlabel('alpha');\nax.set_ylabel('coefficient');\n\n\n\n\nAs you can see for large values of alpha the coefficients are close to zero, however they never quite reach zero."
  },
  {
    "objectID": "chapters/23_ridge_lasso_regression/ridge_lasso_regression.html#lasso",
    "href": "chapters/23_ridge_lasso_regression/ridge_lasso_regression.html#lasso",
    "title": "23  Ridge and Lasso Regression",
    "section": "23.5 Lasso",
    "text": "23.5 Lasso\nLet’s now repeat the same analysis for lasso regression. We begin by calculating coefficients and \\(R^2\\) for various values of alpha, and then display them in a DataFrame.\n\nfrom sklearn.linear_model import Lasso\nalpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000, 1000000]\nincome_coef = []\nlimit_coef = []\nrating_coef = []\nstudent_yes_coef = []\nr_squared = []\nfor ix in alpha:\n    model = Lasso(alpha=ix)\n    model.fit(X_s, df_y)\n    coef = np.ravel(model.coef_)\n    income_coef.append(coef[0].round(2))\n    limit_coef.append(coef[1].round(2))\n    rating_coef.append(coef[2].round(2))\n    student_yes_coef.append(coef[7].round(2))\n    r_squared.append(model.score(X_s, df_y))\ndf_coefficient = pd.DataFrame(\n    {\n        'alpha':alpha,\n        'income':income_coef,\n        'limit':limit_coef,\n        'rating':rating_coef,\n        'student_Yes':student_yes_coef,\n        'r_squared':r_squared,\n    }\n)\ndf_coefficient\n\n/home/pritam/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.516e+06, tolerance: 8.434e+03\n  model = cd_fast.enet_coordinate_descent(\n\n\n\n\n\n\n\n\n\nalpha\nincome\nlimit\nrating\nstudent_Yes\nr_squared\n\n\n\n\n0\n0.001\n-274.67\n442.55\n173.18\n127.74\n0.955101\n\n\n1\n0.010\n-274.64\n444.80\n170.89\n127.74\n0.955101\n\n\n2\n0.100\n-274.23\n449.54\n165.72\n127.67\n0.955099\n\n\n3\n1.000\n-270.07\n439.92\n171.01\n126.60\n0.955006\n\n\n4\n10.000\n-228.48\n335.30\n233.09\n116.13\n0.948874\n\n\n5\n100.000\n-0.00\n23.36\n273.31\n19.63\n0.718970\n\n\n6\n1000.000\n0.00\n0.00\n0.00\n0.00\n0.000000\n\n\n7\n10000.000\n0.00\n0.00\n0.00\n0.00\n0.000000\n\n\n8\n100000.000\n0.00\n0.00\n0.00\n0.00\n0.000000\n\n\n9\n1000000.000\n0.00\n0.00\n0.00\n0.00\n0.000000\n\n\n\n\n\n\n\nOnce again, \\(R^2\\) decreases monotonically.\nFinally, let’s plot the coefficients for various values of alpha. Notice that unlike ridge, with lasso the coefficients get squashed to zero.\n\nax = df_coefficient.\\\n        plot(\n            x = 'alpha',\n            y = ['income', 'limit', 'rating', 'student_Yes'],\n            title = 'Coefficient Values for Various alpha',\n            grid = True,\n            alpha = 0.75,\n            figsize = (9, 5),\n            logx=True,\n            );\nax.set_xlabel('alpha');\nax.set_ylabel('coefficient');"
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#various-vix-indices",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#various-vix-indices",
    "title": "24  K Nearest Neighbors & Cross-Validation",
    "section": "24.1 Various VIX Indices",
    "text": "24.1 Various VIX Indices\nLet’s begin with a brief discussion of the data we will be analyzing.\nThe VIX volatility index is published by the CBOE and is a measure of 30-day implied volatility for the S&P 500 index. Using that same methodology, the CBOE publishes other volatility measures on other stock indices and ETFs, such as the Russell 2000 and EWZ. Most of the CBOE volatility measures have a 30-day tenor, meaning they are calculated using options that have approximately 30 days to maturity.\nThere are, however, several CBOE volatility indices with different tenors. For the S&P 500, in addition to the standard 30-day VIX, there are indices with the following tenors: 9-day, 3-month, 6-month, and 1-year. The analysis in this tutorial is going to involve four of these different S&P 500 VIX tenors."
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#reading-in-the-data",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#reading-in-the-data",
    "title": "24  K Nearest Neighbors & Cross-Validation",
    "section": "24.2 Reading-In the Data",
    "text": "24.2 Reading-In the Data\nLet’s read-in our data set into a variable called df_vix.\n\ndf_vix = pd.read_csv('vix_knn.csv')\ndf_vix = df_vix[df_vix.trade_date &gt; '2011-01-03'] #removing the first row of NaNs\ndf_vix.head()\n\n\n\n\n\n\n\n\ntrade_date\nvix_009\nvix_030\nvix_090\nvix_180\nspy_ret\n\n\n\n\n1\n2011-01-04\n0.02\n-0.23\n-0.01\n-0.21\n-0.000551\n\n\n2\n2011-01-05\n-0.49\n-0.36\n-0.56\n-0.41\n0.005198\n\n\n3\n2011-01-06\n0.14\n0.38\n0.30\n0.09\n-0.001959\n\n\n4\n2011-01-07\n-0.70\n-0.26\n-0.06\n0.05\n-0.001962\n\n\n5\n2011-01-10\n0.80\n0.40\n0.19\n0.01\n-0.001259\n\n\n\n\n\n\n\nThis data set consists of daily SPY returns, and also daily changes in the 9-day, 30-day, 3-month, 6-month VIX indices. I excluded the 1-year index because there is limited historical data. The data is from 2011-2018."
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#a-visualization-aside-pair-plots-with-seaborn",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#a-visualization-aside-pair-plots-with-seaborn",
    "title": "24  K Nearest Neighbors & Cross-Validation",
    "section": "24.3 A Visualization Aside: Pair-Plots with Seaborn",
    "text": "24.3 A Visualization Aside: Pair-Plots with Seaborn\nBefore jumping into classification with KNN, let’s try the pairplot() function in the seaborn package. This function is useful for simultaneously visualizing pairwise relationships for several variables.\nLet’s apply this function to the various VIX indices in our data set.\n\nsns.pairplot(df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]);\n\n\n\n\n\nChallenge Question: How would you characterize the pairwise relationship between the various VIX tenors?\n\n\nSolution\n# All the vix indices are highly correlated with one another."
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#our-simple-classification-problem",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#our-simple-classification-problem",
    "title": "24  K Nearest Neighbors & Cross-Validation",
    "section": "24.4 Our Simple Classification Problem",
    "text": "24.4 Our Simple Classification Problem\nThe leverage effect is a stylized fact about equity index volatility markets; it encapsulates the observation there is an inverse relationship between returns and implied volatility. When returns are negative, implied vols increase; when returns are positive, implied vols decrease. In a previous chapter we quantified this relationship with a LinearRegression, and found the relationship to be quite strong.\nBased on our knowledge of this relationship let’s consider a simple classification exercise: identify whether a return was a “gain” or “loss”, based on changes in the various VIX indices.\nIn the language of machine learning:\n\nLabel: today’s return as a “gain” or “loss”\nFeatures: changes in the VIX indices"
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#some-simple-classifiers",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#some-simple-classifiers",
    "title": "24  K Nearest Neighbors & Cross-Validation",
    "section": "24.5 Some Simple Classifiers",
    "text": "24.5 Some Simple Classifiers\nFor illustrative purposes, and also for the purposes of benchmarking performance, let’s consider some simple classification schemes.\n\n24.5.1 Random Guessing\nA completely random prediction is right 50% of the time. This represents the lower-bound of performance of any learning model.\nThe code below uses DataFrame masking to check the performance of random guessing.\n\n# setting random seed\nnp.random.seed(100)\n\n# making random label predictions\ndf_vix['rand_label'] = np.random.randint(0, 2, df_vix.shape[0])\n\n# masking conditions that identify successful predictions\ncond1 = (df_vix.rand_label == 0) & (df_vix.spy_ret &lt;= 0)\ncond2 = (df_vix.rand_label == 1) & (df_vix.spy_ret &gt;= 0)\n\n# using masking to calculate the success rate\n(df_vix[cond1 | cond2].shape[0]) / (df_vix.shape[0])\n\n0.5017404276479364\n\n\n\n\n24.5.2 High Bias: Alway Guess ‘Gain’\nWhat if we always predict a gain? This is a great example of a classifier with a high bias and low variance. It’s smarter than random guessing because it is rooted in the knowledge that over long stretches of time, equity markets tend to rise (and also that markets rarely ever jump upwards).\n\n# always predict gain\n(df_vix[df_vix.spy_ret &gt; 0].shape[0]) / (df_vix.shape[0])\n\n0.548483341621084\n\n\nSince about 55% of the days in df_vix were gains for SPY, this predictor would have been right 55% of the time.\n\n\n24.5.3 Leverage Effect Rule\nWe could also create a simple rule-based classification that codifies our knowledge of the implied leverage effect. The rule could simply be that if there is an increase in the VIX, predict a SPY loss; if there is a decrease in the VIX predict an SPY gain. We will use 30-day VIX for this classifier.\nThis simple rule produces accurate labels 80% of the time, which further illustrates the strength of the implied leverate effect.\n\n# conditions that define successful predictions\ncond1 = (df_vix.vix_030 &gt;= 0) & (df_vix.spy_ret &lt;= 0)\ncond2 = (df_vix.vix_030 &lt;= 0) & (df_vix.spy_ret &gt;= 0)\n\n# calculating the proportions of successful conditions\n(df_vix[cond1 | cond2].shape[0]) / (df_vix.shape[0])\n\n0.8050721034311288\n\n\nA more sophisticated classifier involving VIX changes should perform at least as well as this, or else it is not adding much beyond simply capturing the leverage effect."
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#k-nearest-neighbors-knn",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#k-nearest-neighbors-knn",
    "title": "24  K Nearest Neighbors & Cross-Validation",
    "section": "24.6 K Nearest Neighbors (KNN)",
    "text": "24.6 K Nearest Neighbors (KNN)\nKNN is a simple classification algorithm that is based on the following intuitive principle: feature observations that are similar, should have similar associated labels. Feature similarity is determined by distance in Euclidean space.\nHere is how the KNN works. Suppose you are trying to predict a label for a given feature observation:\n\nFind the \\(K\\) feature samples in the the training set that are closest to your feature observation.\nFind the labels associated with those \\(K\\) closest samples.\nThe KNN prediction is the label that occurs most often in that set of \\(K\\) labels.\n\nKNN is an example of an instance-based classifier, there really is no “fitting” process other than storing the training data. The prediction algorithm amounts to calculating distances between the feature observation and the other feature observations in the train set, sorting the feature set by this distance, and then surveying the labels of the \\(K\\) closest feature observations.\n\n24.6.1 Preparing the Data\nIn anticipation of performing KNN on our data, let’s add a label column and do some additional data cleaning.\nThe following code defines a simple function that will add a column of labels to our data set: L stands for a loss, G stands for a gain.\n\ndef labeler(ret):\n    if ret &lt; 0:\n        return('L')\n    else:\n        return('G')\n\nNext, let’s add a label column called spy_label_0. We can do this conveniently with the .apply() method which has the effect of vectorizing our labeler function.\n\ndf_vix.drop(['rand_label'], axis=1, inplace=True)\ndf_vix['spy_label_0'] = df_vix['spy_ret'].apply(labeler)\ndf_vix.head()\n\n\n\n\n\n\n\n\ntrade_date\nvix_009\nvix_030\nvix_090\nvix_180\nspy_ret\nspy_label_0\n\n\n\n\n1\n2011-01-04\n0.02\n-0.23\n-0.01\n-0.21\n-0.000551\nL\n\n\n2\n2011-01-05\n-0.49\n-0.36\n-0.56\n-0.41\n0.005198\nG\n\n\n3\n2011-01-06\n0.14\n0.38\n0.30\n0.09\n-0.001959\nL\n\n\n4\n2011-01-07\n-0.70\n-0.26\n-0.06\n0.05\n-0.001962\nL\n\n\n5\n2011-01-10\n0.80\n0.40\n0.19\n0.01\n-0.001259\nL\n\n\n\n\n\n\n\n\n\n24.6.2 Predicting with 30-day VIX\nIn this section we’ll apply KNN to various subsets of our VIX features, and train our model using the entirety of our data set.\nLet’s begin by importing the contructor function that we will need.\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nWe are now ready to fit our classifier model, let’s begin by predicting the current day label with the standard (30-day) VIX. We are going to use a \\(K\\) value of 10. As usual, our steps are:\n\nFeature selection: identify the data we will be using to train with.\nModel selection: instantiate the model using the constructor, and set model hyperparameters.\nFitting: using the .fit() of our instantiated mode.\n\n\n# (1) feature selection \nX = df_vix[['vix_030']]\ny = df_vix['spy_label_0'].values\n\n# (2) model selection and hyper-parameters\nclf = KNeighborsClassifier(n_neighbors = 10)\n\n# (3) fitting the model\nclf.fit(X, y)\n\nKNeighborsClassifier(n_neighbors=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=10)\n\n\nThe most fundamental metric for investigating the quality of a classifier is accuracy, which is simply the proportion of correct classifications.\nRecall that for a LinearRegression, the .score() method gave the \\(R^2\\) of the model. For a KNeighborsClassifier the .score() method gives the accuracy.\n\nclf.score(X, y)\n\n0.8219791148682247\n\n\nOur accuracy is much better than random guessing, or a constant guess of G. However, our KNN only slightly outperforms the leverage effect rule.\n\n\n24.6.3 Predicting with Multiple VIX Indices\nLet’s now add in the other VIX indices as additional features. Ultimately, we’re interested in seeing if our in-sample accuracy improves.\nWhen considering multiple features it is important to normalize the features to make sure they are all the same order of magnitude. We can do this easily with the scale() function, who’s default behavior is to subtract the mean, and divide by the standard deviation.\n\n# importing scale function\nfrom sklearn.preprocessing import scale\n\n## (1) feature selection\nX = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\ny = df_vix['spy_label_0'].values\n\n# scaling fetures\nXs = scale(X)\n\n## (2) model selection\nclf = KNeighborsClassifier(n_neighbors = 10)\n\n## (3) fitting the model\nclf.fit(X, y)\n\n# checking in-sample accuracy score\nprint(clf.score(X, y))\n\n0.841869716558926\n\n\nAs we can see, there is a slight improvement in our in-sample accuracy.\n\nDiscussion Question: What would happen if we added option volume to our analysis but didn’t normalize our feature set?\n\n\nSolution\n# Option volumes are numbers that are much larger than VIX values.  \n# Therefore, if they are not normalized, they will dominate the fitting and \n# the VIX values will be ignored.\n\n\n\nCode Challenge: Rerun the analysis with \\(K = 1\\). What do you make of the accuracy score?\n\n\nSolution\n# importing scale function\nfrom sklearn.preprocessing import scale\n\n## (1) feature selection\nX = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\ny = df_vix['spy_label_0'].values\n\n# scaling fetures\nXs = scale(X)\n\n## (2) model selection\nclf = KNeighborsClassifier(n_neighbors = 1)\n\n## (3) fitting the model\nclf.fit(X, y)\n\n# checking in-sample accuracy score\nprint(clf.score(X, y))\n\n\n1.0"
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#hold-out-sets",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#hold-out-sets",
    "title": "24  K Nearest Neighbors & Cross-Validation",
    "section": "24.7 Hold-Out Sets",
    "text": "24.7 Hold-Out Sets\nSo far we have only been looking at our accuracy score in-sample, meaning we are simply calculating the accuracy score on the training set. This gives a skewed perception of how accurate the model will be on new data.\nIn order to account for this, we can partition the data set into two subsets, one for training the model and one for testing the trained model. The model_selection module contains a convenience function for splitting data into training sets and testing sets. It is called train_test_split(). Let’s import it now.\n\nfrom sklearn.model_selection import train_test_split\n\nLet’s again perform a KNN with multiple VIX indices, but this time splitting the data set. We will then calculate the accuracy score of the model on both the training set and the test set.\n\n## (1) feature selection \nX = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\ny = df_vix['spy_label_0'].values\n\n# scaling fetures\nXs = scale(X)\n\n# train-test_split the data\nX_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.20, random_state=0)\n\n## (2) model selection\nclf = KNeighborsClassifier(n_neighbors = 10)\n\n## (3) fitting the model\nclf.fit(X_train, y_train)\n\nKNeighborsClassifier(n_neighbors=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=10)\n\n\nNow that we have fit our model, let’s check for accuracy on both the training set and the test set.\n\nprint('Train Accuracy: ', np.round(clf.score(X_train, y_train), 4))\nprint('Test Accuracy:  ', np.round(clf.score(X_test, y_test), 4))\n\nTrain Accuracy:  0.8451\nTest Accuracy:   0.8337\n\n\nNotice that the accuracy on the test set is lower than the accuracy on the training set. This is almost always the case.\n\nCoding Challenge: Copy and paste the code from the example above, and then re-run it using a \\(K=1\\). Is the spread between training accuracy and testing accuracy larger or smaller than when \\(K=10\\)?\n\n\nSolution\n## (1) feature selection \nX = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\ny = df_vix['spy_label_0'].values\n\n# scaling fetures\nXs = scale(X)\n\n# train-test_split the data\nX_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.20, random_state=0)\n\n## (2) model selection\nclf = KNeighborsClassifier(n_neighbors = 1)\n\n## (3) fitting the model\nclf.fit(X_train, y_train)\n\nprint('Train Accuracy: ', np.round(clf.score(X_train, y_train), 4))\nprint('Test Accuracy:  ', np.round(clf.score(X_test, y_test), 4))\n\n\nTrain Accuracy:  1.0\nTest Accuracy:   0.804"
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#cross-validation",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#cross-validation",
    "title": "24  K Nearest Neighbors & Cross-Validation",
    "section": "24.8 Cross-Validation",
    "text": "24.8 Cross-Validation\nIn the previous section we used train_test_split() to partition our data so that we could check the accuracy of our model on observations that were not used in the fitting process. When doing this we noticed that the accuracy of our model is lower on the testing set then on the training set. The testing set accuracy is more reflective of how the model would perform in the wild.\nBut why stop with just doing this splitting once? We could could do a train_test_split() multiple times, each time producing a different test accuracy. This collection of accuracies, in aggregate, would form a more robust measure of model performance. This is precisely the notion of model cross-validation.\nThe cross_val_score() function in the model_selection module provides a convenient way to perform cross-validation.\n\n# importing cross_val_score\nfrom sklearn.model_selection import cross_val_score\n\n# feature selection \nX = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\ny = df_vix['spy_label_0'].values\n\n# scaling fetures\nXs = scale(X)\n\n# model selection\nclf = KNeighborsClassifier(n_neighbors = 10)\n\n# cross validation\ncross_val_score(clf, Xs, y, cv=5)\n\narray([0.82878412, 0.77363184, 0.83333333, 0.7960199 , 0.84825871])\n\n\nThe code above splits the data into five parts, and then performs a train-test-split on each of them. Convenience functions like cross_val_score() are one of the reasons that sklearn is such a popular library. This function works the same with all kinds of different models and saves us from writing a lot of boilerplate code."
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#visualizing-the-variance-bias-trade-off",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#visualizing-the-variance-bias-trade-off",
    "title": "24  K Nearest Neighbors & Cross-Validation",
    "section": "24.9 Visualizing the Variance-Bias Trade-Off",
    "text": "24.9 Visualizing the Variance-Bias Trade-Off\nAs discussed in a code challenge above, when \\(K=1\\) the in-sample accuracy is perfect, but the out-of-sample accuracy is relatively poor. This is a classic illustration of over-fitting the data. By setting \\(K=1\\) we are allowing for maximum model complexity. Said in another way, we are attributing a lot of informational value to each (noisy) training observation.\nThe following code allows us to visualize this.\n\n# feature selection \nX = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\ny = df_vix['spy_label_0'].values\n\n# scaling fetures\nXs = scale(X)\n\n# train-test_split the data\nX_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.20, random_state=0)\n\n# various choices of neighbors\nk_neighbors = list(range(1, 20))\n\ntraining_error = []\ntesting_error = []\n\nfor k in k_neighbors:\n    clf = KNeighborsClassifier(n_neighbors=k)\n    clf.fit(X_train, y_train)\n    training_error.append(clf.score(X_train, y_train))\n    testing_error.append(clf.score(X_test, y_test))\n    \n\n# plotting training and testing errors for various K\nplt.figure(figsize=(8, 6))\nplt.plot(k_neighbors, training_error)\nplt.plot(k_neighbors, testing_error)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Accuracy')\nplt.show()\n\n\n\n\nAs we reduce the complexity of our algorithm (by increasing the value of \\(K\\)) the training accuracy and testing accuracy converge.\n\nDiscussion Question: Based on this graph, which \\(K\\) value would you choose and why?\n\n\nSolution\n# I would probably choose K=10.  This allows for maximum model variance, \n# while maintaining minimal bias."
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#hyperparameter-selection-with-cross-validation",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#hyperparameter-selection-with-cross-validation",
    "title": "24  K Nearest Neighbors & Cross-Validation",
    "section": "24.10 Hyperparameter Selection with Cross-Validation",
    "text": "24.10 Hyperparameter Selection with Cross-Validation\nAbove we saw that an \\(n\\)-fold cross-validation will produce \\(n\\) different test set errors, resulting from \\(n\\) different training sets of the model. Cross-validation can be used for the purposes of hyperparameter selection because averaging over the cross-validation scores is a more robust measurement of model performance.\nIn the following code, we perform a 10-fold cross validation for \\(K=1, \\ldots, 50\\).\n\n# feature selection \nX = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\ny = df_vix['spy_label_0'].values\n\n# scaling fetures\nXs = scale(X)\n\n# creating odd list of K for KNN\nk_neighbors = list(range(1, 50))\n\n# empty list that will hold cv scores\ncv_scores = []\n\n# perform 10-fold cross validation\nfor k in k_neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n    cv_scores.append(scores.mean())\n\nHyperparameter selection is usually couched in terms of misclassification rate, which is simply 1 - accuracy.\n\n# changing to misclassification error\nMSE = [1 - x for x in cv_scores]\n\n\n# plot misclassification error vs k\nplt.figure(figsize=(8, 6))\nplt.plot(k_neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()\n\n\n\n\n\nDiscussion Question: Based on this graph, which value of \\(K\\) would you choose?\n\n\nSolution\n# Based on this graph I would probably choose K=15, which allows for maximal \n# model complexity with minimal misclassification."
  },
  {
    "objectID": "chapters/25_stock_prediction/stock_prediction.html#import-packages",
    "href": "chapters/25_stock_prediction/stock_prediction.html#import-packages",
    "title": "25  Predicting Stock Returns",
    "section": "25.1 Import Packages",
    "text": "25.1 Import Packages\nLet’s begin by loading the packages that we will need.\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr\nimport sklearn"
  },
  {
    "objectID": "chapters/25_stock_prediction/stock_prediction.html#reading-in-data",
    "href": "chapters/25_stock_prediction/stock_prediction.html#reading-in-data",
    "title": "25  Predicting Stock Returns",
    "section": "25.2 Reading-In Data",
    "text": "25.2 Reading-In Data\nNext, let’s read-in our data. We will start the stocks, whose data we will get from Yahoo Finance.\n\nstock_tickers = ['MSFT', 'IBM', 'GOOGL'] # define tickers\ndf_stock = pdr.get_data_yahoo(stock_tickers, start='2005-01-01', end='2021-07-31') # grab the data\ndf_stock = df_stock['Adj Close'] # select only the adjusted close price\ndf_stock.columns = df_stock.columns.str.lower() # clean-up column names\ndf_stock.rename_axis('trade_date', inplace=True) # clean-up index name\ndf_stock.rename_axis('', axis=1, inplace=True) # clean-up index name\ndf_stock\n\n[*********************100%***********************]  3 of 3 completed\n\n\n\n\n\n\n\n\n\ngoogl\nibm\nmsft\n\n\ntrade_date\n\n\n\n\n\n\n\n2005-01-03\n5.072823\n53.707920\n18.732994\n\n\n2005-01-04\n4.867367\n53.131016\n18.803059\n\n\n2005-01-05\n4.842593\n53.021130\n18.761021\n\n\n2005-01-06\n4.718468\n52.856297\n18.740002\n\n\n2005-01-07\n4.851101\n52.625530\n18.683958\n\n\n...\n...\n...\n...\n\n\n2021-07-26\n134.035004\n122.236877\n283.364075\n\n\n2021-07-27\n131.899994\n122.219757\n280.903412\n\n\n2021-07-28\n136.093994\n121.380692\n280.589722\n\n\n2021-07-29\n135.777496\n121.517693\n280.864197\n\n\n2021-07-30\n134.726501\n120.687180\n279.305511\n\n\n\n\n4173 rows × 3 columns\n\n\n\nNext we’ll grab currency data from FRED.\n\ncurrency_tickers = ['DEXJPUS', 'DEXUSUK']\ndf_currency = pdr.get_data_fred(currency_tickers, start='2005-01-01', end='2021-07-31')\ndf_currency = df_currency\ndf_currency.columns = df_currency.columns.str.lower()\ndf_currency.rename_axis('trade_date', inplace=True)\ndf_currency.rename_axis('', axis=1, inplace=True)\ndf_currency\n\n\n\n\n\n\n\n\ndexjpus\ndexusuk\n\n\ntrade_date\n\n\n\n\n\n\n2005-01-03\n102.83\n1.9058\n\n\n2005-01-04\n104.27\n1.8834\n\n\n2005-01-05\n103.95\n1.8875\n\n\n2005-01-06\n104.87\n1.8751\n\n\n2005-01-07\n104.93\n1.8702\n\n\n...\n...\n...\n\n\n2021-07-26\n110.31\n1.3829\n\n\n2021-07-27\n109.64\n1.3884\n\n\n2021-07-28\n110.06\n1.3884\n\n\n2021-07-29\n109.53\n1.3966\n\n\n2021-07-30\n109.70\n1.3913\n\n\n\n\n4325 rows × 2 columns\n\n\n\nFinally, we’ll grab index data from Yahoo Finance.\n\nindex_tickers = ['SPY', 'DIA', '^VIX'] \ndf_index = pdr.get_data_yahoo(index_tickers, start='2005-01-01', end='2021-07-31')\ndf_index = df_index['Adj Close']\ndf_index.columns = df_index.columns.str.lower().str.replace('^', '')\ndf_index.rename_axis('trade_date', inplace=True)\ndf_index.rename_axis('', axis=1, inplace=True)\ndf_index\n\n[*********************100%***********************]  3 of 3 completed\n\n\n\n\n\n\n\n\n\ndia\nspy\nvix\n\n\ntrade_date\n\n\n\n\n\n\n\n2005-01-03\n70.050133\n84.258598\n14.080000\n\n\n2005-01-04\n69.384399\n83.228989\n13.980000\n\n\n2005-01-05\n68.999268\n82.654648\n14.090000\n\n\n2005-01-06\n69.214714\n83.074913\n13.580000\n\n\n2005-01-07\n69.084152\n82.955833\n13.490000\n\n\n...\n...\n...\n...\n\n\n2021-07-26\n337.707458\n427.850800\n17.580000\n\n\n2021-07-27\n336.948181\n425.900879\n19.360001\n\n\n2021-07-28\n335.737305\n425.726227\n18.309999\n\n\n2021-07-29\n337.140411\n427.491821\n17.700001\n\n\n2021-07-30\n335.852661\n425.415710\n18.240000\n\n\n\n\n4173 rows × 3 columns"
  },
  {
    "objectID": "chapters/25_stock_prediction/stock_prediction.html#join-and-clean-data",
    "href": "chapters/25_stock_prediction/stock_prediction.html#join-and-clean-data",
    "title": "25  Predicting Stock Returns",
    "section": "25.3 Join and Clean Data",
    "text": "25.3 Join and Clean Data\nNow we can join together our price data and convert it into returns (we actually use differences for VIX as these are more stationary). Notice that we are implicitly adding a time series component to our regression by adding lagged msft returns as a feature.\n\ndf_data = \\\n    (\n    df_stock\n        .merge(df_index, how='left', left_index=True, right_index=True) # join currency data\n        .merge(df_currency, how='left', left_index=True, right_index=True) # join index data\n        .dropna()\n        .assign(msft = lambda df: df['msft'].pct_change())   # percent change\n        .assign(msft_lag_0 = lambda df: df['msft'].shift(0)) #\n        .assign(msft_lag_1 = lambda df: df['msft'].shift(1)) #\n        .assign(ibm = lambda df: df['ibm'].pct_change())     #\n        .assign(googl = lambda df: df['googl'].pct_change()) #\n        .assign(spy = lambda df: df['spy'].pct_change())     #\n        .assign(dia = lambda df: df['dia'].pct_change())     #\n        .assign(vix = lambda df: df['vix'].diff())           # absolute change\n        .assign(dexjpus = lambda df: df['dexjpus'].pct_change()) # percent change\n        .assign(dexusuk = lambda df: df['dexusuk'].pct_change()) #\n        .dropna()\n    )\ndf_data\n\n\n\n\n\n\n\n\ngoogl\nibm\nmsft\ndia\nspy\nvix\ndexjpus\ndexusuk\nmsft_lag_0\nmsft_lag_1\n\n\ntrade_date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2005-01-05\n-0.005090\n-0.002068\n-0.002236\n-0.005551\n-0.006901\n0.110001\n-0.003069\n0.002177\n-0.002236\n0.003740\n\n\n2005-01-06\n-0.025632\n-0.003109\n-0.001120\n0.003122\n0.005085\n-0.510000\n0.008850\n-0.006570\n-0.001120\n-0.002236\n\n\n2005-01-07\n0.028109\n-0.004366\n-0.002991\n-0.001886\n-0.001433\n-0.090000\n0.000572\n-0.002613\n-0.002991\n-0.001120\n\n\n2005-01-10\n0.006242\n-0.001044\n0.004875\n0.003401\n0.004729\n-0.260000\n-0.005813\n0.002620\n0.004875\n-0.002991\n\n\n2005-01-11\n-0.007793\n-0.007107\n-0.002612\n-0.006404\n-0.006891\n-0.040000\n-0.008627\n0.002400\n-0.002612\n0.004875\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2021-07-26\n0.007668\n0.010118\n-0.002140\n0.002396\n0.002455\n0.379999\n-0.001900\n0.005819\n-0.002140\n0.012337\n\n\n2021-07-27\n-0.015929\n-0.000140\n-0.008684\n-0.002248\n-0.004557\n1.780001\n-0.006074\n0.003977\n-0.008684\n-0.002140\n\n\n2021-07-28\n0.031797\n-0.006865\n-0.001117\n-0.003594\n-0.000410\n-1.050001\n0.003831\n0.000000\n-0.001117\n-0.008684\n\n\n2021-07-29\n-0.002326\n0.001129\n0.000978\n0.004179\n0.004147\n-0.609999\n-0.004816\n0.005906\n0.000978\n-0.001117\n\n\n2021-07-30\n-0.007741\n-0.006835\n-0.005550\n-0.003820\n-0.004856\n0.539999\n0.001552\n-0.003795\n-0.005550\n0.000978\n\n\n\n\n4132 rows × 10 columns"
  },
  {
    "objectID": "chapters/25_stock_prediction/stock_prediction.html#training-set-and-testing-set",
    "href": "chapters/25_stock_prediction/stock_prediction.html#training-set-and-testing-set",
    "title": "25  Predicting Stock Returns",
    "section": "25.4 Training Set and Testing Set",
    "text": "25.4 Training Set and Testing Set\nWe’ll train our models on data prior to 2016, and then we’ll use data from 2016 onward for testing. So let’s separate out these two subsets of data.\n\ndf_train = df_data.query('trade_date &lt; \"2016-01-01\"')\ndf_train\n\n\n\n\n\n\n\n\ngoogl\nibm\nmsft\ndia\nspy\nvix\ndexjpus\ndexusuk\nmsft_lag_0\nmsft_lag_1\n\n\ntrade_date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2005-01-05\n-0.005090\n-0.002068\n-0.002236\n-0.005551\n-0.006901\n0.110001\n-0.003069\n0.002177\n-0.002236\n0.003740\n\n\n2005-01-06\n-0.025632\n-0.003109\n-0.001120\n0.003122\n0.005085\n-0.510000\n0.008850\n-0.006570\n-0.001120\n-0.002236\n\n\n2005-01-07\n0.028109\n-0.004366\n-0.002991\n-0.001886\n-0.001433\n-0.090000\n0.000572\n-0.002613\n-0.002991\n-0.001120\n\n\n2005-01-10\n0.006242\n-0.001044\n0.004875\n0.003401\n0.004729\n-0.260000\n-0.005813\n0.002620\n0.004875\n-0.002991\n\n\n2005-01-11\n-0.007793\n-0.007107\n-0.002612\n-0.006404\n-0.006891\n-0.040000\n-0.008627\n0.002400\n-0.002612\n0.004875\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2015-12-24\n-0.003474\n-0.002093\n-0.002687\n-0.003357\n-0.001650\n0.170000\n-0.005127\n0.005382\n-0.002687\n0.008491\n\n\n2015-12-28\n0.021414\n-0.004629\n0.005030\n-0.001369\n-0.002285\n1.170000\n-0.000166\n-0.004015\n0.005030\n-0.002687\n\n\n2015-12-29\n0.014983\n0.015769\n0.010724\n0.011430\n0.010672\n-0.830000\n0.001164\n-0.005980\n0.010724\n0.005030\n\n\n2015-12-30\n-0.004610\n-0.003148\n-0.004244\n-0.006667\n-0.007088\n1.210001\n0.001328\n0.002569\n-0.004244\n0.010724\n\n\n2015-12-31\n-0.015551\n-0.012344\n-0.014740\n-0.010296\n-0.010003\n0.919998\n-0.002736\n-0.005798\n-0.014740\n-0.004244\n\n\n\n\n2745 rows × 10 columns\n\n\n\n\ndf_test = df_data.query('trade_date &gt;= \"2016-01-01\"')\ndf_test\n\n\n\n\n\n\n\n\ngoogl\nibm\nmsft\ndia\nspy\nvix\ndexjpus\ndexusuk\nmsft_lag_0\nmsft_lag_1\n\n\ntrade_date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-01-04\n-0.023869\n-0.012135\n-0.012257\n-0.015517\n-0.013980\n2.490002\n-0.008065\n-0.004069\n-0.012257\n-0.014740\n\n\n2016-01-05\n0.002752\n-0.000736\n0.004562\n0.000583\n0.001691\n-1.360001\n-0.002934\n-0.001498\n0.004562\n-0.012257\n\n\n2016-01-06\n-0.002889\n-0.005006\n-0.018165\n-0.014295\n-0.012614\n1.250000\n-0.003447\n-0.002591\n-0.018165\n0.004562\n\n\n2016-01-07\n-0.024140\n-0.017089\n-0.034783\n-0.023558\n-0.023991\n4.400000\n-0.004555\n-0.003213\n-0.034783\n-0.018165\n\n\n2016-01-08\n-0.013617\n-0.009258\n0.003067\n-0.010427\n-0.010977\n2.020000\n-0.002203\n-0.003841\n0.003067\n-0.034783\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2021-07-26\n0.007668\n0.010118\n-0.002140\n0.002396\n0.002455\n0.379999\n-0.001900\n0.005819\n-0.002140\n0.012337\n\n\n2021-07-27\n-0.015929\n-0.000140\n-0.008684\n-0.002248\n-0.004557\n1.780001\n-0.006074\n0.003977\n-0.008684\n-0.002140\n\n\n2021-07-28\n0.031797\n-0.006865\n-0.001117\n-0.003594\n-0.000410\n-1.050001\n0.003831\n0.000000\n-0.001117\n-0.008684\n\n\n2021-07-29\n-0.002326\n0.001129\n0.000978\n0.004179\n0.004147\n-0.609999\n-0.004816\n0.005906\n0.000978\n-0.001117\n\n\n2021-07-30\n-0.007741\n-0.006835\n-0.005550\n-0.003820\n-0.004856\n0.539999\n0.001552\n-0.003795\n-0.005550\n0.000978\n\n\n\n\n1387 rows × 10 columns"
  },
  {
    "objectID": "chapters/25_stock_prediction/stock_prediction.html#training",
    "href": "chapters/25_stock_prediction/stock_prediction.html#training",
    "title": "25  Predicting Stock Returns",
    "section": "25.5 Training",
    "text": "25.5 Training\nIn order to train our model, we first put our training features into X_train and our training labels into y_train\n\nX_train = df_train.drop(columns=['msft'])[0:len(df_train)-1]\nX_train\n\n\n\n\n\n\n\n\ngoogl\nibm\ndia\nspy\nvix\ndexjpus\ndexusuk\nmsft_lag_0\nmsft_lag_1\n\n\ntrade_date\n\n\n\n\n\n\n\n\n\n\n\n\n\n2005-01-05\n-0.005090\n-0.002068\n-0.005551\n-0.006901\n0.110001\n-0.003069\n0.002177\n-0.002236\n0.003740\n\n\n2005-01-06\n-0.025632\n-0.003109\n0.003122\n0.005085\n-0.510000\n0.008850\n-0.006570\n-0.001120\n-0.002236\n\n\n2005-01-07\n0.028109\n-0.004366\n-0.001886\n-0.001433\n-0.090000\n0.000572\n-0.002613\n-0.002991\n-0.001120\n\n\n2005-01-10\n0.006242\n-0.001044\n0.003401\n0.004729\n-0.260000\n-0.005813\n0.002620\n0.004875\n-0.002991\n\n\n2005-01-11\n-0.007793\n-0.007107\n-0.006404\n-0.006891\n-0.040000\n-0.008627\n0.002400\n-0.002612\n0.004875\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2015-12-23\n0.001799\n0.004423\n0.010344\n0.012383\n-1.030001\n0.000000\n0.003511\n0.008491\n0.009484\n\n\n2015-12-24\n-0.003474\n-0.002093\n-0.003357\n-0.001650\n0.170000\n-0.005127\n0.005382\n-0.002687\n0.008491\n\n\n2015-12-28\n0.021414\n-0.004629\n-0.001369\n-0.002285\n1.170000\n-0.000166\n-0.004015\n0.005030\n-0.002687\n\n\n2015-12-29\n0.014983\n0.015769\n0.011430\n0.010672\n-0.830000\n0.001164\n-0.005980\n0.010724\n0.005030\n\n\n2015-12-30\n-0.004610\n-0.003148\n-0.006667\n-0.007088\n1.210001\n0.001328\n0.002569\n-0.004244\n0.010724\n\n\n\n\n2744 rows × 9 columns\n\n\n\nNotice that the label we are predicting is the next day msft return; the features we are using to predict are the current day returns of the various correlated assets.\n\ny_train = df_train[['msft']][1:len(df_train)]\ny_train\n\n\n\n\n\n\n\n\nmsft\n\n\ntrade_date\n\n\n\n\n\n2005-01-06\n-0.001120\n\n\n2005-01-07\n-0.002991\n\n\n2005-01-10\n0.004875\n\n\n2005-01-11\n-0.002612\n\n\n2005-01-12\n0.001870\n\n\n...\n...\n\n\n2015-12-24\n-0.002687\n\n\n2015-12-28\n0.005030\n\n\n2015-12-29\n0.010724\n\n\n2015-12-30\n-0.004244\n\n\n2015-12-31\n-0.014740\n\n\n\n\n2744 rows × 1 columns\n\n\n\n\n25.5.1 Linear Regression\nLet’s first fit a simple linear regression to our training data.\n\nfrom sklearn.linear_model import LinearRegression\nlinear_regression = LinearRegression()\nlinear_regression.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nRecall that the .score() of a Linear Regression gives the \\(R^2\\).\n\nprint(\"LR R^2:\", linear_regression.score(X_train, y_train))\n\nLR R^2: 0.017818077856861736\n\n\nWe can also examine the coefficients of our model.\n\nnp.round(linear_regression.coef_, 3)\n\narray([[ 0.004, -0.027,  0.288, -0.432,  0.   ,  0.105, -0.006,  0.031,\n        -0.027]])\n\n\n\nCode Challenge: Implement a Lasso regression:\n\nExperiment with the alpha parameter.\n\nExamine the coefficients (.coef_ attribute) of the model\n\nDoes using a Lasso over a Linear Regression seem like a good idea?\n\n\nSolution\nfrom sklearn.linear_model import Lasso\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_train, y_train)\nprint(lasso.score(X_train, y_train))\nprint(lasso.coef_)\n\n# The model seems very sensitive to `alpha`; anything but \n# tiny values makes all the coefficients zero.  Moreover, the $R^2$ \n# doesn't seem to improve so Lasso isn't an improvement over \n# linear regression.\n\n\n0.0\n[-0. -0. -0. -0.  0.  0. -0. -0. -0.]\n\n\n\n\n\n25.5.2 KNN\nNext, let’s fit a KNN model to our data.\n\nfrom sklearn.neighbors import KNeighborsRegressor\nknn = KNeighborsRegressor(n_neighbors=10)\nknn.fit(X_train, y_train)\n\nKNeighborsRegressor(n_neighbors=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsRegressorKNeighborsRegressor(n_neighbors=10)\n\n\nAs you can see, the in-sample \\(R^2\\) is higher for KNN over Linear Regression.\n\nprint(\"KNN R^2:\", knn.score(X_train, y_train))\n\nKNN R^2: 0.10833738212937916\n\n\n\n\n25.5.3 Mean-Squared Error\nAnother goodness of fit metric is the mean squared error. As you can see the models are close on this metric.\n\nprint(\"LR MSE: \", \\\n      sklearn.metrics.mean_squared_error(y_train, linear_regression.predict(X_train)))\nprint(\"KNN MSE:\", \\\n      sklearn.metrics.mean_squared_error(y_train, knn.predict(X_train)))\n\nLR MSE:  0.0002835383523398787\nKNN MSE: 0.000257407048342368"
  },
  {
    "objectID": "chapters/25_stock_prediction/stock_prediction.html#testing",
    "href": "chapters/25_stock_prediction/stock_prediction.html#testing",
    "title": "25  Predicting Stock Returns",
    "section": "25.6 Testing",
    "text": "25.6 Testing\nLet’s now test the model with the data after 2016.\n\nX_test = df_test.drop(columns=['msft'])[0:len(df_test)-1]\nX_test\n\n\n\n\n\n\n\n\ngoogl\nibm\ndia\nspy\nvix\ndexjpus\ndexusuk\nmsft_lag_0\nmsft_lag_1\n\n\ntrade_date\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-01-04\n-0.023869\n-0.012135\n-0.015517\n-0.013980\n2.490002\n-0.008065\n-0.004069\n-0.012257\n-0.014740\n\n\n2016-01-05\n0.002752\n-0.000736\n0.000583\n0.001691\n-1.360001\n-0.002934\n-0.001498\n0.004562\n-0.012257\n\n\n2016-01-06\n-0.002889\n-0.005006\n-0.014295\n-0.012614\n1.250000\n-0.003447\n-0.002591\n-0.018165\n0.004562\n\n\n2016-01-07\n-0.024140\n-0.017089\n-0.023558\n-0.023991\n4.400000\n-0.004555\n-0.003213\n-0.034783\n-0.018165\n\n\n2016-01-08\n-0.013617\n-0.009258\n-0.010427\n-0.010977\n2.020000\n-0.002203\n-0.003841\n0.003067\n-0.034783\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2021-07-23\n0.035769\n0.004477\n0.006633\n0.010288\n-0.490000\n0.003815\n-0.000073\n0.012337\n0.016844\n\n\n2021-07-26\n0.007668\n0.010118\n0.002396\n0.002455\n0.379999\n-0.001900\n0.005819\n-0.002140\n0.012337\n\n\n2021-07-27\n-0.015929\n-0.000140\n-0.002248\n-0.004557\n1.780001\n-0.006074\n0.003977\n-0.008684\n-0.002140\n\n\n2021-07-28\n0.031797\n-0.006865\n-0.003594\n-0.000410\n-1.050001\n0.003831\n0.000000\n-0.001117\n-0.008684\n\n\n2021-07-29\n-0.002326\n0.001129\n0.004179\n0.004147\n-0.609999\n-0.004816\n0.005906\n0.000978\n-0.001117\n\n\n\n\n1386 rows × 9 columns\n\n\n\n\ny_test = df_test[['msft']][1:len(df_test)]\ny_test\n\n\n\n\n\n\n\n\nmsft\n\n\ntrade_date\n\n\n\n\n\n2016-01-05\n0.004562\n\n\n2016-01-06\n-0.018165\n\n\n2016-01-07\n-0.034783\n\n\n2016-01-08\n0.003067\n\n\n2016-01-11\n-0.000573\n\n\n...\n...\n\n\n2021-07-26\n-0.002140\n\n\n2021-07-27\n-0.008684\n\n\n2021-07-28\n-0.001117\n\n\n2021-07-29\n0.000978\n\n\n2021-07-30\n-0.005550\n\n\n\n\n1386 rows × 1 columns\n\n\n\nIn terms of \\(R^2\\), the Linear Regression performs better than KNN on the testing data.\n\nprint(\"LR R^2: \", linear_regression.score(X_test, y_test))\nprint(\"KNN R^2:\", knn.score(X_test, y_test))\n\nLR R^2:  0.02720569878120005\nKNN R^2: 0.008540558936330656\n\n\nOn the testing data, the models are again quite similar from an mean square error perspective.\n\nprint(\"LR MSE: \", \\\n      sklearn.metrics.mean_squared_error(y_test, linear_regression.predict(X_test)))\nprint(\"KNN MSE:\", \\\n      sklearn.metrics.mean_squared_error(y_test, knn.predict(X_test)))\n\nLR MSE:  0.0002831135176255083\nKNN MSE: 0.00028854565614835155"
  },
  {
    "objectID": "chapters/26_pca/pca.html#import-packages",
    "href": "chapters/26_pca/pca.html#import-packages",
    "title": "26  Principal Components Analysis",
    "section": "26.1 Import Packages",
    "text": "26.1 Import Packages\nLet’s begin by loading the packages that we will need.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport sklearn\nimport matplotlib.pyplot as plt\n%matplotlib inline"
  },
  {
    "objectID": "chapters/26_pca/pca.html#reading-in-data",
    "href": "chapters/26_pca/pca.html#reading-in-data",
    "title": "26  Principal Components Analysis",
    "section": "26.2 Reading-In Data",
    "text": "26.2 Reading-In Data\nNext, let’s read-in our data; this is the same data set that we used in the k-nearest-neighbors chapter.\n\ndf_vix = pd.read_csv('vix_knn.csv')\ndf_vix = df_vix[df_vix.trade_date &gt; '2011-01-03'] #removing the first row of NaNs\ndf_vix.drop(['spy_ret'], axis=1, inplace=True) # dropping the returns (label) column\ndf_vix.head()\n\n\n\n\n\n\n\n\ntrade_date\nvix_009\nvix_030\nvix_090\nvix_180\n\n\n\n\n1\n2011-01-04\n0.02\n-0.23\n-0.01\n-0.21\n\n\n2\n2011-01-05\n-0.49\n-0.36\n-0.56\n-0.41\n\n\n3\n2011-01-06\n0.14\n0.38\n0.30\n0.09\n\n\n4\n2011-01-07\n-0.70\n-0.26\n-0.06\n0.05\n\n\n5\n2011-01-10\n0.80\n0.40\n0.19\n0.01\n\n\n\n\n\n\n\nRecall that this data set is the daily changes for various points along the implied volatility term structure. We’ll also be interested in the absolute levels of the VIX term structure, so let’s import that data now.\n\ndf_vix_ts = pd.read_csv('vix_term_structure.csv')\ndf_vix_ts.drop(['spy_ret'], axis=1, inplace=True) # dropping returns column\ndf_vix_ts.head()\n\n\n\n\n\n\n\n\ntrade_date\nvix_009\nvix_030\nvix_090\nvix_180\n\n\n\n\n0\n2011-01-03\n16.04\n17.61\n20.62\n23.40\n\n\n1\n2011-01-04\n16.06\n17.38\n20.61\n23.19\n\n\n2\n2011-01-05\n15.57\n17.02\n20.05\n22.78\n\n\n3\n2011-01-06\n15.71\n17.40\n20.35\n22.87\n\n\n4\n2011-01-07\n15.01\n17.14\n20.29\n22.92"
  },
  {
    "objectID": "chapters/26_pca/pca.html#typical-term-structure",
    "href": "chapters/26_pca/pca.html#typical-term-structure",
    "title": "26  Principal Components Analysis",
    "section": "26.3 Typical Term Structure",
    "text": "26.3 Typical Term Structure\nIt is useful to think of both df_vix and df_vix_ts as living in Euclidean space. However, they mostly live in certain specific subspaces of Euclidean space, and it’s the goal of PCA to give this observation a more specific meaning.\nBefore we dive into the specifics of PCA, let’s develop an intuition for what a typical VIX term structure looks like, and what kind of daily changes are typical in the term structure.\nThe first stylized fact about the VIX term structure is that it is usually monotonically increasing. We can check this with the following code.\n\ncond1 = (df_vix_ts.vix_009 &lt;= df_vix_ts.vix_030)\ncond2 = (df_vix_ts.vix_030 &lt;= df_vix_ts.vix_090)\ncond3 = (df_vix_ts.vix_090 &lt;= df_vix_ts.vix_180)\n\ndf_vix_ts[cond1 & cond2 & cond3].shape[0] / df_vix_ts.shape[0]\n\n0.6923459244532804\n\n\nNotice that about 70% of days in our data set have an unpwardly sloping term struture.\n\nCode Challenge: Find the percentage of days in which the 9-day VIX is greater than the 180-day VIX (we’ll refer to this as an inverted VIX term structure).\n\n\nSolution\ncond = (df_vix_ts.vix_009 &gt; df_vix_ts.vix_180)\ndf_vix_ts[cond].shape[0] / df_vix_ts.shape[0]\n\n\n0.11083499005964215"
  },
  {
    "objectID": "chapters/26_pca/pca.html#typical-term-structure-changes",
    "href": "chapters/26_pca/pca.html#typical-term-structure-changes",
    "title": "26  Principal Components Analysis",
    "section": "26.4 Typical Term-Structure Changes",
    "text": "26.4 Typical Term-Structure Changes\nNext we perform a similar analysis on the daily VIX changes, which are stored in df_vix. The following code shows that on 75% of days, all the points on the VIX term structure either rise or fall together.\n\n# decreasing\ncond1 = (df_vix.vix_009 &lt;= 0) \ncond2 = (df_vix.vix_030 &lt;= 0)\ncond3 = (df_vix.vix_090 &lt;= 0)\ncond4 = (df_vix.vix_180 &lt;= 0)\n\n# increasing\ncond5 = (df_vix.vix_009 &gt;= 0) \ncond6 = (df_vix.vix_030 &gt;= 0)\ncond7 = (df_vix.vix_090 &gt;= 0)\ncond8 = (df_vix.vix_180 &gt;= 0)\n\n\ncond = (cond1 & cond2 & cond3 & cond4) | (cond5 & cond6 & cond7 & cond8)\n\n\ndf_vix[cond].shape[0] / df_vix.shape[0]\n\n0.7454002983590253\n\n\nThis is also reflected in the strong positive correlations that we can observe in the following seaborn pair plot.\n\nsns.pairplot(df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]);"
  },
  {
    "objectID": "chapters/26_pca/pca.html#summarizing-our-observations",
    "href": "chapters/26_pca/pca.html#summarizing-our-observations",
    "title": "26  Principal Components Analysis",
    "section": "26.5 Summarizing Our Observations",
    "text": "26.5 Summarizing Our Observations\nLet’s summarize our observations from above:\n\nAbout 70% of the time the VIX term structure is monotonically increasing.\nAbout 10% of the time the VIX term structure is inverted with 9-day greater than 180-day.\nOn 75% of days, all points on the term structure either rise or fall together.\n\n\nDiscussion Question: If you were to construct a forecasting model for the VIX, would your model assert that all points of the term structure move completely independently of one another?\n\n\nSolution\n# Clearly no.  Most of the time there is a parallel shift, \n# so any model should account for that."
  },
  {
    "objectID": "chapters/26_pca/pca.html#pca-overview",
    "href": "chapters/26_pca/pca.html#pca-overview",
    "title": "26  Principal Components Analysis",
    "section": "26.6 PCA Overview",
    "text": "26.6 PCA Overview\nThe purpose of this chapter is to give an intuitive introduction to PCA and how to implement it in sklearn. We are not going to get into the mathematical specifics, which involve a fair amount of linear algebra.\nInstead, here is an overview of the essential ideas:\n\nIn PCA we are trying to understand a \\(n\\) sized set of \\(p\\)-dimensional features. Each of the \\(n\\) feature observations is a vector of \\(p\\) numbers. It is usually best to scale the features so that each component has mean zero and standard deviation 1.\nWe can think of our feature set as a collection of points living in \\(p\\)-dimensional euclidean space (\\(R^{p}\\)), with the points being centered around the origin.\nIt is often the case that there is some \\(q\\) that is smaller than \\(p\\), such that most of the variation of the features can be described by a \\(q\\) dimensional linear subspace of \\({R}^p\\).\nPCA is a technique for finding the basis of this \\(q\\) dimensional subspace.\nThe first element of this basis is the vector (which defines a line that goes through the origin) that is collectively closest to all of the feature observations. This is conceptually similar to fitting a regression line. This vector is called the first principal component. It has the property that if we project our feature observation onto that vector, the sample variance of the projections is maximized.\nThe second element of this basis is called the second principal component. It is found by considering all vectors which are perpendicular to the first principle component. Among these perpendicular vectors, the second component is the one which bests fits the the feature observations. It has the property that if we project our feature observations onto that vector, then we maximize the sample variance of the projections (among all vectors that are perpendicular to the first principle component).\nWe repeat this algorithm until we arrive at \\(q\\) principle components."
  },
  {
    "objectID": "chapters/26_pca/pca.html#performing-a-pca-on-the-vix-changes",
    "href": "chapters/26_pca/pca.html#performing-a-pca-on-the-vix-changes",
    "title": "26  Principal Components Analysis",
    "section": "26.7 Performing a PCA on the VIX Changes",
    "text": "26.7 Performing a PCA on the VIX Changes\nLet’s now perform a PCA on our df_vix data set using sklearn.\nWe’ll begin by normaling our features so that they have mean zero and unity standard deviation.\n\nfrom sklearn.preprocessing import scale\n\n# feature selection\nX = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\n\n# scaling fetures\nXs = scale(X)\n\nNext, we’ll instantiate our pca model and then fit it for \\(q = 3\\) components.\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=3)\npca.fit(Xs)\n\nPCA(n_components=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA(n_components=3)\n\n\nWe can view the actual principal components by printing the .components property our model.\n\nprint(pca.components_)\n\n[[ 0.48854882  0.50863285  0.5063361   0.49622216]\n [ 0.73980156  0.15854235 -0.30757946 -0.57702067]\n [-0.40057447  0.53375678  0.4395292  -0.60121367]]\n\n\nTo see what percentage of the overall variation is explained by each of the components, we print the .explained_variance_ratio property.\n\nprint(pca.explained_variance_ratio_)\n\n[0.94602979 0.04134279 0.00840214]\n\n\nNotice that 95% the total variance is explained by the first component.\n\nCode Challenge: Try refitting the PCA with n_components=4.\n\nAre the first three components the same as before?\nWhat is the total amount of variance explained by the four componets?\nWhat happens when you set n_components=5 and try to refit?\n\n\n\nSolution\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=4)\npca.fit(Xs)\nprint(\"Components:\")\nprint(pca.components_)\nprint(\" \")\nprint(\"Variance Explaned:\")\nprint(pca.explained_variance_ratio_)\n\n\nComponents:\n[[ 0.48854882  0.50863285  0.5063361   0.49622216]\n [ 0.73980156  0.15854235 -0.30757946 -0.57702067]\n [-0.40057447  0.53375678  0.4395292  -0.60121367]\n [-0.23141694  0.65670438 -0.67515385  0.24362438]]\n \nVariance Explaned:\n[0.94602979 0.04134279 0.00840214 0.00422528]"
  },
  {
    "objectID": "chapters/26_pca/pca.html#understanding-the-first-three-principal-components-level-slope-curvature",
    "href": "chapters/26_pca/pca.html#understanding-the-first-three-principal-components-level-slope-curvature",
    "title": "26  Principal Components Analysis",
    "section": "26.8 Understanding the First Three Principal Components: Level, Slope, Curvature",
    "text": "26.8 Understanding the First Three Principal Components: Level, Slope, Curvature\nLet’s again look at the first three principal components.\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=3)\npca.fit(Xs)\nprint(\"Components:\")\nprint(pca.components_)\nprint(\" \")\nprint(\"Variance Explaned:\")\nprint(pca.explained_variance_ratio_)\n\nComponents:\n[[ 0.48854882  0.50863285  0.5063361   0.49622216]\n [ 0.73980156  0.15854235 -0.30757946 -0.57702067]\n [-0.40057447  0.53375678  0.4395292  -0.60121367]]\n \nVariance Explaned:\n[0.94602979 0.04134279 0.00840214]\n\n\nThese components have a very intuitive interpration (which is not always the case in PCA):\n\nLevel: the first component represents parallel shifts in the VIX term structure - all points increasing or all decreasing.\n\n95% of total variance explained\nthis means that all the points of the term structure are strongly positively correlated\nwe can see this in our pair-plots above, as well as the fact that 75% of days had all points going up or all points going down\n\nSlope: the second component represents the front of the curve going up, and the back of the curve going down.\n\nexplains 4% of total variance\nrecall that about 70% of days had an upward sloping term structure, and 10% of all days had a downward sloping term structure\nin order for the term structure to go from upward sloping to downward sloping, the curve would need this kind of a move (most likely in conjunction with a parallel shift)\n\nCurvature: the third component represnts changes in curvature - moving from more concave to more convex.\n\n1% of total variance"
  },
  {
    "objectID": "chapters/27_k_means_clustering/k_means_clustering.html#importing-packages",
    "href": "chapters/27_k_means_clustering/k_means_clustering.html#importing-packages",
    "title": "27  K-Means Clustering",
    "section": "27.1 Importing Packages",
    "text": "27.1 Importing Packages\nLet’s begin by importing the packages that we will need.\n\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "chapters/27_k_means_clustering/k_means_clustering.html#reading-in-data",
    "href": "chapters/27_k_means_clustering/k_means_clustering.html#reading-in-data",
    "title": "27  K-Means Clustering",
    "section": "27.2 Reading-In Data",
    "text": "27.2 Reading-In Data\nNext let’s read-in our data, which consists of performance statistics of the various underlyings over the backtest period. In particular, we will be interested in:\n\npnl - daily average pnl (multplied by 100)\nbeta - the beta to the strategy applied to SPY (not SPY itself)\nstdev - the standard deviation of the daily PNLs.\n\n\ndf_cluster = pd.read_csv(\"strategy.csv\")\ndf_cluster.head()\n\n\n\n\n\n\n\n\nunderlying\nstdev\nspy_stdev\ncorr_spy\nbeta\npnl\n\n\n\n\n0\nDIA\n0.010383\n0.014772\n0.824586\n0.579557\n0.613557\n\n\n1\nEEM\n0.025753\n0.014772\n0.550046\n0.958898\n0.457594\n\n\n2\nEFA\n0.021988\n0.014772\n0.608328\n0.905449\n-0.582501\n\n\n3\nEMB\n0.005283\n0.014772\n0.185949\n0.066495\n-0.117455\n\n\n4\nEWH\n0.037914\n0.014772\n0.134310\n0.344708\n0.036128\n\n\n\n\n\n\n\nWe will also import some additional information about the universe of underlyings which we will find useful later in our analysis.\n\ndf_universe = df_cluster[['underlying']]\ndf = pd.read_csv('universe.csv')\ndf_universe = df_universe.merge(df, how='inner', on='underlying')\ndf_universe.head()\n\n\n\n\n\n\n\n\nunderlying\nname\n\n\n\n\n0\nDIA\nSPDR DOW JONES INDL AVERAGE ET UT SER 1\n\n\n1\nEEM\nISHARES TR MSCI EMG MKT ETF\n\n\n2\nEFA\nISHARES TR MSCI EAFE ETF\n\n\n3\nEMB\nISHARES TR JPMORGAN USD EMG\n\n\n4\nEWH\nISHARES INC MSCI HONG KG ETF"
  },
  {
    "objectID": "chapters/27_k_means_clustering/k_means_clustering.html#checking-for-outliers",
    "href": "chapters/27_k_means_clustering/k_means_clustering.html#checking-for-outliers",
    "title": "27  K-Means Clustering",
    "section": "27.3 Checking for Outliers",
    "text": "27.3 Checking for Outliers\nLet’s plot our data quickly in order to check for potential outliers.\n\nwith sns.axes_style('whitegrid'):\n    g = sns.relplot(\n            x='beta'\n            , y='pnl'\n            , data=df_cluster\n            , color = 'black'\n            , alpha = 0.75\n            , height=7 \n            , aspect=1.3\n            , s=70\n        );\n    plt.subplots_adjust(top=0.93);\n    g.fig.suptitle('beta vs pnl');"
  },
  {
    "objectID": "chapters/27_k_means_clustering/k_means_clustering.html#wrangling-removing-outliers",
    "href": "chapters/27_k_means_clustering/k_means_clustering.html#wrangling-removing-outliers",
    "title": "27  K-Means Clustering",
    "section": "27.4 Wrangling: Removing Outliers",
    "text": "27.4 Wrangling: Removing Outliers\nThere seem to be a few extreme data point, which is likely the result of bad data. Let’s write a query to isolate these.\n\ndf_cluster.query(\"pnl &lt; -8 or pnl &gt; 5\")\n\n\n\n\n\n\n\n\nunderlying\nstdev\nspy_stdev\ncorr_spy\nbeta\npnl\n\n\n\n\n13\nGDX\n0.061813\n0.014772\n0.197046\n0.824502\n-8.315433\n\n\n19\nSLV\n0.048230\n0.014772\n0.221409\n0.722864\n5.975203\n\n\n23\nUNG\n0.082083\n0.014772\n0.041565\n0.230951\n-8.397742\n\n\n\n\n\n\n\nWe can see that the offenders are GDX, SLV, and UNG. Let’s remove these manually now.\n\ndf_cluster = df_cluster[~df_cluster['underlying'].isin(['UNG', 'SLV', 'GDX'])].reset_index(drop=True)\ndf_cluster.head()\n\n\n\n\n\n\n\n\nunderlying\nstdev\nspy_stdev\ncorr_spy\nbeta\npnl\n\n\n\n\n0\nDIA\n0.010383\n0.014772\n0.824586\n0.579557\n0.613557\n\n\n1\nEEM\n0.025753\n0.014772\n0.550046\n0.958898\n0.457594\n\n\n2\nEFA\n0.021988\n0.014772\n0.608328\n0.905449\n-0.582501\n\n\n3\nEMB\n0.005283\n0.014772\n0.185949\n0.066495\n-0.117455\n\n\n4\nEWH\n0.037914\n0.014772\n0.134310\n0.344708\n0.036128\n\n\n\n\n\n\n\nLet’s regraph our cleaned data.\n\nwith sns.axes_style('whitegrid'):\n    g = sns.relplot(\n            x='beta'\n            , y='pnl'\n            , data=df_cluster\n            , color = 'black'\n            , alpha = 0.75\n            , height=7 \n            , aspect=1.3\n            , s=70\n        );\n    plt.subplots_adjust(top=0.93);\n    g.fig.suptitle('beta vs pnl');\n\n\n\n\n\nDiscussion Question: Inspect the graph above and see if you can predict a K-means clustering based on four clusters.\n\n\nSolution\n#"
  },
  {
    "objectID": "chapters/27_k_means_clustering/k_means_clustering.html#k-means-clustering---first-try",
    "href": "chapters/27_k_means_clustering/k_means_clustering.html#k-means-clustering---first-try",
    "title": "27  K-Means Clustering",
    "section": "27.5 K-Means Clustering - First Try",
    "text": "27.5 K-Means Clustering - First Try\nWe are now ready to fit our clustering model. We begin by importing the KMeans constructor.\n\nfrom sklearn.cluster import KMeans\n\nNext, we will isolate the features we want to use for grouping.\n\nX = df_cluster[['beta', 'pnl']]\nX.head()\n\n\n\n\n\n\n\n\nbeta\npnl\n\n\n\n\n0\n0.579557\n0.613557\n\n\n1\n0.958898\n0.457594\n\n\n2\n0.905449\n-0.582501\n\n\n3\n0.066495\n-0.117455\n\n\n4\n0.344708\n0.036128\n\n\n\n\n\n\n\nWe are now ready to fit our clustering model. I chose to use 4 groups in the hope that the groupings would align with the following categories:\n\nhigh beta, high pnl\nhigh beta, low pnl\nlow beta, high pnl\nlow beta, low pnl\n\n\nkmeans = KMeans(n_clusters = 4, n_init=100, random_state=0)\nkmeans.fit(X)\n\nKMeans(n_clusters=4, n_init=100, random_state=0)\n\n\nLet’s add a column to our original data frame that includes the group number.\n\ndf_cluster['group'] = kmeans.labels_\ndf_cluster['group'] = df_cluster['group'].apply(str)\ndf_cluster.sort_values('group').head()\n\n\n\n\n\n\n\n\nunderlying\nstdev\nspy_stdev\ncorr_spy\nbeta\npnl\ngroup\n\n\n\n\n0\nDIA\n0.010383\n0.014772\n0.824586\n0.579557\n0.613557\n0\n\n\n31\nXLU\n0.011834\n0.014772\n0.236587\n0.189522\n-0.695763\n0\n\n\n30\nXLP\n0.009352\n0.014772\n0.408279\n0.258476\n-0.653215\n0\n\n\n28\nXLI\n0.018132\n0.014772\n0.605048\n0.742666\n-0.576125\n0\n\n\n27\nXLF\n0.031330\n0.014772\n0.444355\n0.942399\n-0.571652\n0\n\n\n\n\n\n\n\nAnd now let’s graph our clustering with colors to show the grouping.\n\nwith sns.axes_style('whitegrid'):\n    sns.color_palette(\"Paired\")\n    g = sns.relplot(\n            x='beta'\n            , y='pnl'\n            , data=df_cluster\n            , height=7 \n            , aspect=1.3\n            , hue='group'\n            , palette=[\"b\", \"r\", 'k', 'y']\n            , s=100\n        );\n    plt.subplots_adjust(top=0.93);\n    g.fig.suptitle('beta vs pnl');\n\n\n\n\n\nDiscussion Question: What are your observations about this clustering?\n\n\nSolution\n# The PNL feature is dominating the clustering because of scale."
  },
  {
    "objectID": "chapters/27_k_means_clustering/k_means_clustering.html#k-means-clustering---second-try",
    "href": "chapters/27_k_means_clustering/k_means_clustering.html#k-means-clustering---second-try",
    "title": "27  K-Means Clustering",
    "section": "27.6 K-Means Clustering - Second Try",
    "text": "27.6 K-Means Clustering - Second Try\nAs we can see from the previous section, unless we perform scaling, the pnl feature will dominate because its values are an order of magnitude bigger than the beta feature.\nSo let’s scale and perform our analysis again.\n\nfrom sklearn.preprocessing import scale\nXs = scale(X)\nkmeans = KMeans(n_clusters = 4, n_init=100, random_state=0)\nkmeans.fit(Xs)\n\nKMeans(n_clusters=4, n_init=100, random_state=0)\n\n\n\ndf_cluster['group'] = kmeans.labels_\ndf_cluster['group'] = df_cluster['group'].apply(str)\ndf_cluster = df_cluster.sort_values(['group', 'underlying']).merge(df_universe)\ndf_cluster.head()\n\n\n\n\n\n\n\n\nunderlying\nstdev\nspy_stdev\ncorr_spy\nbeta\npnl\ngroup\nname\n\n\n\n\n0\nEEM\n0.025753\n0.014772\n0.550046\n0.958898\n0.457594\n0\nISHARES TR MSCI EMG MKT ETF\n\n\n1\nEFA\n0.021988\n0.014772\n0.608328\n0.905449\n-0.582501\n0\nISHARES TR MSCI EAFE ETF\n\n\n2\nEWJ\n0.025301\n0.014772\n0.343123\n0.587681\n-2.597758\n0\nISHARES INC MSCI JPN ETF NEW\n\n\n3\nEWU\n0.050743\n0.014772\n0.268563\n0.922505\n-1.406626\n0\nISHARES TR MSCI UK ETF NEW\n\n\n4\nEWW\n0.026251\n0.014772\n0.372520\n0.661983\n-0.608147\n0\nISHARES INC MSCI MEXICO ETF\n\n\n\n\n\n\n\nAnd let’s graph our new grouping.\n\nwith sns.axes_style('whitegrid'):\n    sns.color_palette(\"Paired\")\n    g = sns.relplot(\n            x='beta'\n            , y='pnl'\n            , data=df_cluster\n            , height=7 \n            , aspect=1.3\n            , hue='group'\n            , palette=[\"b\", \"r\", 'k', 'y']\n            , s=100\n        );\n    plt.subplots_adjust(top=0.93);\n    g.fig.suptitle('beta vs pnl');\n\n\n\n\n\nDiscussion Question: What are your observations about this grouping?\n\n\nSolution\n# group 0: high beta, zeroish pnl\n# group 1: low beta, zeroish pnl\n# group 2: extremely low performers\n# group 3: high beta, high performers\n\n# group 3 consists of: QQQ, SPY, IWM, USO, EWZ, FXI -- three of these are major stock indices, which is what I would expect\n# group 0 contains 7 of the 11 SPDR sector select funds (X--), which makes sense based on what I know of the strategy\n# group 0 also contains 5 emerging market related funds (E--), I don't have an opinion on this \n\n\n\nCode Challenge: Redo the clustering for 3 groups.\n\n\nSolution\n# fitting\nX = df_cluster[['beta', 'pnl']]\nXs = scale(X)\nkmeans = KMeans(n_clusters = 3, n_init=100, random_state=0)\nkmeans.fit(Xs)\n# adding labels to df_cluster\ndf_cluster['group'] = kmeans.labels_\ndf_cluster['group'] = df_cluster['group'].apply(str)\ndf_cluster.sort_values('group')\n# graphing\nwith sns.axes_style('whitegrid'):\n    sns.color_palette(\"Paired\")\n    g = sns.relplot(\n            x='beta'\n            , y='pnl'\n            , data=df_cluster\n            , height=7 \n            , aspect=1.3\n            , hue='group'\n            , palette=[\"b\", \"r\", 'k']\n            , s=100\n        );\n    plt.subplots_adjust(top=0.93);\n    g.fig.suptitle('beta vs pnl');"
  },
  {
    "objectID": "chapters/28_pca_visualization/pca_visualization.html#importing-packages",
    "href": "chapters/28_pca_visualization/pca_visualization.html#importing-packages",
    "title": "28  Visualizing Principal Components Analysis",
    "section": "28.1 Importing Packages",
    "text": "28.1 Importing Packages\nLet’s begin by importing the packages that we will need.\n\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "chapters/28_pca_visualization/pca_visualization.html#reading-in-data",
    "href": "chapters/28_pca_visualization/pca_visualization.html#reading-in-data",
    "title": "28  Visualizing Principal Components Analysis",
    "section": "28.2 Reading-In Data",
    "text": "28.2 Reading-In Data\nNext, let’s read-in the data that we will analyze. This data consists of weekly PNLs of an options trading strategy that is applied to various underlyings.\n\ndf_strategy = pd.read_csv('strategy_pca.csv')\ndf_strategy\n\n\n\n\n\n\n\n\ntrade_date\nDIA\nEEM\nEFA\nEWJ\nEWW\nEWZ\nFXE\nFXI\nFXY\n...\nXLF\nXLI\nXLK\nXLP\nXLU\nXLV\nXLY\nXME\nXOP\nXRT\n\n\n\n\n0\n2010-06-11\n0.007174\n-0.017362\n-0.010725\n-0.026382\n-0.025379\n-0.013350\n-0.002375\n-0.009553\n-0.003428\n...\n-0.023686\n-0.024645\n-0.019757\n-0.012389\n-0.008406\n-0.012576\n-0.020438\n-0.032221\n-0.051404\n-0.019148\n\n\n1\n2010-06-14\n0.010173\n0.035633\n0.014950\n0.014282\n0.033116\n0.033594\n-0.000735\n0.019362\n0.005734\n...\n0.003849\n0.044187\n0.049763\n0.015300\n0.011585\n0.019370\n0.040846\n0.060549\n0.047449\n0.030016\n\n\n2\n2010-06-15\n-0.002297\n-0.013010\n-0.019534\n-0.005922\n0.000404\n-0.004744\n-0.001553\n-0.000144\n0.002334\n...\n-0.000972\n-0.010887\n-0.027390\n0.010241\n0.006655\n0.014511\n-0.014392\n0.037345\n0.016062\n0.032434\n\n\n3\n2010-06-16\n0.010865\n0.023527\n0.020398\n0.009137\n0.016342\n0.032344\n0.003108\n0.027393\n0.004025\n...\n0.024156\n0.021204\n0.016975\n-0.000245\n-0.002232\n0.006271\n0.030290\n0.024339\n0.040401\n-0.004622\n\n\n4\n2010-06-17\n0.007654\n0.021416\n0.013009\n0.019784\n0.012869\n0.020282\n0.002741\n0.005559\n0.003677\n...\n0.029602\n-0.001968\n-0.006714\n0.003149\n0.011234\n-0.000091\n0.004604\n0.027596\n0.021323\n0.030434\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n749\n2018-12-21\n-0.010358\n-0.008775\n-0.025460\n-0.058596\n-0.009828\n0.004381\n-0.002566\n0.011680\n-0.001237\n...\n-0.052536\n-0.026648\n-0.070161\n-0.017729\n0.102341\n-0.017573\n-0.008129\n-0.101895\n0.012906\n-0.089217\n\n\n750\n2018-12-24\n-0.037064\n0.004238\n-0.017792\n-0.007173\n0.021813\n0.007675\n-0.000743\n0.002134\n-0.006349\n...\n0.000666\n-0.027368\n-0.101256\n-0.080759\n-0.141095\n-0.043487\n-0.168685\n0.164792\n-0.190494\n-0.006617\n\n\n751\n2018-12-26\n-0.074702\n0.023863\n0.015465\n0.014819\n0.047261\n0.035732\n0.000935\n0.021011\n0.001450\n...\n-0.033270\n-0.105362\n-0.053457\n-0.002229\n0.016203\n-0.033066\n-0.079741\n-0.138356\n-0.422292\n-0.074324\n\n\n752\n2018-12-27\n0.015387\n0.016627\n0.013537\n0.013345\n0.028583\n0.016973\n-0.002762\n0.007095\n0.001280\n...\n0.025021\n0.013005\n0.033152\n0.012723\n0.011532\n0.012957\n0.020950\n0.073538\n0.067077\n0.016728\n\n\n753\n2018-12-28\n0.025541\n0.011025\n0.007857\n0.014097\n0.020565\n0.020779\n0.001501\n0.020864\n-0.003356\n...\n0.024544\n0.033024\n0.025723\n0.009940\n0.020296\n0.030015\n0.030467\n0.024561\n0.059169\n0.031644\n\n\n\n\n754 rows × 35 columns"
  },
  {
    "objectID": "chapters/28_pca_visualization/pca_visualization.html#fitting-pca",
    "href": "chapters/28_pca_visualization/pca_visualization.html#fitting-pca",
    "title": "28  Visualizing Principal Components Analysis",
    "section": "28.3 Fitting PCA",
    "text": "28.3 Fitting PCA\nWe are now ready to fit a PCA to this PNL data; let’s start by scaling.\n\nfrom sklearn.preprocessing import scale\nXs = scale(df_strategy.loc[:, df_strategy.columns != 'trade_date'])\nXs\n\narray([[ 0.55349196, -0.7868123 , -0.40985713, ..., -0.53168657,\n        -1.11237419, -1.01039618],\n       [ 0.77882186,  1.61951344,  0.71837458, ...,  1.2393787 ,\n         1.01447345,  1.72395022],\n       [-0.15822341, -0.58920351, -0.796928  , ...,  0.79640469,\n         0.33917233,  1.85846243],\n       ...,\n       [-5.59881083,  1.08506556,  0.74096939, ..., -2.55791817,\n        -9.09217992, -4.07915548],\n       [ 1.17060449,  0.75653352,  0.65627559, ...,  1.48735182,\n         1.43678679,  0.98492788],\n       [ 1.93360316,  0.50215785,  0.40666156, ...,  0.55234503,\n         1.26664176,  1.81448399]])\n\n\nNext we’ll fit the first five principal components.\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=5)\npca.fit(Xs)\n\nPCA(n_components=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA(n_components=5)\n\n\n\nCode Challenge: What is the percent of variance explained of the first principal component?\n\n\nSolution\npca.explained_variance_ratio_\n\n\narray([0.40549205, 0.07136019, 0.04928076, 0.04223706, 0.03744758])"
  },
  {
    "objectID": "chapters/28_pca_visualization/pca_visualization.html#visualizing-loading-vectors",
    "href": "chapters/28_pca_visualization/pca_visualization.html#visualizing-loading-vectors",
    "title": "28  Visualizing Principal Components Analysis",
    "section": "28.4 Visualizing Loading Vectors",
    "text": "28.4 Visualizing Loading Vectors\nNext, we will visualize the loading vectors. In order to do this, let’s begin by creating a DataFrame to hold them.\n\ndf_components = \\\n    pd.DataFrame({\n        'underlying':df_strategy.drop(columns=['trade_date']).columns,\n        'PCA1':-pca.components_[0],\n        'PCA2':-pca.components_[1],\n        'PCA3':-pca.components_[2],\n        'PCA4':-pca.components_[3],\n        'PCA5':-pca.components_[4],\n    })\ndf_components.head()\n\n\n\n\n\n\n\n\nunderlying\nPCA1\nPCA2\nPCA3\nPCA4\nPCA5\n\n\n\n\n0\nDIA\n0.239596\n-0.169226\n0.090560\n0.015176\n0.005542\n\n\n1\nEEM\n0.222856\n0.125537\n-0.084749\n0.123141\n-0.089064\n\n\n2\nEFA\n0.204699\n0.084143\n-0.030489\n0.260802\n-0.137933\n\n\n3\nEWJ\n0.141398\n0.016348\n-0.005798\n0.202220\n0.008194\n\n\n4\nEWW\n0.146514\n0.136649\n-0.138207\n0.125954\n-0.217323\n\n\n\n\n\n\n\nLet’s also add in the grouping from the cluster analysis we performed in the previous tutorial. We being by reading in a CSV that contains the grouping.\n\ndf_cluster = pd.read_csv('cluster_analysis.csv')\ndf_cluster.head()\n\n\n\n\n\n\n\n\nunderlying\nstdev\nspy_stdev\ncorr_spy\nbeta\npnl\ngroup\nname\n\n\n\n\n0\nEEM\n0.025753\n0.014772\n0.550046\n0.958898\n0.457594\n0\nISHARES TR MSCI EMG MKT ETF\n\n\n1\nEWZ\n0.045733\n0.014772\n0.388278\n1.202046\n4.057360\n0\nISHARES INC MSCI BRAZIL ETF\n\n\n2\nFXI\n0.030682\n0.014772\n0.433179\n0.899703\n1.700224\n0\nISHARES TR CHINA LG-CAP ETF\n\n\n3\nIWM\n0.019681\n0.014772\n0.756170\n1.007407\n2.827085\n0\nISHARES TR RUSSELL 2000 ETF\n\n\n4\nQQQ\n0.016196\n0.014772\n0.737255\n0.808304\n1.240073\n0\nINVESCO QQQ TR UNIT SER 1\n\n\n\n\n\n\n\nNow we can join in the groupings to df_components.\n\ndf_components = \\\n    (\n    df_components\n        .merge(df_cluster[['underlying','group']], how='inner', on='underlying')\n    )\ndf_components.head()\n\n\n\n\n\n\n\n\nunderlying\nPCA1\nPCA2\nPCA3\nPCA4\nPCA5\ngroup\n\n\n\n\n0\nDIA\n0.239596\n-0.169226\n0.090560\n0.015176\n0.005542\n1\n\n\n1\nEEM\n0.222856\n0.125537\n-0.084749\n0.123141\n-0.089064\n0\n\n\n2\nEFA\n0.204699\n0.084143\n-0.030489\n0.260802\n-0.137933\n2\n\n\n3\nEWJ\n0.141398\n0.016348\n-0.005798\n0.202220\n0.008194\n2\n\n\n4\nEWW\n0.146514\n0.136649\n-0.138207\n0.125954\n-0.217323\n2\n\n\n\n\n\n\n\nAnd finally we can use seaborn to visualize the first component.\n\nwith sns.axes_style('darkgrid'):\n    g = sns.catplot(\n        x='underlying',\n        y='PCA1',\n        kind='bar',\n        color='black',\n        alpha=0.75,\n        height=5,\n        aspect = 3,\n        data=df_components.sort_values(['group', 'underlying']),\n        hue='group',\n        palette=[\"b\", \"r\", 'k', 'y'],\n        dodge=False,\n    );\n    plt.subplots_adjust(top=0.93);\n    g.fig.suptitle('First Principal Loading Vector');\n\n\n\n\n\nDiscussion Question: Give an interpretation of the first principal component.\n\n\nSolution\n# all underlyings tend to make/lose money together.\n\n\n\nCode Challenge: Graph the other principal components. Do you see any interesting patterns?\n\n\nSolution\n# just change the graph above to point to the different components by altering the y arguement.\nwith sns.axes_style('darkgrid'):\n    g = sns.catplot(\n        x='underlying',\n        y='PCA2',\n        kind='bar',\n        color='black',\n        alpha=0.75,\n        height=5,\n        aspect = 3,\n        data=df_components.sort_values(['group', 'underlying']),\n        hue='group',\n        palette=[\"b\", \"r\", 'k', 'y'],\n        dodge=False,\n    );\n    plt.subplots_adjust(top=0.93);\n    g.fig.suptitle('Second Principal Loading Vector');"
  },
  {
    "objectID": "chapters/28_pca_visualization/pca_visualization.html#visualizing-scatter-plots-of-the-scores",
    "href": "chapters/28_pca_visualization/pca_visualization.html#visualizing-scatter-plots-of-the-scores",
    "title": "28  Visualizing Principal Components Analysis",
    "section": "28.5 Visualizing Scatter Plots of the Scores",
    "text": "28.5 Visualizing Scatter Plots of the Scores\nLet’s now graph the scores of the principal components. We can access these with the .transform attribute.\n\ntransform = pca.transform(Xs)\ntransform\n\narray([[ 3.89770162,  1.05257476, -0.89223461, -0.35589021, -0.35466412],\n       [-6.57546892, -0.40130749, -0.62645217,  1.17417457, -0.0447888 ],\n       [-1.24527477, -0.22133038, -0.47610851,  1.01747872, -0.95672622],\n       ...,\n       [11.11022384, -8.76996808, -2.62431904, -8.75691498,  2.29959156],\n       [-4.13229712,  0.8258197 ,  0.22816105,  0.82739011, -0.21214192],\n       [-6.48726319,  1.57657387,  0.08028833,  1.82019844, -0.81119418]])\n\n\nFinally, we construct a DataFrame with the scores and create a seaborn pair-plot.\n\ndf = \\\n    pd.DataFrame({\n        'PC1':transform[:,0],\n        'PC2':transform[:,1],\n        'PC3':transform[:,2],\n        'PC4':transform[:,3],\n    })\n\nsns.pairplot(df);\n\n\n\n\n\nDiscussion Question: Are you able to extract any insights from these plots?\n\n\nSolution\n# I can see that the variance of the first principal scores is much higher than the others."
  },
  {
    "objectID": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#loading-packages",
    "href": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#loading-packages",
    "title": "29  Student Loan: Overfitting",
    "section": "29.1 Loading Packages",
    "text": "29.1 Loading Packages\nLet’s begin by loading the packages that we will need.\n\nimport pandas as pd\nimport numpy as np\nimport sklearn\npd.options.display.max_rows = 10"
  },
  {
    "objectID": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#reading-in-data",
    "href": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#reading-in-data",
    "title": "29  Student Loan: Overfitting",
    "section": "29.2 Reading-In Data",
    "text": "29.2 Reading-In Data\nNext, let’s read-in our data set.\n\ndf_train = pd.read_csv('../data/student_loan.csv')\ndf_train\n\n\n\n\n\n\n\n\nload_id\ndeal_name\nloan_age\ncosign\nincome_annual\nupb\nmonthly_payment\nfico\norigbalance\nmos_to_repay\nrepay_status\nmos_to_balln\npaid_label\n\n\n\n\n0\n765579\n2014_b\n56\n0\n113401.60\n36011.11\n397.91\n814\n51453.60\n0\n0\n124\n0\n\n\n1\n765580\n2014_b\n56\n1\n100742.34\n101683.38\n1172.10\n711\n130271.33\n0\n0\n124\n0\n\n\n2\n765581\n2014_b\n56\n0\n46000.24\n49249.37\n593.57\n772\n62918.96\n0\n0\n124\n0\n\n\n3\n765582\n2014_b\n56\n0\n428958.96\n36554.85\n404.63\n849\n48238.73\n0\n0\n125\n0\n\n\n4\n765583\n2014_b\n56\n0\n491649.96\n7022.30\n1967.46\n815\n106124.68\n0\n0\n4\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1043306\n1808885\n2019_c\n2\n0\n152885.00\n115363.12\n1212.22\n798\n116834.64\n0\n0\n118\n0\n\n\n1043307\n1808886\n2019_c\n2\n0\n116480.00\n77500.70\n831.13\n826\n79566.03\n0\n0\n118\n0\n\n\n1043308\n1808887\n2019_c\n2\n0\n96800.00\n16156.76\n232.34\n781\n16472.50\n0\n0\n82\n0\n\n\n1043309\n1808888\n2019_c\n2\n0\n78400.14\n77197.03\n833.57\n777\n78135.54\n0\n0\n118\n0\n\n\n1043310\n1808889\n2019_c\n65\n0\n50447.28\n65667.85\n767.10\n765\n82602.38\n0\n0\n119\n0\n\n\n\n\n1043311 rows × 13 columns\n\n\n\nWe can inspect the columns of our data set with the DataFrame.info() method.\n\ndf_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1043311 entries, 0 to 1043310\nData columns (total 13 columns):\n #   Column           Non-Null Count    Dtype  \n---  ------           --------------    -----  \n 0   load_id          1043311 non-null  int64  \n 1   deal_name        1043311 non-null  object \n 2   loan_age         1043311 non-null  int64  \n 3   cosign           1043311 non-null  int64  \n 4   income_annual    1043311 non-null  float64\n 5   upb              1043311 non-null  float64\n 6   monthly_payment  1043311 non-null  float64\n 7   fico             1043311 non-null  int64  \n 8   origbalance      1043311 non-null  float64\n 9   mos_to_repay     1043311 non-null  int64  \n 10  repay_status     1043311 non-null  int64  \n 11  mos_to_balln     1043311 non-null  int64  \n 12  paid_label       1043311 non-null  int64  \ndtypes: float64(4), int64(8), object(1)\nmemory usage: 103.5+ MB"
  },
  {
    "objectID": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#organizing-features-and-labels",
    "href": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#organizing-features-and-labels",
    "title": "29  Student Loan: Overfitting",
    "section": "29.3 Organizing Features and Labels",
    "text": "29.3 Organizing Features and Labels\nNow that we have our data in memory, we can separate the features and labels in preparation for model fitting. We begin with the features.\n\nlst_features = \\\n    ['loan_age','cosign','income_annual', 'upb',              \n    'monthly_payment','fico','origbalance',\n    'mos_to_repay','repay_status','mos_to_balln',]    \ndf_X = df_train[lst_features]\ndf_X\n\n\n\n\n\n\n\n\nloan_age\ncosign\nincome_annual\nupb\nmonthly_payment\nfico\norigbalance\nmos_to_repay\nrepay_status\nmos_to_balln\n\n\n\n\n0\n56\n0\n113401.60\n36011.11\n397.91\n814\n51453.60\n0\n0\n124\n\n\n1\n56\n1\n100742.34\n101683.38\n1172.10\n711\n130271.33\n0\n0\n124\n\n\n2\n56\n0\n46000.24\n49249.37\n593.57\n772\n62918.96\n0\n0\n124\n\n\n3\n56\n0\n428958.96\n36554.85\n404.63\n849\n48238.73\n0\n0\n125\n\n\n4\n56\n0\n491649.96\n7022.30\n1967.46\n815\n106124.68\n0\n0\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1043306\n2\n0\n152885.00\n115363.12\n1212.22\n798\n116834.64\n0\n0\n118\n\n\n1043307\n2\n0\n116480.00\n77500.70\n831.13\n826\n79566.03\n0\n0\n118\n\n\n1043308\n2\n0\n96800.00\n16156.76\n232.34\n781\n16472.50\n0\n0\n82\n\n\n1043309\n2\n0\n78400.14\n77197.03\n833.57\n777\n78135.54\n0\n0\n118\n\n\n1043310\n65\n0\n50447.28\n65667.85\n767.10\n765\n82602.38\n0\n0\n119\n\n\n\n\n1043311 rows × 10 columns\n\n\n\nAnd next we do the same for the labels. Note that in our encoding a 1 stands for prepayment, while a 0 stands for non-prepayment.\n\ndf_y = df_train['paid_label']\ndf_y\n\n0          0\n1          0\n2          0\n3          0\n4          0\n          ..\n1043306    0\n1043307    0\n1043308    0\n1043309    0\n1043310    0\nName: paid_label, Length: 1043311, dtype: int64"
  },
  {
    "objectID": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#logistic-regression",
    "href": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#logistic-regression",
    "title": "29  Student Loan: Overfitting",
    "section": "29.4 Logistic Regression",
    "text": "29.4 Logistic Regression\nThe first classification model that we fit is called logistic regression. The name is a poor choice of words because despite being called a regression, it is actually used for classification. Although logistic regression can be used to predict a label with more than two outcomes, it is most effective when used to predict binary outcomes.\nAs with any modeling task, we begin by importing the constructor function for our model.\n\nfrom sklearn.linear_model import LogisticRegression\n\nNext, we instantiate our model.\n\nmdl_logit = LogisticRegression(random_state = 0)\n\nNow we can go ahead and fit our model, which will take a few seconds.\n\nmdl_logit.fit(df_X, df_y)\n\nLogisticRegression(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=0)\n\n\nSo how well does our model predict the training data? The standard metric for determining goodness of fit in a classification setting is accuracy, which is simply the the ratio of correct predictions to total predictions. This is the default metric that is used by the .score() method of classification models.\n\nmdl_logit.score(df_X, df_y)\n\n0.9835887860858363\n\n\n\nDiscussion Question: This accuracy looks great. So is our work done? Why might this accuracy be misleading?\n\n\nSolution\n##&gt; The data set is highly imbalanced, meaning there are very few \n##&gt; prepayments. Even a degenerate model that always predicts \n##&gt; non-prepayment would have a high accuracy.\n\n\n\nCode Challenge: Calculate the probability of prepayment in our data set.\n\n\nSolution\ndf_y.mean()\n\n\n0.01621472408514815\n\n\n\nAs we can see from the code challenge, our student loan data is highly imbalanced, meaning there are far more loans that don’t prepay than those that do prepay. Predicting rare outcomes via classification can be challenging. We will address the imbalance issue in future chapters.\nIt is often useful consider other model performance metrics when performing classification. In order to invoke these methods, we will need to be able to grab the predictions from our model as follows.\n\nmdl_logit.predict(df_X)\n\narray([0, 0, 0, ..., 0, 0, 0])\n\n\n\nCode Challenge: Calculate the probability of prepayment as predicted by our logistic regression model.\n\n\nSolution\nmdl_logit.predict(df_X).mean()\n\n\n0.0005837185652216837\n\n\n\nAn alternative goodness of fit metric is called precision, which is the percentage of prepayment predictions that were correct. The code below demonstrates that 33% of the prediction prepayments were correct\n\nsklearn.metrics.precision_score(df_y, mdl_logit.predict(df_X))\n\n0.33169129720853857\n\n\nAnother metric that we will consider is recall, which is the percentage of actual prepayments that were predicted correctly. The code below demonstrates that 1% of the prepayments were identified correctly.\n\nsklearn.metrics.recall_score(df_y, mdl_logit.predict(df_X))\n\n0.01194065141573565\n\n\nWhen performing classification, we strive for a model that has both high precision and high recall. Thus, it makes sense to combine these two metrics into a single metric. The standard combined metric that is used is called F1 and is defined as follows:\nF1 = 2 * (precision * recall) / (precision + recall)\nThe following code calculates F1.\n\nprecision = sklearn.metrics.precision_score(df_y, mdl_logit.predict(df_X))\nrecall = sklearn.metrics.recall_score(df_y, mdl_logit.predict(df_X))\n\n2 * (precision * recall) / (precision + recall)\n\n0.023051466392787857\n\n\nThe sklearn has F1 built into the the metrics module.\n\nsklearn.metrics.f1_score(df_y, mdl_logit.predict(df_X))\n\n0.023051466392787857"
  },
  {
    "objectID": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#decision-tree",
    "href": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#decision-tree",
    "title": "29  Student Loan: Overfitting",
    "section": "29.5 Decision Tree",
    "text": "29.5 Decision Tree\nThe next model we are going to fit to our student loan data is a decision tree classifier. As with any model, our steps are as follows:\n\nimport the constructor\ninstantiate the model\nfit model to data\n\nLet’s do all three steps in the following code cell.\n\nfrom sklearn.tree import DecisionTreeClassifier\nmdl_tree = DecisionTreeClassifier(random_state = 0)\nmdl_tree.fit(df_X, df_y)\n\nDecisionTreeClassifier(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(random_state=0)\n\n\n\nCode Challenge: Calculate the accuracy and F1 for our fitted decision tree model. Can we pat ourselves on the back and call it quits?\n\n\nSolution\nprint(mdl_tree.score(df_X, df_y))\nprint(sklearn.metrics.f1_score(df_y,mdl_tree.predict(df_X)))\n\n\n1.0\n1.0\n\n\n\nDecision trees often overfit the data, which is what we are observing in code challenge above. Thus, while mdl_tree looks great with the training data, it won’t look nearly so good in the wild. One way to get a sense for this is to use a holdout set, which can conveniently do with the train_test_split() function in sklearn.\n\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(df_X, df_y, random_state = 0)\n\nLet’s instantiate a new decision tree model and fit it to only X_train and y_train.\n\nmdl_holdout = DecisionTreeClassifier(random_state = 0)\nmdl_holdout.fit(X_train, y_train)\n\nDecisionTreeClassifier(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(random_state=0)\n\n\nAnd let’s see how our hold out model performs on the test data X_test and y_test.\n\nsklearn.metrics.f1_score(y_test, mdl_holdout.predict(X_test))\n\n0.36954662104362707\n\n\nOne of the byproducts of fitting a decision tree is that it assigns an importance to the features. This can be accessed with the feature_importances_ attribute.\n\nmdl_tree.feature_importances_\n\narray([9.34238470e-02, 6.94084895e-04, 8.93879170e-02, 2.51588832e-01,\n       1.03062456e-01, 6.62052798e-02, 9.42629043e-02, 5.99678238e-04,\n       2.34529524e-04, 3.00540471e-01])\n\n\nLet’s make this output more readable by putting it in a DataFrame.\n\ndct_cols = {'feature':df_X.columns.values, 'importance':mdl_tree.feature_importances_}\npd.DataFrame(dct_cols).sort_values('importance', ascending = False)\n\n\n\n\n\n\n\n\nfeature\nimportance\n\n\n\n\n9\nmos_to_balln\n0.300540\n\n\n3\nupb\n0.251589\n\n\n4\nmonthly_payment\n0.103062\n\n\n6\norigbalance\n0.094263\n\n\n0\nloan_age\n0.093424\n\n\n2\nincome_annual\n0.089388\n\n\n5\nfico\n0.066205\n\n\n1\ncosign\n0.000694\n\n\n7\nmos_to_repay\n0.000600\n\n\n8\nrepay_status\n0.000235"
  },
  {
    "objectID": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#random-forest",
    "href": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#random-forest",
    "title": "29  Student Loan: Overfitting",
    "section": "29.6 Random Forest",
    "text": "29.6 Random Forest\nThe final classifier we will consider is a random forest. A random forest is gotten by fitting several decision trees to random subsets of the features, and then averaging the results. Random forest are ensemble methods, meaning they aggregate the results of a number of submodels.\nAs usual, we begin by instantiating and fitting the model. The n_estimators input controls the number of sub-decision-trees that will be aggregated.\n\nfrom sklearn.ensemble import RandomForestClassifier\nmdl_forest = RandomForestClassifier(n_estimators = 10, random_state = 0)\nmdl_forest.fit(df_X, df_y)\n\nRandomForestClassifier(n_estimators=10, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(n_estimators=10, random_state=0)\n\n\nLet’s take a look at the in-sample F1 score.\n\nsklearn.metrics.f1_score(df_y, mdl_forest.predict(df_X))\n\n0.9082128714465085\n\n\nNext, let’s fit our model to the holdout training set that we defined above.\n\nmdl_holdout_forest = RandomForestClassifier(n_estimators = 10, random_state = 0)\nmdl_holdout_forest.fit(X_train, y_train)\n\nRandomForestClassifier(n_estimators=10, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(n_estimators=10, random_state=0)\n\n\nFinally, let’s check the F1 score on our holdout test set.\n\nsklearn.metrics.f1_score(y_test, mdl_holdout_forest.predict(X_test))\n\n0.493071000855432"
  },
  {
    "objectID": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#further-reading",
    "href": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#further-reading",
    "title": "29  Student Loan: Overfitting",
    "section": "29.7 Further Reading",
    "text": "29.7 Further Reading\nSklearn User Guides\nhttps://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees\nhttps://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\nhttps://scikit-learn.org/stable/modules/tree.html\nSklearn API Documentation\nhttps://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nhttps://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\nhttps://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#importing-packages",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#importing-packages",
    "title": "30  Student Loan: Alternative Metric & Cross-Validation",
    "section": "30.1 Importing Packages",
    "text": "30.1 Importing Packages\nLet’s begin by importing the packages that we will need.\n\nimport pandas as pd\nimport numpy as np\nimport sklearn\npd.options.display.max_rows = 10"
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#reading-in-data",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#reading-in-data",
    "title": "30  Student Loan: Alternative Metric & Cross-Validation",
    "section": "30.2 Reading-In Data",
    "text": "30.2 Reading-In Data\nNext, let’s read-in our data set.\n\ndf_train = pd.read_csv('../data/student_loan.csv')\ndf_train\n\n\n\n\n\n\n\n\nload_id\ndeal_name\nloan_age\ncosign\nincome_annual\nupb\nmonthly_payment\nfico\norigbalance\nmos_to_repay\nrepay_status\nmos_to_balln\npaid_label\n\n\n\n\n0\n765579\n2014_b\n56\n0\n113401.60\n36011.11\n397.91\n814\n51453.60\n0\n0\n124\n0\n\n\n1\n765580\n2014_b\n56\n1\n100742.34\n101683.38\n1172.10\n711\n130271.33\n0\n0\n124\n0\n\n\n2\n765581\n2014_b\n56\n0\n46000.24\n49249.37\n593.57\n772\n62918.96\n0\n0\n124\n0\n\n\n3\n765582\n2014_b\n56\n0\n428958.96\n36554.85\n404.63\n849\n48238.73\n0\n0\n125\n0\n\n\n4\n765583\n2014_b\n56\n0\n491649.96\n7022.30\n1967.46\n815\n106124.68\n0\n0\n4\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1043306\n1808885\n2019_c\n2\n0\n152885.00\n115363.12\n1212.22\n798\n116834.64\n0\n0\n118\n0\n\n\n1043307\n1808886\n2019_c\n2\n0\n116480.00\n77500.70\n831.13\n826\n79566.03\n0\n0\n118\n0\n\n\n1043308\n1808887\n2019_c\n2\n0\n96800.00\n16156.76\n232.34\n781\n16472.50\n0\n0\n82\n0\n\n\n1043309\n1808888\n2019_c\n2\n0\n78400.14\n77197.03\n833.57\n777\n78135.54\n0\n0\n118\n0\n\n\n1043310\n1808889\n2019_c\n65\n0\n50447.28\n65667.85\n767.10\n765\n82602.38\n0\n0\n119\n0\n\n\n\n\n1043311 rows × 13 columns\n\n\n\nWe can inspect the columns of our data set with the DataFrame.info() method.\n\ndf_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1043311 entries, 0 to 1043310\nData columns (total 13 columns):\n #   Column           Non-Null Count    Dtype  \n---  ------           --------------    -----  \n 0   load_id          1043311 non-null  int64  \n 1   deal_name        1043311 non-null  object \n 2   loan_age         1043311 non-null  int64  \n 3   cosign           1043311 non-null  int64  \n 4   income_annual    1043311 non-null  float64\n 5   upb              1043311 non-null  float64\n 6   monthly_payment  1043311 non-null  float64\n 7   fico             1043311 non-null  int64  \n 8   origbalance      1043311 non-null  float64\n 9   mos_to_repay     1043311 non-null  int64  \n 10  repay_status     1043311 non-null  int64  \n 11  mos_to_balln     1043311 non-null  int64  \n 12  paid_label       1043311 non-null  int64  \ndtypes: float64(4), int64(8), object(1)\nmemory usage: 103.5+ MB"
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#organizing-our-features-and-labels",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#organizing-our-features-and-labels",
    "title": "30  Student Loan: Alternative Metric & Cross-Validation",
    "section": "30.3 Organizing Our Features and Labels",
    "text": "30.3 Organizing Our Features and Labels\nNow that we have our data in memory, we can separate the features and labels in preparation for model fitting. We begin with the features.\n\nlst_features = \\\n    ['loan_age','cosign','income_annual', 'upb',              \n    'monthly_payment','fico','origbalance',\n    'mos_to_repay','repay_status','mos_to_balln',]    \ndf_X = df_train[lst_features]\ndf_X.head()\n\n\n\n\n\n\n\n\nloan_age\ncosign\nincome_annual\nupb\nmonthly_payment\nfico\norigbalance\nmos_to_repay\nrepay_status\nmos_to_balln\n\n\n\n\n0\n56\n0\n113401.60\n36011.11\n397.91\n814\n51453.60\n0\n0\n124\n\n\n1\n56\n1\n100742.34\n101683.38\n1172.10\n711\n130271.33\n0\n0\n124\n\n\n2\n56\n0\n46000.24\n49249.37\n593.57\n772\n62918.96\n0\n0\n124\n\n\n3\n56\n0\n428958.96\n36554.85\n404.63\n849\n48238.73\n0\n0\n125\n\n\n4\n56\n0\n491649.96\n7022.30\n1967.46\n815\n106124.68\n0\n0\n4\n\n\n\n\n\n\n\nNext we do the same for the labels. Note that in our encoding a 1 stands for prepayment, while a 0 stands for non-prepayment.\n\ndf_y = df_train['paid_label']\ndf_y\n\n0          0\n1          0\n2          0\n3          0\n4          0\n          ..\n1043306    0\n1043307    0\n1043308    0\n1043309    0\n1043310    0\nName: paid_label, Length: 1043311, dtype: int64"
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#creating-a-holdout-set-with-train_test_split",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#creating-a-holdout-set-with-train_test_split",
    "title": "30  Student Loan: Alternative Metric & Cross-Validation",
    "section": "30.4 Creating a Holdout Set with train_test_split()",
    "text": "30.4 Creating a Holdout Set with train_test_split()\nIn subsequent sections we will require a holdout set to measure the out-of-sample performance of our models, so let’s create that now.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_X, df_y, random_state = 0, test_size = 0.1)\n\n\nCode Challenge: Explore X_train and X_test and verify that the test_size parameter controls the size of the test set.\n\n\nSolution\nprint(X_train.shape)\nprint(X_test.shape)\n\n\n(938979, 10)\n(104332, 10)"
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#logistic-regression---accuracy-precision-reall-f1",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#logistic-regression---accuracy-precision-reall-f1",
    "title": "30  Student Loan: Alternative Metric & Cross-Validation",
    "section": "30.5 Logistic Regression - Accuracy, Precision, Reall, F1",
    "text": "30.5 Logistic Regression - Accuracy, Precision, Reall, F1\nIn this section we’ll review the traditional goodness-of-fit metrics: accuracy, precision, recall, and F1. We’ll do this in the context of logistic regression.\nLet’s begin by fitting a logistic regression to the entirety of our training data.\n\nfrom sklearn.linear_model import LogisticRegression\nmdl_logit = LogisticRegression(random_state = 0, solver = 'lbfgs')\nmdl_logit.fit(df_X, df_y)\n\nLogisticRegression(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=0)\n\n\nWe can use the .predict() method of our model to generate the predictions of our model.\n\narr_pred_logit = mdl_logit.predict(df_X)\narr_pred_logit\n\narray([0, 0, 0, ..., 0, 0, 0])\n\n\nLet’s take a look at various in-sample accuracy measures of our model.\n\nprint(\"Accuracy:  \", np.round(mdl_logit.score(df_X, df_y), 3))\nprint(\"Precision: \", np.round(sklearn.metrics.precision_score(df_y, arr_pred_logit), 3))\nprint(\"Recall:    \", np.round(sklearn.metrics.recall_score(df_y, arr_pred_logit), 3))\n\nAccuracy:   0.984\nPrecision:  0.332\nRecall:     0.012\n\n\n\nCode Challenge: Use the built-in function in sklearn.metrics to calculate the F1 score.\n\n\nSolution\nprint(np.round(sklearn.metrics.f1_score(df_y, arr_pred_logit), 3))      \n\n\n0.023\n\n\n\nAs we know, in-sample goodness-of-fit metrics are usually too optimistic about model performance. Using a holdout set is a simple way to get a sense for how the model will perform in the wild.\nThe following code fits a logistic regression model to the training set that we created above.\n\nmdl_logit_holdout = LogisticRegression(random_state = 0)\nmdl_logit_holdout.fit(X_train, y_train)\n\nLogisticRegression(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=0)\n\n\nHere is code that calculates the out-of-sample goodness of fit metrics on the test-set.\n\narr_pred_logit_holdout = mdl_logit_holdout.predict(X_test)\n\nprint(\"Accuracy:  \", np.round(mdl_logit_holdout.score(X_test, y_test), 3))\nprint(\"Precision: \", np.round(sklearn.metrics.precision_score(y_test, arr_pred_logit_holdout), 3))\nprint(\"Recall:    \", np.round(sklearn.metrics.recall_score(y_test, arr_pred_logit_holdout), 3))\nprint(\"F1:        \", np.round(sklearn.metrics.f1_score(y_test, arr_pred_logit_holdout), 3))\n\nAccuracy:   0.984\nPrecision:  0.304\nRecall:     0.01\nF1:         0.02"
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#balances-of-loans-that-prepaid",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#balances-of-loans-that-prepaid",
    "title": "30  Student Loan: Alternative Metric & Cross-Validation",
    "section": "30.6 Balances of Loans that Prepaid",
    "text": "30.6 Balances of Loans that Prepaid\nThus far our all of our goodness-of-fit measures have focused on tallying the accuracy of individual predictions. However, ABS investors are not interested in which particular loans prepayed, but rather the total UPB that prepayed.\nThe following code calculates the total UPB of the loans that actually prepayed in the training data.\n\nupb_prepay_actual = \\\n    (\n    df_train[['upb', 'paid_label']]\n        .assign(prepay_upb = lambda df: df.upb * df.paid_label)\n        ['prepay_upb'].sum()\n    )\nupb_prepay_actual\n\n683871848.0400001"
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#balances-of-predicted-prepays",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#balances-of-predicted-prepays",
    "title": "30  Student Loan: Alternative Metric & Cross-Validation",
    "section": "30.7 Balances of Predicted Prepays",
    "text": "30.7 Balances of Predicted Prepays\nLet’s now calculate the balance of the loans that our logistic regression model predicts will prepay.\n\nupb_prepay_logit = \\\n    (\n    df_train\n        .assign(pred_logit = mdl_logit.predict(df_X))\n        .assign(prepay_upb_logit = lambda df: df.pred_logit * df.upb)\n        ['prepay_upb_logit'].sum()\n    )\n\nupb_prepay_logit\n\n28332218.820000004\n\n\nAs you can see, the logistic regression UPB prepay predictions are only 4% of what actually occurred.\n\nupb_prepay_logit / upb_prepay_actual\n\n0.04142913456841527"
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#expected-value-of-balance-of-loan-prepayment-in-sample",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#expected-value-of-balance-of-loan-prepayment-in-sample",
    "title": "30  Student Loan: Alternative Metric & Cross-Validation",
    "section": "30.8 Expected Value of Balance of Loan Prepayment (In-Sample)",
    "text": "30.8 Expected Value of Balance of Loan Prepayment (In-Sample)\nUnder the hood, most classification algorithms calculate a probability for each class. The specific prediction is then simply the class with the highest probability.\nIn sklearn we can view these probabilities with the .predict_proba() method. Let’s do this with mdl_logit.\n\nmdl_logit.predict_proba(df_X)\n\narray([[0.99119454, 0.00880546],\n       [0.98607272, 0.01392728],\n       [0.98927091, 0.01072909],\n       ...,\n       [0.98597615, 0.01402385],\n       [0.99228992, 0.00771008],\n       [0.98784426, 0.01215574]])\n\n\nIn our example, the probability of prepayment is in the second column, which we can isolate as follows.\n\nmdl_logit.predict_proba(df_X)[:, 1]\n\narray([0.00880546, 0.01392728, 0.01072909, ..., 0.01402385, 0.00771008,\n       0.01215574])\n\n\nUsing these probabilities, let’s calculate an expected value for the total UPB that will be prepaid.\n\nev_logit = \\\n    (\n    df_train\n        .assign(pred_logit = mdl_logit.predict_proba(df_X)[:,1])\n        .assign(prepay_upb_logit = lambda df: df.pred_logit * df.upb)\n        ['prepay_upb_logit'].sum()\n    )\n\nev_logit\n\n683873044.6387616\n\n\nAs you can see, the in-sample expected value calculation is almost exactly in-line with the actual prepayments.\n\nev_logit / upb_prepay_actual\n\n1.0000017497412197"
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#expected-value-of-balance-of-loan-prepayments-out-of-sample",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#expected-value-of-balance-of-loan-prepayments-out-of-sample",
    "title": "30  Student Loan: Alternative Metric & Cross-Validation",
    "section": "30.9 Expected Value of Balance of Loan Prepayments (Out-of-Sample)",
    "text": "30.9 Expected Value of Balance of Loan Prepayments (Out-of-Sample)\nAs we can see above, from an expected UPB standpoint, our model seems to be working quite well. However, the above calculation was done in-sample. Let’s try an out-of-sample accuracy measure calculation with our holdout set.\nWe begin by fitting a model to the training data.\n\nmdl_logit_holdout = LogisticRegression(random_state = 0)\nmdl_logit_holdout.fit(X_train, y_train)\n\nLogisticRegression(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=0)\n\n\nNext, let’s calculated the actual prepayments in the test-set.\n\nprepay_test = \\\n    (\n    X_test\n        .merge(y_test, left_index=True, right_index = True)\n        .assign(upb_prepay = lambda df: df.upb * df.paid_label)\n        ['upb_prepay'].sum()    \n    )\n\nprepay_test\n\n68602482.71000001\n\n\nThe following code returns the out-of-sample prediction probabilities for the test set.\n\nmdl_logit_holdout.predict_proba(X_test)\n\narray([[0.98717972, 0.01282028],\n       [0.99661209, 0.00338791],\n       [0.99589556, 0.00410444],\n       ...,\n       [0.98467554, 0.01532446],\n       [0.99119456, 0.00880544],\n       [0.97107693, 0.02892307]])\n\n\n\nCode Challenge: Calculate the out-of-sample expected value of prepaid UPB for the hold-out test set; also, find it’s proportion relative to the actual prepayments.\n\n\nSolution\nprepay_holdout = \\\n    (\n    X_test\n        .assign(pred_holdout = mdl_logit_holdout.predict_proba(X_test)[:, 1])\n        .assign(upb_prepay_holdout = lambda df: df.upb * df.pred_holdout)\n        ['upb_prepay_holdout'].sum()\n    )\n\nprint(prepay_holdout)\nprint(prepay_holdout / prepay_test)\n\n\n67477986.49727169\n0.9836085201539738"
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#cross-validation-for-precision-recall-and-f1-score",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#cross-validation-for-precision-recall-and-f1-score",
    "title": "30  Student Loan: Alternative Metric & Cross-Validation",
    "section": "30.10 Cross-Validation for Precision, Recall, and F1 Score",
    "text": "30.10 Cross-Validation for Precision, Recall, and F1 Score\nThe holdout set methodology can be generalized to \\(n\\)-fold cross validation. The set of goodness-of-fit measures that result from cross-validation are, in aggregate, more robust than a metric calculated on a single holdout test set.\nIn this final section, we’ll see what the code looks like to generate these cross-validation metrics for a decision tree classifier.\nLet’s begin by instantiating a decision tree model.\n\nfrom sklearn.tree import DecisionTreeClassifier\nmdl_tree = DecisionTreeClassifier(random_state = 0)\n\nThe following code generates F1, precision, and recall via cross-validation.\n\ndct_cv = sklearn.model_selection.cross_validate(mdl_tree, df_X, df_y, scoring = ['f1', 'precision', 'recall'], cv = 5)\ndct_cv\n\n{'fit_time': array([12.7782793 , 13.40303349, 11.756989  , 11.9663868 , 11.73292732]),\n 'score_time': array([0.17642283, 0.17362189, 0.17155647, 0.17140222, 0.17079616]),\n 'test_f1': array([0.22146021, 0.35535158, 0.37082628, 0.38977097, 0.41728045]),\n 'test_precision': array([0.21438451, 0.39882266, 0.38671374, 0.37145153, 0.40070729]),\n 'test_recall': array([0.22901891, 0.32042566, 0.35619273, 0.40999113, 0.43528369])}\n\n\n\nCode Challenge: Calculate the average F1 score in our cross-validation scheme.\n\n\nSolution\ndct_cv['test_f1'].mean()\n\n\n0.35093789908958994"
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#cross-validation-on-the-alternative-metric",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#cross-validation-on-the-alternative-metric",
    "title": "30  Student Loan: Alternative Metric & Cross-Validation",
    "section": "30.11 Cross-Validation on the Alternative Metric",
    "text": "30.11 Cross-Validation on the Alternative Metric\nThe goodness of fit metric that will be most useful to us will be the expected value of prepaid balance. Unfortunately, this does not fit neatly into the .cross_validate() method in the previous section. Thus, in order to use our expected value of prepaid balance metric in a cross-validation context, we will have to write some of the boiler-plate code. Luckily, the sklearn.model_selection module does much of the heavy lifting for us. This is what we will do in the next chapter."
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#further-reading",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#further-reading",
    "title": "30  Student Loan: Alternative Metric & Cross-Validation",
    "section": "30.12 Further Reading",
    "text": "30.12 Further Reading\nSklearn User Guides\nhttps://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\nhttps://scikit-learn.org/stable/modules/tree.html\nhttps://scikit-learn.org/stable/modules/cross_validation.html\nSklearn API Documentation\nhttps://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nhttps://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate"
  },
  {
    "objectID": "chapters/31_student_loan_hyperparameter_tuning/student_loan_hyperparameter_tuning.html#loading-packages",
    "href": "chapters/31_student_loan_hyperparameter_tuning/student_loan_hyperparameter_tuning.html#loading-packages",
    "title": "31  Student Loan: Hyperparameter Tuning",
    "section": "31.1 Loading Packages",
    "text": "31.1 Loading Packages\nLet’s begin by loading the packages that we will need.\n\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport time\nimport seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "chapters/31_student_loan_hyperparameter_tuning/student_loan_hyperparameter_tuning.html#reading-in-data",
    "href": "chapters/31_student_loan_hyperparameter_tuning/student_loan_hyperparameter_tuning.html#reading-in-data",
    "title": "31  Student Loan: Hyperparameter Tuning",
    "section": "31.2 Reading-In Data",
    "text": "31.2 Reading-In Data\nNext, let’s read-in our data set.\n\ndf_train = pd.read_csv('../data/student_loan.csv')\ndf_train\n\n\n\n\n\n\n\n\nload_id\ndeal_name\nloan_age\ncosign\nincome_annual\nupb\nmonthly_payment\nfico\norigbalance\nmos_to_repay\nrepay_status\nmos_to_balln\npaid_label\n\n\n\n\n0\n765579\n2014_b\n56\n0\n113401.60\n36011.11\n397.91\n814\n51453.60\n0\n0\n124\n0\n\n\n1\n765580\n2014_b\n56\n1\n100742.34\n101683.38\n1172.10\n711\n130271.33\n0\n0\n124\n0\n\n\n2\n765581\n2014_b\n56\n0\n46000.24\n49249.37\n593.57\n772\n62918.96\n0\n0\n124\n0\n\n\n3\n765582\n2014_b\n56\n0\n428958.96\n36554.85\n404.63\n849\n48238.73\n0\n0\n125\n0\n\n\n4\n765583\n2014_b\n56\n0\n491649.96\n7022.30\n1967.46\n815\n106124.68\n0\n0\n4\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1043306\n1808885\n2019_c\n2\n0\n152885.00\n115363.12\n1212.22\n798\n116834.64\n0\n0\n118\n0\n\n\n1043307\n1808886\n2019_c\n2\n0\n116480.00\n77500.70\n831.13\n826\n79566.03\n0\n0\n118\n0\n\n\n1043308\n1808887\n2019_c\n2\n0\n96800.00\n16156.76\n232.34\n781\n16472.50\n0\n0\n82\n0\n\n\n1043309\n1808888\n2019_c\n2\n0\n78400.14\n77197.03\n833.57\n777\n78135.54\n0\n0\n118\n0\n\n\n1043310\n1808889\n2019_c\n65\n0\n50447.28\n65667.85\n767.10\n765\n82602.38\n0\n0\n119\n0\n\n\n\n\n1043311 rows × 13 columns\n\n\n\nWe can inspect the columns of our data set with the DataFrame.info() method.\n\ndf_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1043311 entries, 0 to 1043310\nData columns (total 13 columns):\n #   Column           Non-Null Count    Dtype  \n---  ------           --------------    -----  \n 0   load_id          1043311 non-null  int64  \n 1   deal_name        1043311 non-null  object \n 2   loan_age         1043311 non-null  int64  \n 3   cosign           1043311 non-null  int64  \n 4   income_annual    1043311 non-null  float64\n 5   upb              1043311 non-null  float64\n 6   monthly_payment  1043311 non-null  float64\n 7   fico             1043311 non-null  int64  \n 8   origbalance      1043311 non-null  float64\n 9   mos_to_repay     1043311 non-null  int64  \n 10  repay_status     1043311 non-null  int64  \n 11  mos_to_balln     1043311 non-null  int64  \n 12  paid_label       1043311 non-null  int64  \ndtypes: float64(4), int64(8), object(1)\nmemory usage: 103.5+ MB"
  },
  {
    "objectID": "chapters/31_student_loan_hyperparameter_tuning/student_loan_hyperparameter_tuning.html#organizing-our-features-and-labels",
    "href": "chapters/31_student_loan_hyperparameter_tuning/student_loan_hyperparameter_tuning.html#organizing-our-features-and-labels",
    "title": "31  Student Loan: Hyperparameter Tuning",
    "section": "31.3 Organizing Our Features and Labels",
    "text": "31.3 Organizing Our Features and Labels\nNow that we have our data in memory, we can separate the features and labels in preparation for model fitting. We begin with the features.\n\nlst_features = \\\n    ['loan_age','cosign','income_annual', 'upb',              \n    'monthly_payment','fico','origbalance',\n    'mos_to_repay','repay_status','mos_to_balln',]    \ndf_X = df_train[lst_features]\ndf_y = df_train['paid_label']\n\nAnd next we do the same for the labels. Note that in our encoding a 1 stands for prepayment, while a 0 stands for non-prepayment.\n\ndf_y = df_train['paid_label']\ndf_y\n\n0          0\n1          0\n2          0\n3          0\n4          0\n          ..\n1043306    0\n1043307    0\n1043308    0\n1043309    0\n1043310    0\nName: paid_label, Length: 1043311, dtype: int64"
  },
  {
    "objectID": "chapters/31_student_loan_hyperparameter_tuning/student_loan_hyperparameter_tuning.html#sklearn.model_selection.kfold",
    "href": "chapters/31_student_loan_hyperparameter_tuning/student_loan_hyperparameter_tuning.html#sklearn.model_selection.kfold",
    "title": "31  Student Loan: Hyperparameter Tuning",
    "section": "31.4 sklearn.model_selection.KFold",
    "text": "31.4 sklearn.model_selection.KFold\nCross-validation techniques are a useful for estimating out-of-sample goodness of fit metrics for a model. Writing cross-validation code from scratch would involve a lot of boiler plate code, which basically amounts to lots of for-loops whose implementation yields very little insight.\nOne of great things about sklearn is that it contains a variety of convenience functions that take care a lot of this sort boiler-plate code for you.\nA great example of such a convenience function is KFold() which produces arrays of indexes that define a K-Fold cross validation.\nThe following code returns an object that when applied to a data set will yield the indexes for the training set and test set for each iteration of a 2-fold cross validation.\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits = 2, shuffle = True, random_state = 0)\n\nWe can use a for-loop to inspect these indexes:\n\nfor train_index, test_index in kf.split(df_X, df_y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n\nTRAIN: [      1       2       3 ... 1043303 1043307 1043309] TEST: [      0       5       6 ... 1043306 1043308 1043310]\nTRAIN: [      0       5       6 ... 1043306 1043308 1043310] TEST: [      1       2       3 ... 1043303 1043307 1043309]\n\n\n\nResearch Challenge: Google sklearn.model_selection.StratifiedKFold and describe the difference between it and KFold().\n\n\nSolution\n# The folds are made by preserving the percentage of samples for each class."
  },
  {
    "objectID": "chapters/31_student_loan_hyperparameter_tuning/student_loan_hyperparameter_tuning.html#choosing-optimal-max_depth-for-decisiontreeclassifier",
    "href": "chapters/31_student_loan_hyperparameter_tuning/student_loan_hyperparameter_tuning.html#choosing-optimal-max_depth-for-decisiontreeclassifier",
    "title": "31  Student Loan: Hyperparameter Tuning",
    "section": "31.5 Choosing Optimal max_depth for DecisionTreeClassifier",
    "text": "31.5 Choosing Optimal max_depth for DecisionTreeClassifier\nRecall that the decision tree algorithm is a process that iteratively partitions the feature space. Specifically, for each iteration of the algorithm, a split is made along one particular dimension of the feature space. The process is repeated until some kind of stopping criteria is met.\nThe hyperparameters of a decision tree model control various criteria for stopping this iterative splitting process. The more strict the stopping criteria (i.e. stopping faster) the less flexible the model. The more lenient the stopping criteria (i.e. stopping slower) the more flexible the model.\nRestricting the flexibility of a model by changing hyperparameters is also referred to as regularization.\nIn this section we demonstrate choosing an optimal max_depth value for a decision tree classifier on our student loan data. Towards this end we will use cross-validation.\nLet’s import the functions that we will need.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import f1_score\n\nWe will use 10-fold cross-validation, so let’s invoke the required indexes with KFold.\n\nkf = KFold(n_splits = 10, shuffle = True, random_state = 0)\n\nThe following code calculates a 10-fold CV F1-score, expected-balance ratio, and average fit-time for various levels of max_depth.\n\n# various levels of max_depth we will try\nlst_depth = [1, 2, 3, 4, 5, 10, 15, 20, 25]\n\n# lists for storing average statistics \nlst_f1_avg = []\nlst_eb_ratio_avg = []\nlst_fit_time_avg = []\nfor ix_depth in lst_depth:\n\n    # list for storing statitics for each fold of cross-validation\n    lst_f1 = []\n    lst_eb_ratio = []\n    lst_fit_time = []\n    for train_index, test_index in kf.split(df_X, df_y):    \n        # creating training set \n        X_train = df_X.iloc[train_index]\n        y_train = df_y.iloc[train_index]\n\n        # creating test set\n        X_test = df_X.iloc[test_index]\n        y_test = df_y.iloc[test_index]\n\n        # intantiating model\n        mdl = DecisionTreeClassifier(max_depth = ix_depth, random_state = 0)\n\n        # fit the model\n        start = time.time()\n        mdl.fit(X_train, y_train)\n        arr_pred = mdl.predict(X_test)\n        end = time.time()\n        \n        # fit time\n        dbl_fit_time = end - start\n        lst_fit_time.append(dbl_fit_time)\n\n        # f1-score\n        dbl_f1 = f1_score(y_test, arr_pred)\n        lst_f1.append(dbl_f1) \n\n        # expected-balance ratio\n        arr_pred_proba = mdl.predict_proba(X_test)[:,1]\n        dbl_eb_ratio = (arr_pred_proba * X_test['upb']).sum() / (y_test * X_test['upb']).sum()\n        lst_eb_ratio.append(dbl_eb_ratio)\n\n    # calculating and storing average metrics\n    fit_time_avg = np.round(np.mean(lst_fit_time), 1)\n    lst_fit_time_avg.append(fit_time_avg)\n    f1_avg = np.round(np.mean(lst_f1), 3)\n    lst_f1_avg.append(f1_avg)\n    eb_ratio_avg = np.round(np.mean(lst_eb_ratio), 3)\n    lst_eb_ratio_avg.append(eb_ratio_avg)\n    \n    # printing some output so I know my code is running\n    print(ix_depth)\n\n1\n2\n3\n4\n5\n10\n15\n20\n25\n\n\nLet’s put our results into a DataFrame, and then graph them.\n\ndf_results = pd.DataFrame({'max_depth':lst_depth, 'f1':lst_f1_avg, 'eb_ratio':lst_eb_ratio_avg, 'fit_time':lst_fit_time_avg})\ndf_results\n\n\n\n\n\n\n\n\nmax_depth\nf1\neb_ratio\nfit_time\n\n\n\n\n0\n1\n0.374\n1.261\n0.7\n\n\n1\n2\n0.464\n1.128\n1.1\n\n\n2\n3\n0.444\n1.048\n1.8\n\n\n3\n4\n0.459\n1.054\n2.3\n\n\n4\n5\n0.470\n0.984\n2.9\n\n\n5\n10\n0.486\n1.043\n5.8\n\n\n6\n15\n0.478\n1.016\n8.4\n\n\n7\n20\n0.463\n0.997\n10.5\n\n\n8\n25\n0.441\n0.978\n12.2\n\n\n\n\n\n\n\n\n31.5.1 Graph of F1\n\n%matplotlib inline\nwith sns.axes_style('darkgrid'):\n    g = sns.relplot(\n            x = 'max_depth',\n            y = 'f1',\n            data = df_results,\n            alpha = 0.75,\n            height = 5, \n            aspect = 1.5,\n        );\n    plt.subplots_adjust(top = 0.93);\n    g.fig.suptitle('F1 for Various Levels of Max-Depth');\n\n\n\n\n\n\n31.5.2 Graph of Expected-Balance Ratio\n\nwith sns.axes_style('darkgrid'):\n    g = sns.relplot(\n            x = 'max_depth',\n            y = 'eb_ratio',\n            data = df_results,\n            alpha = 0.75,\n            height = 5, \n            aspect = 1.5,\n        );\n    plt.subplots_adjust(top = 0.93);\n    g.fig.suptitle('Expected-Balance Ratio for Various Levels of Max-Depth');\n\n\n\n\n\n\n31.5.3 Graph of Fit Time\n\nwith sns.axes_style('darkgrid'):\n    g = sns.relplot(\n            x = 'max_depth',\n            y = 'fit_time',\n            data = df_results,\n            alpha = 0.75,\n            height = 5, \n            aspect = 1.5,\n        );\n    plt.subplots_adjust(top = 0.93);\n    g.fig.suptitle('Fit-Time for Various Levels of Max-Depth');\n\n\n\n\n\nDiscussion Question: Based on these results, which what would you choose for max_depth?\n\n\nSolution\n# I would probaby choose something in the 5-10 range."
  },
  {
    "objectID": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#import-packages",
    "href": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#import-packages",
    "title": "32  Student Loan: xgboost",
    "section": "32.1 Import Packages",
    "text": "32.1 Import Packages\n\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport xgboost\npd.options.display.max_rows = 10"
  },
  {
    "objectID": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#read-in-data",
    "href": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#read-in-data",
    "title": "32  Student Loan: xgboost",
    "section": "32.2 Read-In Data",
    "text": "32.2 Read-In Data\n\ndf_train = pd.read_csv('../data/student_loan.csv')\ndf_train\n\n\n\n\n\n\n\n\nload_id\ndeal_name\nloan_age\ncosign\nincome_annual\nupb\nmonthly_payment\nfico\norigbalance\nmos_to_repay\nrepay_status\nmos_to_balln\npaid_label\n\n\n\n\n0\n765579\n2014_b\n56\n0\n113401.60\n36011.11\n397.91\n814\n51453.60\n0\n0\n124\n0\n\n\n1\n765580\n2014_b\n56\n1\n100742.34\n101683.38\n1172.10\n711\n130271.33\n0\n0\n124\n0\n\n\n2\n765581\n2014_b\n56\n0\n46000.24\n49249.37\n593.57\n772\n62918.96\n0\n0\n124\n0\n\n\n3\n765582\n2014_b\n56\n0\n428958.96\n36554.85\n404.63\n849\n48238.73\n0\n0\n125\n0\n\n\n4\n765583\n2014_b\n56\n0\n491649.96\n7022.30\n1967.46\n815\n106124.68\n0\n0\n4\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1043306\n1808885\n2019_c\n2\n0\n152885.00\n115363.12\n1212.22\n798\n116834.64\n0\n0\n118\n0\n\n\n1043307\n1808886\n2019_c\n2\n0\n116480.00\n77500.70\n831.13\n826\n79566.03\n0\n0\n118\n0\n\n\n1043308\n1808887\n2019_c\n2\n0\n96800.00\n16156.76\n232.34\n781\n16472.50\n0\n0\n82\n0\n\n\n1043309\n1808888\n2019_c\n2\n0\n78400.14\n77197.03\n833.57\n777\n78135.54\n0\n0\n118\n0\n\n\n1043310\n1808889\n2019_c\n65\n0\n50447.28\n65667.85\n767.10\n765\n82602.38\n0\n0\n119\n0\n\n\n\n\n1043311 rows × 13 columns"
  },
  {
    "objectID": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#feature-selection",
    "href": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#feature-selection",
    "title": "32  Student Loan: xgboost",
    "section": "32.3 Feature Selection",
    "text": "32.3 Feature Selection\n\nlst_features = \\\n    ['loan_age','cosign','income_annual', 'upb',              \n    'monthly_payment','fico','origbalance',\n    'mos_to_repay','repay_status','mos_to_balln',]    \ndf_X = df_train[lst_features]\ndf_X\n\n\n\n\n\n\n\n\nloan_age\ncosign\nincome_annual\nupb\nmonthly_payment\nfico\norigbalance\nmos_to_repay\nrepay_status\nmos_to_balln\n\n\n\n\n0\n56\n0\n113401.60\n36011.11\n397.91\n814\n51453.60\n0\n0\n124\n\n\n1\n56\n1\n100742.34\n101683.38\n1172.10\n711\n130271.33\n0\n0\n124\n\n\n2\n56\n0\n46000.24\n49249.37\n593.57\n772\n62918.96\n0\n0\n124\n\n\n3\n56\n0\n428958.96\n36554.85\n404.63\n849\n48238.73\n0\n0\n125\n\n\n4\n56\n0\n491649.96\n7022.30\n1967.46\n815\n106124.68\n0\n0\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1043306\n2\n0\n152885.00\n115363.12\n1212.22\n798\n116834.64\n0\n0\n118\n\n\n1043307\n2\n0\n116480.00\n77500.70\n831.13\n826\n79566.03\n0\n0\n118\n\n\n1043308\n2\n0\n96800.00\n16156.76\n232.34\n781\n16472.50\n0\n0\n82\n\n\n1043309\n2\n0\n78400.14\n77197.03\n833.57\n777\n78135.54\n0\n0\n118\n\n\n1043310\n65\n0\n50447.28\n65667.85\n767.10\n765\n82602.38\n0\n0\n119\n\n\n\n\n1043311 rows × 10 columns\n\n\n\n\ndf_y = df_train['paid_label']\ndf_y\n\n0          0\n1          0\n2          0\n3          0\n4          0\n          ..\n1043306    0\n1043307    0\n1043308    0\n1043309    0\n1043310    0\nName: paid_label, Length: 1043311, dtype: int64"
  },
  {
    "objectID": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#creating-holdout-sets",
    "href": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#creating-holdout-sets",
    "title": "32  Student Loan: xgboost",
    "section": "32.4 Creating Holdout Sets",
    "text": "32.4 Creating Holdout Sets\n\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(df_X, df_y, random_state = 0)\n\n\nX_train = X_train.copy()\nX_test = X_test.copy()\ny_train = y_train.copy()\ny_test = y_test.copy()"
  },
  {
    "objectID": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#initial-fit",
    "href": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#initial-fit",
    "title": "32  Student Loan: xgboost",
    "section": "32.5 Initial Fit",
    "text": "32.5 Initial Fit\n\nfrom xgboost import XGBClassifier\nmodel = XGBClassifier(eval_metric='logloss')\nmodel.fit(X_train, y_train)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)\n\n\n\ny_pred = model.predict(X_test)\n\n\nprint('Actual:    ', y_test.sum())\nprint('Predicted: ', y_pred.sum())\n\nActual:     4227\nPredicted:  1647\n\n\n\nprint('Actual:   ', sum(X_test['upb'] * y_test))\nprint('Predicted: ', sum(X_test['upb'] * y_pred))\nprint('Ratio:     ', sum(X_test['upb'] * y_pred) / sum(X_test['upb'] * y_test))\n\nActual:    166234148.19000015\nPredicted:  32604191.500000022\nRatio:      0.19613413883370406\n\n\n\nsklearn.metrics.f1_score(y_test, y_pred)\n\n0.5110657133129044"
  },
  {
    "objectID": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#modifying-scale_pos_weight",
    "href": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#modifying-scale_pos_weight",
    "title": "32  Student Loan: xgboost",
    "section": "32.6 Modifying scale_pos_weight",
    "text": "32.6 Modifying scale_pos_weight\n\n# fit model no training data\nmodel = XGBClassifier(eval_metric='logloss', scale_pos_weight=25)\nmodel.fit(X_train, y_train)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)\n\n\n\ny_pred = model.predict(X_test)\n\n\nprint('Actual:    ', y_test.sum())\nprint('Predicted: ', y_pred.sum())\n\nActual:     4227\nPredicted:  6905\n\n\n\nprint('Actual:   ', sum(X_test['upb'] * y_test))\nprint('Predicted: ', sum(X_test['upb'] * y_pred))\nprint('Ratio:     ', sum(X_test['upb'] * y_pred) / sum(X_test['upb'] * y_test))\n\nActual:    166234148.19000015\nPredicted:  128185269.83000004\nRatio:      0.7711127420311289\n\n\n\nsklearn.metrics.f1_score(y_test, y_pred)\n\n0.3645346748113546"
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#importing-packages",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#importing-packages",
    "title": "33  Option Volume: Introduction",
    "section": "33.1 Importing Packages",
    "text": "33.1 Importing Packages\nLet’s begin by importing the packages that we will need.\n\nimport numpy as np\nimport pandas as pd\nimport sklearn"
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#reading-in-data",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#reading-in-data",
    "title": "33  Option Volume: Introduction",
    "section": "33.2 Reading-In Data",
    "text": "33.2 Reading-In Data\nNext let’s read-in the data that we will need.\n\ndf_stats = pd.read_csv('../data/option_stats.csv')\ndf_stock = pd.read_csv('../data/option_stock_quote.csv')\n\nThe data in df_stats consists of volume, open-interest, and implied volatility statistics for all available underlyings.\n\ndf_stats\n\n\n\n\n\n\n\n\nsymbol\nquotedate\nimplied_vol\ntotalvol\ntotaloi\n\n\n\n\n0\nA\n2016-01-04\n0.3289\n1330\n116961\n\n\n1\nAA\n2016-01-04\n0.4843\n38615\n1152498\n\n\n2\nAAC\n2016-01-04\n0.8606\n3\n1386\n\n\n3\nAAL\n2016-01-04\n0.4096\n56386\n875178\n\n\n4\nAAN\n2016-01-04\n0.4471\n23\n5480\n\n\n...\n...\n...\n...\n...\n...\n\n\n2202060\nZTO\n2018-01-31\n0.4140\n53\n15086\n\n\n2202061\nZTS\n2018-01-31\n0.2419\n1333\n29890\n\n\n2202062\nZUMZ\n2018-01-31\n0.5936\n21\n4121\n\n\n2202063\nZX\n2018-01-31\n0.0000\n0\n21\n\n\n2202064\nZYNE\n2018-01-31\n0.7505\n253\n10646\n\n\n\n\n2202065 rows × 5 columns\n\n\n\nThe data in df_stock consists of end-of-day prices for each of the underlyings.\n\ndf_stock\n\n\n\n\n\n\n\n\nsymbol\nquotedate\npx_close\nvolume\n\n\n\n\n0\nA\n2016-01-04\n40.69000\n3287300\n\n\n1\nAA\n2016-01-04\n9.70999\n12639700\n\n\n2\nAAC\n2016-01-04\n18.52000\n119400\n\n\n3\nAAL\n2016-01-04\n40.91000\n12037200\n\n\n4\nAAN\n2016-01-04\n22.68000\n698000\n\n\n...\n...\n...\n...\n...\n\n\n2201883\nZTO\n2018-01-31\n15.81000\n1121202\n\n\n2201884\nZTS\n2018-01-31\n76.77000\n3690956\n\n\n2201885\nZUMZ\n2018-01-31\n20.75000\n361661\n\n\n2201886\nZX\n2018-01-31\n1.31000\n17279\n\n\n2201887\nZYNE\n2018-01-31\n12.09000\n263042\n\n\n\n\n2201888 rows × 4 columns"
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#wrangling-selecting-the-universe",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#wrangling-selecting-the-universe",
    "title": "33  Option Volume: Introduction",
    "section": "33.3 Wrangling: Selecting the Universe",
    "text": "33.3 Wrangling: Selecting the Universe\nWe will use data from 2016 to choose the universe for our analysis. The universe we will ultimately arrive at is the 301-500 most liquid underlyings for which there is data for all of 2017 (training period) and January of 2018 (backtest period).\nLet’s begin by isolating the 2016 data from df_stats.\n\ndf_stats_2016 = df_stats.query('quotedate &gt;= \"2016-01-01\"').query('quotedate &lt;= \"2016-12-31\"')\n\nNext, we calculate the daily volume volume ranks for all the symbols in 2016.\n\ndf_volume_rank = \\\n    (\n    df_stats_2016\n        .groupby(['symbol'])[['totalvol']].mean()\n        .reset_index()\n        .rename(columns={'totalvol':'average_daily_volume'})\n        .assign(average_daily_volume = lambda df: np.round(df['average_daily_volume']))\n        .assign(volume_rank = lambda df: df['average_daily_volume'].rank(ascending=False))\n    )\ndf_volume_rank\n\n\n\n\n\n\n\n\nsymbol\naverage_daily_volume\nvolume_rank\n\n\n\n\n0\nA\n905.0\n895.5\n\n\n1\nAA\n39852.0\n55.0\n\n\n2\nAAC\n108.0\n2118.0\n\n\n3\nAAL\n37165.0\n60.0\n\n\n4\nAAN\n402.0\n1308.0\n\n\n...\n...\n...\n...\n\n\n4742\nZTO\n358.0\n1381.0\n\n\n4743\nZTS\n1945.0\n589.5\n\n\n4744\nZUMZ\n206.0\n1693.0\n\n\n4745\nZX\n0.0\n4703.5\n\n\n4746\nZYNE\n91.0\n2235.0\n\n\n\n\n4747 rows × 3 columns\n\n\n\nLet’s initially create df_raw_universe that has more symbols than we will need because not all symbols have data for all days. In particular, we will first grab the 301-700th most liquid underlyings. In the next steps we will select only symbols that have data for all days.\n\ndf_universe_raw = \\\n    (\n    df_volume_rank\n        .query('volume_rank &gt;= 301 & volume_rank &lt;= 700')\n        .sort_values(['volume_rank'])\n        [['symbol', 'average_daily_volume', 'volume_rank']]\n    )\ndf_universe_raw\n\n\n\n\n\n\n\n\nsymbol\naverage_daily_volume\nvolume_rank\n\n\n\n\n624\nBUD\n5623.0\n301.0\n\n\n2159\nIILG\n5619.0\n302.0\n\n\n4032\nTAP\n5611.0\n303.0\n\n\n325\nASHR\n5606.0\n304.0\n\n\n3301\nPOM\n5565.0\n305.0\n\n\n...\n...\n...\n...\n\n\n2812\nMTW\n1441.0\n696.0\n\n\n957\nCRC\n1438.0\n697.0\n\n\n3296\nPNRA\n1432.0\n698.5\n\n\n1653\nFNSR\n1432.0\n698.5\n\n\n4433\nVIXY\n1428.0\n700.0\n\n\n\n\n400 rows × 3 columns\n\n\n\nNext, let’s grab the data for both the training period (2017) and the backtest period (January 2018).\n\ndf_stats_analysis = df_stats.query('quotedate &gt;= \"2017-01-01\"').query('quotedate &lt;= \"2018-01-31\"')\n\nAnd finally we are ready to select our universe.\n\ndf_universe = \\\n    (\n    df_universe_raw                                                   # start with big universe\n        .merge(df_stats_analysis, how='left', on='symbol')            # join volume and volatility stats\n        .merge(df_stock, how='left', on=['symbol', 'quotedate'])      # join stock price data\n        .dropna()\n        .groupby(['symbol', 'volume_rank'])[['quotedate']].count()    # count the number of rows of data that exist\n        .reset_index()\n        .sort_values(['volume_rank'])\n        .query('quotedate == 272')                                    # grab the symbols that have all 272 days worth of data - this is hardcoded\n        .assign(rerank = lambda df: df['volume_rank'].rank())         # rerank this smaller universe\n    ).query('rerank &lt;= 300')                                          # grab the 300 most liquid underlyings in restricted universe\ndf_universe\n\n\n\n\n\n\n\n\nsymbol\nvolume_rank\nquotedate\nrerank\n\n\n\n\n44\nBUD\n301.0\n272\n1.0\n\n\n321\nTAP\n303.0\n272\n2.0\n\n\n25\nASHR\n304.0\n272\n3.0\n\n\n19\nAMJ\n306.0\n272\n4.0\n\n\n69\nCOF\n307.0\n272\n5.0\n\n\n...\n...\n...\n...\n...\n\n\n9\nAEP\n647.0\n272\n296.0\n\n\n259\nPII\n648.0\n272\n297.0\n\n\n27\nAU\n649.0\n272\n298.0\n\n\n320\nSYY\n650.0\n272\n299.0\n\n\n266\nPSTG\n651.0\n272\n300.0\n\n\n\n\n300 rows × 4 columns"
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#wrangling-handling-zero-implied-vols",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#wrangling-handling-zero-implied-vols",
    "title": "33  Option Volume: Introduction",
    "section": "33.4 Wrangling: Handling Zero Implied Vols",
    "text": "33.4 Wrangling: Handling Zero Implied Vols\nThere are a number of rows in df_stats_analysis that have zero implied vols, which will affect our analysis down the road.\n\ndf_stats_analysis.query('implied_vol &lt;= 0')\n\n\n\n\n\n\n\n\nsymbol\nquotedate\nimplied_vol\ntotalvol\ntotaloi\n\n\n\n\n1066997\nAAU\n2017-01-03\n0.0\n103\n3217\n\n\n1067042\nACUR\n2017-01-03\n0.0\n0\n647\n\n\n1067084\nAFMD\n2017-01-03\n0.0\n50\n1655\n\n\n1067140\nALIM\n2017-01-03\n0.0\n0\n2454\n\n\n1067150\nALQA\n2017-01-03\n0.0\n0\n16\n\n\n...\n...\n...\n...\n...\n...\n\n\n2201849\nVVUS\n2018-01-31\n0.0\n0\n5085\n\n\n2201895\nWG\n2018-01-31\n0.0\n0\n1598\n\n\n2201967\nXCOOQ\n2018-01-31\n0.0\n0\n10573\n\n\n2202017\nXSPA\n2018-01-31\n0.0\n3\n3031\n\n\n2202063\nZX\n2018-01-31\n0.0\n0\n21\n\n\n\n\n45968 rows × 5 columns\n\n\n\nLet’s take care of these now. We will set these implied vols to the average non-zero implied vol in the data set. There are probably more sophisticated ways to do this that are more accurate, but using this crude approach will not affect the analysis.\n\nnon_zero_mean = df_stats_analysis.query('implied_vol &gt; 0')['implied_vol'].mean()\ndf_stats_analysis.loc[df_stats_analysis.implied_vol == 0, \"implied_vol\"] = non_zero_mean\ndf_stats_analysis.query('implied_vol &lt;= 0')\n\n\n\n\n\n\n\n\nsymbol\nquotedate\nimplied_vol\ntotalvol\ntotaloi"
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#feature-construction-of-training-set-2017-and-backtest-set-2018",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#feature-construction-of-training-set-2017-and-backtest-set-2018",
    "title": "33  Option Volume: Introduction",
    "section": "33.5 Feature Construction of Training Set (2017) and Backtest Set (2018)",
    "text": "33.5 Feature Construction of Training Set (2017) and Backtest Set (2018)\nLet’s define the training set, and create the features that we may want to use.\n\ndf_train = \\\n    (\n    df_universe[['symbol','rerank']]\n        .rename(columns={'rerank':'volume_rank'})\n        .merge(df_stats_analysis, how='left', on=['symbol'])\n        .query('quotedate &lt;= \"2017-12-31\"')\n        .assign(iv_one_lag = lambda df: df.groupby(['symbol'])['implied_vol'].shift())\n        .assign(iv_change_one_lag = lambda df: df.groupby(['symbol'])['implied_vol'].pct_change().shift())\n        .assign(iv_change_two_lag = lambda df: df.groupby(['symbol'])['implied_vol'].pct_change().shift(2))\n        .assign(daily_volume_rank = lambda df: df.groupby(['quotedate'])['totalvol'].rank(ascending=False))\n        .sort_values(['symbol', 'quotedate'])\n        .merge(df_stock[['symbol', 'quotedate', 'px_close']], how='left', on=['symbol', 'quotedate'])\n        .assign(daily_return = lambda df: df.groupby(['symbol'])['px_close'].pct_change())\n        .assign(scaled_return = lambda df: df.daily_return / (df.iv_one_lag / np.sqrt(252)))\n        .assign(scaled_return_one_lag = lambda df: df['scaled_return'].shift())\n        .assign(scaled_return_two_lag = lambda df: df['scaled_return'].shift(2))\n        .assign(rank_one_lag = lambda df: df.groupby(['symbol'])['daily_volume_rank'].shift())\n        .assign(rank_two_lag = lambda df: df.groupby(['symbol'])['daily_volume_rank'].shift(2))\n        .assign(rank_change = lambda df: df.groupby(['symbol'])['daily_volume_rank'].diff())\n        .assign(rank_change_one_lag = lambda df: df.groupby(['symbol'])['rank_change'].shift())\n        .assign(rank_change_two_lag = lambda df: df.groupby(['symbol'])['rank_change'].shift(2))\n        .dropna()\n     )\ndf_train\n\n\n\n\n\n\n\n\nsymbol\nvolume_rank\nquotedate\nimplied_vol\ntotalvol\ntotaloi\niv_one_lag\niv_change_one_lag\niv_change_two_lag\ndaily_volume_rank\npx_close\ndaily_return\nscaled_return\nscaled_return_one_lag\nscaled_return_two_lag\nrank_one_lag\nrank_two_lag\nrank_change\nrank_change_one_lag\nrank_change_two_lag\n\n\n\n\n3\nACAD\n57.0\n2017-01-06\n0.8471\n4184\n120009\n0.7940\n-0.075992\n0.097165\n65.5\n32.64\n0.030303\n0.605851\n-0.150381\n2.233300\n73.0\n7.0\n-7.5\n66.0\n-103.0\n\n\n4\nACAD\n57.0\n2017-01-09\n0.7463\n2541\n118250\n0.8471\n0.066877\n-0.075992\n97.0\n32.69\n0.001532\n0.028707\n0.605851\n-0.150381\n65.5\n73.0\n31.5\n-7.5\n66.0\n\n\n5\nACAD\n57.0\n2017-01-10\n0.8257\n2145\n119307\n0.7463\n-0.118994\n0.066877\n116.0\n31.47\n-0.037320\n-0.793838\n0.028707\n0.605851\n97.0\n65.5\n19.0\n31.5\n-7.5\n\n\n6\nACAD\n57.0\n2017-01-11\n0.7765\n3366\n120644\n0.8257\n0.106392\n-0.118994\n62.0\n29.87\n-0.050842\n-0.977465\n-0.793838\n0.028707\n116.0\n97.0\n-54.0\n19.0\n31.5\n\n\n7\nACAD\n57.0\n2017-01-12\n0.7953\n5229\n122426\n0.7765\n-0.059586\n0.106392\n32.0\n31.78\n0.063944\n1.307245\n-0.977465\n-0.793838\n62.0\n116.0\n-30.0\n-54.0\n19.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n75295\nZTS\n249.0\n2017-12-22\n0.1581\n2843\n89209\n0.1865\n0.085565\n0.216714\n79.0\n71.99\n-0.004012\n-0.341508\n-0.546451\n-0.138988\n29.0\n19.0\n50.0\n10.0\n-200.0\n\n\n75296\nZTS\n249.0\n2017-12-26\n0.1485\n773\n89968\n0.1581\n-0.152279\n0.085565\n202.0\n72.34\n0.004862\n0.488162\n-0.341508\n-0.546451\n79.0\n29.0\n123.0\n50.0\n10.0\n\n\n75297\nZTS\n249.0\n2017-12-27\n0.1490\n780\n90270\n0.1485\n-0.060721\n-0.152279\n192.0\n72.45\n0.001521\n0.162550\n0.488162\n-0.341508\n202.0\n79.0\n-10.0\n123.0\n50.0\n\n\n75298\nZTS\n249.0\n2017-12-28\n0.1484\n718\n90161\n0.1490\n0.003367\n-0.060721\n199.5\n72.39\n-0.000828\n-0.088232\n0.162550\n0.488162\n192.0\n202.0\n7.5\n-10.0\n123.0\n\n\n75299\nZTS\n249.0\n2017-12-29\n0.1491\n808\n89855\n0.1484\n-0.004027\n0.003367\n212.0\n72.04\n-0.004835\n-0.517197\n-0.088232\n0.162550\n199.5\n192.0\n12.5\n7.5\n-10.0\n\n\n\n\n74400 rows × 20 columns\n\n\n\nWe will do the same feature construction for the backtest period, but the filtering will have different dates.\n\ndf_test = \\\n    (\n    df_universe[['symbol','rerank']]\n        .rename(columns={'rerank':'volume_rank'})\n        .merge(df_stats_analysis, how='left', on=['symbol'])\n        .query('quotedate &gt; \"2017-12-31\"')\n        .assign(iv_one_lag = lambda df: df.groupby(['symbol'])['implied_vol'].shift())\n        .assign(iv_change_one_lag = lambda df: df.groupby(['symbol'])['implied_vol'].pct_change().shift())\n        .assign(iv_change_two_lag = lambda df: df.groupby(['symbol'])['implied_vol'].pct_change().shift(2))\n        .assign(daily_volume_rank = lambda df: df.groupby(['quotedate'])['totalvol'].rank(ascending=False))\n        .sort_values(['symbol', 'quotedate'])\n        .merge(df_stock[['symbol', 'quotedate', 'px_close']], how='left', on=['symbol', 'quotedate'])\n        .assign(daily_return = lambda df: df.groupby(['symbol'])['px_close'].pct_change())\n        .assign(scaled_return = lambda df: df.daily_return / (df.iv_one_lag / np.sqrt(252)))\n        .assign(scaled_return_one_lag = lambda df: df['scaled_return'].shift())\n        .assign(scaled_return_two_lag = lambda df: df['scaled_return'].shift(2))\n        .assign(rank_one_lag = lambda df: df.groupby(['symbol'])['daily_volume_rank'].shift())\n        .assign(rank_two_lag = lambda df: df.groupby(['symbol'])['daily_volume_rank'].shift(2))\n        .assign(rank_change = lambda df: df.groupby(['symbol'])['daily_volume_rank'].diff())\n        .assign(rank_change_one_lag = lambda df: df.groupby(['symbol'])['rank_change'].shift())\n        .assign(rank_change_two_lag = lambda df: df.groupby(['symbol'])['rank_change'].shift(2))\n        .dropna()\n     )\ndf_test\n\n\n\n\n\n\n\n\nsymbol\nvolume_rank\nquotedate\nimplied_vol\ntotalvol\ntotaloi\niv_one_lag\niv_change_one_lag\niv_change_two_lag\ndaily_volume_rank\npx_close\ndaily_return\nscaled_return\nscaled_return_one_lag\nscaled_return_two_lag\nrank_one_lag\nrank_two_lag\nrank_change\nrank_change_one_lag\nrank_change_two_lag\n\n\n\n\n3\nACAD\n57.0\n2018-01-05\n0.5870\n650\n59158\n0.5443\n0.073147\n0.047717\n255.0\n28.90\n0.004170\n0.121605\n-0.774238\n-1.022725\n60.0\n142.0\n195.0\n-82.0\n121.0\n\n\n4\nACAD\n57.0\n2018-01-08\n0.6228\n15210\n58518\n0.5870\n0.078449\n0.073147\n7.0\n27.11\n-0.061938\n-1.675010\n0.121605\n-0.774238\n255.0\n60.0\n-248.0\n195.0\n-82.0\n\n\n5\nACAD\n57.0\n2018-01-09\n0.6016\n2152\n68583\n0.6228\n0.060988\n0.078449\n153.0\n28.29\n0.043526\n1.109441\n-1.675010\n0.121605\n7.0\n255.0\n146.0\n-248.0\n195.0\n\n\n6\nACAD\n57.0\n2018-01-10\n0.5761\n798\n65129\n0.6016\n-0.034040\n0.060988\n237.0\n28.63\n0.012018\n0.317131\n1.109441\n-1.675010\n153.0\n7.0\n84.0\n146.0\n-248.0\n\n\n7\nACAD\n57.0\n2018-01-11\n0.5355\n1081\n65813\n0.5761\n-0.042387\n-0.034040\n236.0\n27.90\n-0.025498\n-0.702593\n0.317131\n1.109441\n237.0\n153.0\n-1.0\n84.0\n146.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6295\nZTS\n249.0\n2018-01-25\n0.2039\n1662\n24055\n0.2025\n0.006962\n0.021331\n177.0\n79.25\n0.011745\n0.920736\n0.752860\n0.114461\n203.0\n227.0\n-26.0\n-24.0\n-5.5\n\n\n6296\nZTS\n249.0\n2018-01-26\n0.2046\n4137\n25213\n0.2039\n0.006914\n0.006962\n85.0\n80.09\n0.010599\n0.825207\n0.920736\n0.752860\n177.0\n203.0\n-92.0\n-26.0\n-24.0\n\n\n6297\nZTS\n249.0\n2018-01-29\n0.2426\n3662\n26275\n0.2046\n0.003433\n0.006914\n96.0\n79.18\n-0.011362\n-0.881572\n0.825207\n0.920736\n85.0\n177.0\n11.0\n-92.0\n-26.0\n\n\n6298\nZTS\n249.0\n2018-01-30\n0.2604\n365\n29722\n0.2426\n0.185728\n0.003433\n276.0\n78.35\n-0.010482\n-0.685918\n-0.881572\n0.825207\n96.0\n85.0\n180.0\n11.0\n-92.0\n\n\n6299\nZTS\n249.0\n2018-01-31\n0.2419\n1333\n29890\n0.2604\n0.073372\n0.185728\n184.0\n76.77\n-0.020166\n-1.229355\n-0.685918\n-0.881572\n276.0\n96.0\n-92.0\n180.0\n11.0\n\n\n\n\n5400 rows × 20 columns\n\n\n\nLet’s export both the training data and backtest data to CSVs for future use.\n\ndf_train.to_csv('../data/option_train_2017.csv', index=False)\ndf_test.to_csv('../data/option_test_2018.csv', index=False)"
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#exploratory-data-analysis-of-training-data",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#exploratory-data-analysis-of-training-data",
    "title": "33  Option Volume: Introduction",
    "section": "33.6 Exploratory Data Analysis of Training Data",
    "text": "33.6 Exploratory Data Analysis of Training Data\nLet’s perform some exploratory data analysis on our training data.\nOur first observation is that there is a high correlation between an underlying’s previous day rank and its current rank, which makes intuitive sense.\n\ndf_train[['rank_one_lag', 'daily_volume_rank']].corr()\n\n\n\n\n\n\n\n\nrank_one_lag\ndaily_volume_rank\n\n\n\n\nrank_one_lag\n1.000000\n0.559952\n\n\ndaily_volume_rank\n0.559952\n1.000000\n\n\n\n\n\n\n\nNext, we can see that there is a fair amount of mean reversion in ranks, which means if the rank increased yesterday, it is likely to decrease today, and vice versa.\n\ndf_train.plot(kind='scatter', x='rank_change_one_lag', y='rank_change');\n\n\n\n\n\ndf_train[['rank_change_one_lag', 'rank_change']].corr()\n\n\n\n\n\n\n\n\nrank_change_one_lag\nrank_change\n\n\n\n\nrank_change_one_lag\n1.000000\n-0.434256\n\n\nrank_change\n-0.434256\n1.000000\n\n\n\n\n\n\n\nSince the correlation is stronger for rank than it is for rank-change, so we will perform predictive modeling on rank."
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#top-n-ratio",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#top-n-ratio",
    "title": "33  Option Volume: Introduction",
    "section": "33.7 Top-\\(n\\) Ratio",
    "text": "33.7 Top-\\(n\\) Ratio\nUltimately, we don’t care if we get the volume rank predictions exactly right. For example, if we correctly guess the top 10 underlyings, we don’t care if we get the ordering right.\nFor this reason, let’s consider the top-\\(n\\) ratio metric, which compares the ratio of our predicted top-\\(n\\) ranked volume, and the actual top-\\(n\\) ranked volume. Note that this ratio is always less that or equal to 1.\nWe’ll begin by creating a function that calculates the top-\\(n\\) volume for all the days in our backtest period.\n\ndef top_n_volume(n):\n    df_test = pd.read_csv(\"../data/option_test_2018.csv\")\n    df_top_n_volume = \\\n    (\n    df_test\n        .query('daily_volume_rank &lt;= @n')\n        .groupby(['quotedate'])[['totalvol']].sum()\n        .reset_index()\n        .rename(columns={'totalvol':'top_' + str(n) + '_volume'})\n    )\n    return(df_top_n_volume)\n\nLet’s examine the output of this function.\n\ntop_n_volume(25)\n\n\n\n\n\n\n\n\nquotedate\ntop_25_volume\n\n\n\n\n0\n2018-01-05\n659260\n\n\n1\n2018-01-08\n516046\n\n\n2\n2018-01-09\n543219\n\n\n3\n2018-01-10\n598589\n\n\n4\n2018-01-11\n559668\n\n\n5\n2018-01-12\n634138\n\n\n6\n2018-01-16\n484750\n\n\n7\n2018-01-17\n711695\n\n\n8\n2018-01-18\n510736\n\n\n9\n2018-01-19\n735554\n\n\n10\n2018-01-22\n508607\n\n\n11\n2018-01-23\n580468\n\n\n12\n2018-01-24\n601835\n\n\n13\n2018-01-25\n524369\n\n\n14\n2018-01-26\n547504\n\n\n15\n2018-01-29\n511379\n\n\n16\n2018-01-30\n493253\n\n\n17\n2018-01-31\n499669"
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#a-simple-rule-based-predictor",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#a-simple-rule-based-predictor",
    "title": "33  Option Volume: Introduction",
    "section": "33.8 A Simple Rule-Based Predictor",
    "text": "33.8 A Simple Rule-Based Predictor\nRecall that there is a high degree of correlation between yesterday’s volume rank and today’s volume rank. This observation leads to the following rules based predictor: tomorrow’s volume rank is equal to today’s. In a later section we will compare the performance of this simple rule-based predictor with linear regression on two features."
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#user-defined-functions",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#user-defined-functions",
    "title": "33  Option Volume: Introduction",
    "section": "33.9 User Defined Functions",
    "text": "33.9 User Defined Functions\nIn order to evaluate predictors with top-\\(n\\) ratio we will need the following two function functions.\nThe calc_top_n_ratio() function calculates the top-\\(n\\) ratio for a given fitted model, for a given trade_date.\n\ndef calc_top_n_ratio(n, trade_date, df_test, model=None, features=[]):\n    \n    # grabbing top-n volume for each day in backtest\n    df_top_n = top_n_volume(n)\n    \n    # grabbing feature observations for trade_date\n    df_prediction = df_test.query('quotedate == @trade_date').copy()\n    \n    # selecting features from df_X\n    df_X = df_prediction[features]\n    \n    # calculating model predictions\n    if model is not None:\n        df_prediction['prediction'] = model.predict(df_X) # predictions base on model\n    else:\n        df_prediction['prediction'] = df_prediction['rank_one_lag'] # simple-rule based\n    \n    # sorting by predicted rank\n    df_prediction = df_prediction.sort_values(['prediction'])\n    # calculating predicted top-n volume\n    predicted_top_n_volume = df_prediction.head(n)['totalvol'].sum()\n    # querying for actual top-n volume\n    actual_top_n_volume = df_top_n.query('quotedate == @trade_date')['top_' + str(n) + '_volume'].values[0]\n    \n    # return the top-n-ratio\n    return(predicted_top_n_volume / actual_top_n_volume)\n\nThe backtest() function takes a fitted model and loops through all the trade_dates in the backtest period and uses the calc_top_n_ratio() function to calculate the all the top-\\(n\\) ratios.\n\ndef backtest(n, df_test, model=None, features=[]):\n    # all trade dates in backtest period\n    trade_dates = df_test['quotedate'].unique().tolist()\n    \n    # calculating all top-n ratios\n    top_n_ratios = []\n    for ix_trade_date in trade_dates:\n        top_n_ratios.append(calc_top_n_ratio(n, ix_trade_date, df_test, model, features))\n\n    # creating a dataframe of daily top-n ratios\n    df_daily = pd.DataFrame({\n        'trade_date':trade_dates,\n        'top_'+str(n)+'_volume': np.round(top_n_ratios, 3),\n    })\n\n    # calculating summary statsics of top-n ratios during backtest period\n    df_stats = pd.DataFrame({\n        'model':[str(model)],\n        'average':[np.mean(top_n_ratios).round(3)],\n        'std_dev':[np.std(top_n_ratios).round(3)],\n        'minimum':[np.min(top_n_ratios).round(3)],\n        'maximum':[np.max(top_n_ratios).round(3)],\n    })\n\n    return([df_daily, df_stats])"
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#calculating-performance-of-simple-rule-based-predictor",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#calculating-performance-of-simple-rule-based-predictor",
    "title": "33  Option Volume: Introduction",
    "section": "33.10 Calculating Performance of Simple Rule-Based Predictor",
    "text": "33.10 Calculating Performance of Simple Rule-Based Predictor\nWe can now use the functions defined above to calculate the performance of the simple rule-based predictor.\nAs we can see, a simple rule-based scheme of simply guessing that volume rank remains unchanged has an average top-25 ratio of 59%.\n\nbacktest(25, df_test)\n\n[    trade_date  top_25_volume\n 0   2018-01-05          0.768\n 1   2018-01-08          0.556\n 2   2018-01-09          0.624\n 3   2018-01-10          0.467\n 4   2018-01-11          0.678\n 5   2018-01-12          0.504\n 6   2018-01-16          0.591\n 7   2018-01-17          0.516\n 8   2018-01-18          0.419\n 9   2018-01-19          0.610\n 10  2018-01-22          0.675\n 11  2018-01-23          0.562\n 12  2018-01-24          0.550\n 13  2018-01-25          0.563\n 14  2018-01-26          0.722\n 15  2018-01-29          0.592\n 16  2018-01-30          0.525\n 17  2018-01-31          0.753,\n   model  average  std_dev  minimum  maximum\n 0  None    0.593    0.094    0.419    0.768]"
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#simple-linear-regression",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#simple-linear-regression",
    "title": "33  Option Volume: Introduction",
    "section": "33.11 Simple Linear Regression",
    "text": "33.11 Simple Linear Regression\nLet’s fit a linear regression with rank_one_lag and rank_two_lag and then see if this model does better.\n\nfeatures = ['rank_one_lag', 'rank_two_lag']\nfrom sklearn.linear_model import LinearRegression\ndf_features = df_train[features]\ndf_label = df_train[['daily_volume_rank']]\nlinear_regression = LinearRegression()\nlinear_regression.fit(df_features, np.ravel(df_label.values))\nlinear_regression.score(df_features, df_label)\n\n0.3652412348607029\n\n\n\nbacktest(25, df_test, linear_regression, features)\n\n[    trade_date  top_25_volume\n 0   2018-01-05          0.816\n 1   2018-01-08          0.634\n 2   2018-01-09          0.651\n 3   2018-01-10          0.531\n 4   2018-01-11          0.734\n 5   2018-01-12          0.500\n 6   2018-01-16          0.620\n 7   2018-01-17          0.461\n 8   2018-01-18          0.478\n 9   2018-01-19          0.610\n 10  2018-01-22          0.696\n 11  2018-01-23          0.580\n 12  2018-01-24          0.584\n 13  2018-01-25          0.620\n 14  2018-01-26          0.709\n 15  2018-01-29          0.605\n 16  2018-01-30          0.736\n 17  2018-01-31          0.667,\n                 model  average  std_dev  minimum  maximum\n 0  LinearRegression()    0.624    0.092    0.461    0.816]\n\n\nClearly the improvement is only modest. The average top-25 ratio of the linear regression is 3-points higher. Additionally, the standard deviation is slightly lower, and the min and max are both closer to 100%."
  },
  {
    "objectID": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#import-packages",
    "href": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#import-packages",
    "title": "34  Option Volume: Feature Selection and Hyperparameter Tuning",
    "section": "34.1 Import Packages",
    "text": "34.1 Import Packages\nLet’s begin by importing the packages that we will need.\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#reading-in-data",
    "href": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#reading-in-data",
    "title": "34  Option Volume: Feature Selection and Hyperparameter Tuning",
    "section": "34.2 Reading-In Data",
    "text": "34.2 Reading-In Data\nNext, let’s read-in our training data and backtest data.\n\ndf_train = pd.read_csv('../data/option_train_2017.csv')\ndf_test = pd.read_csv('../data/option_test_2018.csv')"
  },
  {
    "objectID": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#starting-features",
    "href": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#starting-features",
    "title": "34  Option Volume: Feature Selection and Hyperparameter Tuning",
    "section": "34.3 Starting Features",
    "text": "34.3 Starting Features\nWe will start with the following four features.\n\nfeatures = ['iv_change_one_lag', 'scaled_return_one_lag', 'rank_one_lag', 'rank_change_one_lag']"
  },
  {
    "objectID": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#linear-regression",
    "href": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#linear-regression",
    "title": "34  Option Volume: Feature Selection and Hyperparameter Tuning",
    "section": "34.4 Linear Regression",
    "text": "34.4 Linear Regression\nWe can now run a linear regression with these features.\n\nfrom sklearn.linear_model import LinearRegression\ndf_features = df_train[features]\ndf_label = df_train[['daily_volume_rank']]\nlinear_regression = LinearRegression()\nlinear_regression.fit(df_features, np.ravel(df_label.values))\nlinear_regression.score(df_features, df_label)\n\n0.3654074977254579\n\n\nLet’s check the parameters and see if they make intuitive sense.\n\ndf_linear_regression_coefficients = \\\n    pd.DataFrame({\n        'feature':features,\n        'coefficient':linear_regression.coef_\n    })\ndf_linear_regression_coefficients\n\n\n\n\n\n\n\n\nfeature\ncoefficient\n\n\n\n\n0\niv_change_one_lag\n0.010543\n\n\n1\nscaled_return_one_lag\n-1.208765\n\n\n2\nrank_one_lag\n0.680732\n\n\n3\nrank_change_one_lag\n-0.274649\n\n\n\n\n\n\n\nInterpretation: 1. iv_change - a positive change in implied vol could be caused by supply/demand effects of increased option buying, which could carry through to the following day 1. scaled_return - when a stock goes down, long positions in the stock get fearful (or greedy) and option buying increases 1. rank_one_lag - if an underlying has high rank one day, it will likely have high rank the next day 1. rank_change_one_lag - if an underlying has a jump in volume one day, it will usually revert back to previous levels the next day"
  },
  {
    "objectID": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#feature-selection-using-lasso",
    "href": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#feature-selection-using-lasso",
    "title": "34  Option Volume: Feature Selection and Hyperparameter Tuning",
    "section": "34.5 Feature Selection Using Lasso",
    "text": "34.5 Feature Selection Using Lasso\nLasso regression is a linear regression technique that minimizes an objective function that involves residual-sum-of-squares and also the magnitude of the regression coefficients.\nIn particular, it penalizes the objective for the collective magnitude of the regression coefficients. This has the effect of making the coefficients of the non-predictive features equal to zero.\nThus, lasso regression can be a way of weeding out non-predictive coefficients.\nLet’s next fit a lasso regression to our four features.\n\nfrom sklearn.linear_model import Lasso\ndf_features = df_train[features]\ndf_label = df_train[['daily_volume_rank']]\nlasso = Lasso(alpha=0.10)\nlasso.fit(df_features, np.ravel(df_label.values))\nlasso.score(df_features, df_label)\n\n0.3654059332222376\n\n\nWe can now examine the coefficients. Notice that iv_change_one_lag has a value of 0, and thus it is not that predictive.\n\ndf_lasso_coefficients = \\\n    pd.DataFrame({\n        'feature':features,\n        'coefficient':lasso.coef_\n    })\ndf_lasso_coefficients\n\n\n\n\n\n\n\n\nfeature\ncoefficient\n\n\n\n\n0\niv_change_one_lag\n0.000000\n\n\n1\nscaled_return_one_lag\n-1.091635\n\n\n2\nrank_one_lag\n0.680723\n\n\n3\nrank_change_one_lag\n-0.274577\n\n\n\n\n\n\n\nThe alpha hyperparameter controls how much the coefficient sizes are penalized. We can use cross-validation to choose the optimal level of alpha.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import Lasso\ndf_features = df_train[features]\ndf_label = df_train[['daily_volume_rank']]\nalphas = np.linspace(0.1, 1, 10)\nfor ix_alpha in alphas:\n   lasso = Lasso(alpha=ix_alpha)\n   cvs = cross_val_score(lasso, df_features, df_label, cv = 10)\n   print(np.round(ix_alpha, 2), cvs.mean())\n\n0.1 0.3565912774339949\n0.2 0.35658669987577574\n0.3 0.35657893041868144\n0.4 0.35656800276703626\n0.5 0.35655390432426176\n0.6 0.35653663509035816\n0.7 0.35651619506532517\n0.8 0.3564925842491631\n0.9 0.3564673297611635\n1.0 0.3564481756197352\n\n\nIn our case, the value of alpha doesn’t seem to matter that much. So we’ll leave it as is."
  },
  {
    "objectID": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#selecting-predictive-features",
    "href": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#selecting-predictive-features",
    "title": "34  Option Volume: Feature Selection and Hyperparameter Tuning",
    "section": "34.6 Selecting Predictive Features",
    "text": "34.6 Selecting Predictive Features\nWe can remove iv_change_one_lag as our lasso regression showed that it has low predictive power.\n\nfeatures = ['scaled_return_one_lag', 'rank_one_lag', 'rank_change_one_lag']"
  },
  {
    "objectID": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#user-defined-functions",
    "href": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#user-defined-functions",
    "title": "34  Option Volume: Feature Selection and Hyperparameter Tuning",
    "section": "34.7 User Defined Functions",
    "text": "34.7 User Defined Functions\nLet’s create the user defined functions we will need to use our top-\\(n\\) metric in our backtest. These functions were introduced in the previous chapter.\n\ndef top_n_volume(n):\n    df_test = pd.read_csv(\"../data/option_test_2018.csv\")\n    df_top_n_volume = \\\n    (\n    df_test\n        .query('daily_volume_rank &lt;= @n')\n        .groupby(['quotedate'])[['totalvol']].sum()\n        .reset_index()\n        .rename(columns={'totalvol':'top_' + str(n) + '_volume'})\n    )\n    return(df_top_n_volume)\n\n\ndef calc_top_n_ratio(n, trade_date, df_test, model=None, features=[]):\n    \n    # grabbing top-n volume for each day in backtest\n    df_top_n = top_n_volume(n)\n    \n    # grabbing feature observations for trade_date\n    df_prediction = df_test.query('quotedate == @trade_date').copy()\n    \n    # selecting features from df_X\n    df_X = df_prediction[features]\n    \n    # calculating model predictions\n    if model is not None:\n        df_prediction['prediction'] = model.predict(df_X) # predictions based on model\n    else:\n        df_prediction['prediction'] = df_prediction['rank_one_lag'] # simple-rule based predictor\n    \n    # sorting by predicted rank\n    df_prediction = df_prediction.sort_values(['prediction'])\n    # calculating predicted top-n volume\n    predicted_top_n_volume = df_prediction.head(n)['totalvol'].sum()\n    # querying for actual top-n volume\n    actual_top_n_volume = df_top_n.query('quotedate == @trade_date')['top_' + str(n) + '_volume'].values[0]\n    \n    # return the top-n-ratio\n    return(predicted_top_n_volume / actual_top_n_volume)\n\n\ndef backtest(n, df_test, model=None, features=[]):\n    # all trade dates in backtest period\n    trade_dates = df_test['quotedate'].unique().tolist()\n    \n    # calculating all top-n ratios\n    top_n_ratios = []\n    for ix_trade_date in trade_dates:\n        top_n_ratios.append(calc_top_n_ratio(n, ix_trade_date, df_test, model, features))\n\n    # creating a dataframe of daily top-n ratios\n    df_daily = pd.DataFrame({\n        'trade_date':trade_dates,\n        'top_'+str(n)+'_volume': np.round(top_n_ratios, 3),\n    })\n\n    # calculating summary statsics of top-n ratios during backtest period\n    df_stats = pd.DataFrame({\n        'model':[str(model)],\n        'average':[np.mean(top_n_ratios).round(3)],\n        'std_dev':[np.std(top_n_ratios).round(3)],\n        'minimum':[np.min(top_n_ratios).round(3)],\n        'maximum':[np.max(top_n_ratios).round(3)],\n    })\n\n    return([df_daily, df_stats])"
  },
  {
    "objectID": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#k-nearest-neighbors",
    "href": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#k-nearest-neighbors",
    "title": "34  Option Volume: Feature Selection and Hyperparameter Tuning",
    "section": "34.8 K Nearest Neighbors",
    "text": "34.8 K Nearest Neighbors\nIn this section we’ll fit a KNeighborsRegressor to our training data and see how it performs during the backtest period.\nFirst, let’s use a 10-fold cross validation (using \\(R^2\\) as metric) to determine optimal value of n_neighbors.\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import cross_val_score\ndf_features = df_train[features]\ndf_label = df_train[['daily_volume_rank']]\nk = range(100, 1100, 100)\nfor ix_k in k:\n    knn = KNeighborsRegressor(n_neighbors=ix_k)\n    cvs = cross_val_score(knn, df_features, df_label, cv = 10)\n    print(ix_k, cvs.mean())\n\n100 0.36101494817841573\n200 0.36386843260646606\n300 0.3646175979349766\n400 0.36480461178161694\n500 0.3647867229860612\n600 0.3646215082035253\n700 0.3644970083233438\n800 0.36423480801137564\n900 0.36403894419896105\n1000 0.3638359335614717\n\n\nThe model doesn’t seem particularly sensitive to the value of n_neighbors, so let’s just use 400 because it had the highest \\(R^2\\) and the run-time seems reasonable.\nNext, let’s fit a KNeighborsRegressor to the entirety of our training data use n_neighbors=400.\n\nfrom sklearn.neighbors import KNeighborsRegressor\ndf_features = df_train[features]\ndf_label = df_train[['daily_volume_rank']]\nknn = KNeighborsRegressor(n_neighbors=400)\nknn.fit(df_features, np.ravel(df_label.values))\nknn.score(df_features, df_label)\n\n0.37700389902243137\n\n\nWe can now use our fitted model to perform our backtest using top-25 ratio as our metric for success.\n\nbacktest(10, df_test, knn, features)\n\n[    trade_date  top_10_volume\n 0   2018-01-05          0.831\n 1   2018-01-08          0.577\n 2   2018-01-09          0.659\n 3   2018-01-10          0.421\n 4   2018-01-11          0.532\n 5   2018-01-12          0.309\n 6   2018-01-16          0.599\n 7   2018-01-17          0.467\n 8   2018-01-18          0.398\n 9   2018-01-19          0.625\n 10  2018-01-22          0.708\n 11  2018-01-23          0.657\n 12  2018-01-24          0.496\n 13  2018-01-25          0.616\n 14  2018-01-26          0.763\n 15  2018-01-29          0.608\n 16  2018-01-30          0.471\n 17  2018-01-31          0.690,\n                                   model  average  std_dev  minimum  maximum\n 0  KNeighborsRegressor(n_neighbors=400)    0.579    0.131    0.309    0.831]\n\n\nAs we can see, our KNN model actually performs worse than the simple rule-based predictor that we introduced in the previous chapter."
  },
  {
    "objectID": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#random-forest",
    "href": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#random-forest",
    "title": "34  Option Volume: Feature Selection and Hyperparameter Tuning",
    "section": "34.9 Random Forest",
    "text": "34.9 Random Forest\nIn this section we’ll run our backtest using a RandomForestRegressor. I’ve already run a cross-validation analysis that n_estimators=10 has a good trade off of performance and run time.\nLet’s find an optimal value of max_depth using a 10-fold cross validation.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\ndf_features = df_train[features]\ndf_label = df_train[['daily_volume_rank']]\nd = range(1, 21, 1)\nfor ix_d in d:\n    random_forest = RandomForestRegressor(n_estimators=10, max_depth=ix_d)\n    cvs = cross_val_score(random_forest, df_features, np.ravel(df_label.values), cv = 10)\n    print(ix_d, cvs.mean())\n\n1 0.21323542515349816\n2 0.28667668385608386\n3 0.32823705965447214\n4 0.350204904212642\n5 0.3604337950155293\n6 0.3643107573903573\n7 0.36517895534208716\n8 0.3638805625055838\n9 0.3627851611360925\n10 0.3582779076013644\n11 0.3539930070013009\n12 0.34701968731147276\n13 0.34101687956906235\n14 0.3326469340556921\n15 0.3248038486930812\n16 0.31443952345817633\n17 0.3059982711930008\n18 0.29436642171242966\n19 0.2866621073083838\n20 0.27945630028663293\n\n\nBased on our cross-validation analysis above, let’s use max_depth=7 to train our model.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\ndf_features = df_train[features]\ndf_label = df_train[['daily_volume_rank']]\nrandom_forest = RandomForestRegressor(n_estimators = 10, max_depth=7)\nrandom_forest.fit(df_features, np.ravel(df_label.values))\nrandom_forest.score(df_features, df_label)\n\n0.387368629069925\n\n\nWe can now run our backtest using the top-25 metric for our measure of success.\n\nbacktest(10, df_test, random_forest, features)\n\n[    trade_date  top_10_volume\n 0   2018-01-05          0.831\n 1   2018-01-08          0.577\n 2   2018-01-09          0.694\n 3   2018-01-10          0.421\n 4   2018-01-11          0.532\n 5   2018-01-12          0.338\n 6   2018-01-16          0.599\n 7   2018-01-17          0.511\n 8   2018-01-18          0.398\n 9   2018-01-19          0.625\n 10  2018-01-22          0.728\n 11  2018-01-23          0.571\n 12  2018-01-24          0.496\n 13  2018-01-25          0.661\n 14  2018-01-26          0.763\n 15  2018-01-29          0.576\n 16  2018-01-30          0.471\n 17  2018-01-31          0.690,\n                                                model  average  std_dev   \n 0  RandomForestRegressor(max_depth=7, n_estimator...    0.582    0.128  \\\n \n    minimum  maximum  \n 0    0.338    0.831  ]\n\n\nAs we can see, our random forest model also under performs relative our simple rule based model."
  },
  {
    "objectID": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#importing-packages",
    "href": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#importing-packages",
    "title": "35  Option Volume: xgboost",
    "section": "35.1 Importing Packages",
    "text": "35.1 Importing Packages\nLet’s begin by importing the packages that we will need.\n\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfrom xgboost import XGBRegressor\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#reading-in-data",
    "href": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#reading-in-data",
    "title": "35  Option Volume: xgboost",
    "section": "35.2 Reading-In Data",
    "text": "35.2 Reading-In Data\nNext, let’s read-in our training data and testing data.\n\ndf_train = pd.read_csv('../data/option_train_2017.csv')\ndf_test = pd.read_csv('../data/option_test_2018.csv')"
  },
  {
    "objectID": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#feature-selection",
    "href": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#feature-selection",
    "title": "35  Option Volume: xgboost",
    "section": "35.3 Feature Selection",
    "text": "35.3 Feature Selection\nFor this exercise we will use all the features available in our training data set.\n\nfeatures = ['iv_change_one_lag', 'iv_change_two_lag', 'scaled_return_one_lag', \n            'scaled_return_two_lag', 'rank_one_lag', 'rank_two_lag',\n            'rank_change_one_lag', 'rank_change_two_lag',]"
  },
  {
    "objectID": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#user-defined-functions",
    "href": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#user-defined-functions",
    "title": "35  Option Volume: xgboost",
    "section": "35.4 User Defined Functions",
    "text": "35.4 User Defined Functions\nIn this section we import the three custom functions that are needed to execute our backtest. These functions were introduced in a previous chapter.\n\ndef top_n_volume(n):\n    df_test = pd.read_csv(\"../data/option_test_2018.csv\")\n    df_top_n_volume = \\\n    (\n    df_test\n        .query('daily_volume_rank &lt;= @n')\n        .groupby(['quotedate'])[['totalvol']].sum()\n        .reset_index()\n        .rename(columns={'totalvol':'top_' + str(n) + '_volume'})\n    )\n    return(df_top_n_volume)\n\n\ndef calc_top_n_ratio(n, trade_date, df_test, model=None, features=[]):\n    \n    # grabbing top-n volume for each day in backtest\n    df_top_n = top_n_volume(n)\n    \n    # grabbing feature observations for trade_date\n    df_prediction = df_test.query('quotedate == @trade_date').copy()\n    \n    # selecting features from df_X\n    df_X = df_prediction[features]\n    \n    # calculating model predictions\n    if model is not None:\n        df_prediction['prediction'] = model.predict(df_X) # predictions based on model\n    else:\n        df_prediction['prediction'] = df_prediction['rank_one_lag'] # simple-rule based predictor\n    \n    # sorting by predicted rank\n    df_prediction = df_prediction.sort_values(['prediction'])\n    # calculating predicted top-n volume\n    predicted_top_n_volume = df_prediction.head(n)['totalvol'].sum()\n    # querying for actual top-n volume\n    actual_top_n_volume = df_top_n.query('quotedate == @trade_date')['top_' + str(n) + '_volume'].values[0]\n    \n    # return the top-n-ratio\n    return(predicted_top_n_volume / actual_top_n_volume)\n\n\ndef backtest(n, df_test, model=None, features=[]):\n    # all trade dates in backtest period\n    trade_dates = df_test['quotedate'].unique().tolist()\n    \n    # calculating all top-n ratios\n    top_n_ratios = []\n    for ix_trade_date in trade_dates:\n        top_n_ratios.append(calc_top_n_ratio(n, ix_trade_date, df_test, model, features))\n\n    # creating a dataframe of daily top-n ratios\n    df_daily = pd.DataFrame({\n        'trade_date':trade_dates,\n        'top_'+str(n)+'_volume': np.round(top_n_ratios, 3),\n    })\n\n    # calculating summary statistics of top-n ratios during backtest period\n    df_stats = pd.DataFrame({\n        'model':[str(model)],\n        'average':[np.mean(top_n_ratios).round(3)],\n        'std_dev':[np.std(top_n_ratios).round(3)],\n        'minimum':[np.min(top_n_ratios).round(3)],\n        'maximum':[np.max(top_n_ratios).round(3)],\n    })\n\n    return([df_daily, df_stats])"
  },
  {
    "objectID": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#hyperparameter-tuning",
    "href": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#hyperparameter-tuning",
    "title": "35  Option Volume: xgboost",
    "section": "35.5 Hyperparameter Tuning",
    "text": "35.5 Hyperparameter Tuning\nThe learning_rate is rate at which successive trees are boosted; a lower learning_rate amounts to slower learning.\nHere we use a 5-fold cross-validation to select the optimal learning_rate. We will use \\(R^2\\) as our goodness of fit metric.\n\nfrom sklearn.model_selection import cross_val_score\ndf_features = df_train[features]\ndf_label = df_train[['daily_volume_rank']]\nalphas = np.linspace(0.1, 1, 10)\nfor ix_alpha in alphas:\n   xgb_model = XGBRegressor(n_estimators=25, max_depth=3, learning_rate=ix_alpha, random_state=0)\n   cvs = cross_val_score(xgb_model, df_features, df_label, cv = 5)\n   print(np.round(ix_alpha, 2), cvs.mean())\n\n0.1 0.36246615782763447\n0.2 0.38969475875156323\n0.3 0.3910816017808643\n0.4 0.38967754013651945\n0.5 0.38808813540767895\n0.6 0.38692635332836656\n0.7 0.38474370336623864\n0.8 0.3841157442722449\n0.9 0.38194061118381856\n1.0 0.37935841896562783\n\n\nAs we can see, learning_rate = 0.3 yields the highest \\(R^2\\)."
  },
  {
    "objectID": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#fitting-model",
    "href": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#fitting-model",
    "title": "35  Option Volume: xgboost",
    "section": "35.6 Fitting Model",
    "text": "35.6 Fitting Model\nNow we are ready to fit the our model with learning_rate = 0.3. Notice that we are increasing n_estimators=500.\n\ndf_features = df_train[features]\ndf_label = df_train[['daily_volume_rank']]\nxg_model = XGBRegressor(n_estimators=500, max_depth=3, learning_rate=0.3, random_state=0)\nxg_model.fit(df_features, df_label)\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.3, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=3, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=500, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=0, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.3, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=3, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=500, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=0, ...)\n\n\n\nsklearn.metrics.r2_score(df_label, xg_model.predict(df_features))\n\n0.4621096942085411"
  },
  {
    "objectID": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#backtest",
    "href": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#backtest",
    "title": "35  Option Volume: xgboost",
    "section": "35.7 Backtest",
    "text": "35.7 Backtest\nLet’s run our backtest with the fit model.\n\nbacktest(25, df_test, xg_model, features)\n\n[    trade_date  top_25_volume\n 0   2018-01-05          0.827\n 1   2018-01-08          0.574\n 2   2018-01-09          0.694\n 3   2018-01-10          0.535\n 4   2018-01-11          0.780\n 5   2018-01-12          0.562\n 6   2018-01-16          0.584\n 7   2018-01-17          0.552\n 8   2018-01-18          0.475\n 9   2018-01-19          0.642\n 10  2018-01-22          0.717\n 11  2018-01-23          0.563\n 12  2018-01-24          0.599\n 13  2018-01-25          0.588\n 14  2018-01-26          0.868\n 15  2018-01-29          0.577\n 16  2018-01-30          0.546\n 17  2018-01-31          0.679,\n                                                model  average  std_dev   \n 0  XGBRegressor(base_score=None, booster=None, ca...    0.631    0.105  \\\n \n    minimum  maximum  \n 0    0.475    0.868  ]\n\n\nAnd we can compare our results to the simple rules based strategy.\n\nbacktest(25, df_test)\n\n[    trade_date  top_25_volume\n 0   2018-01-05          0.768\n 1   2018-01-08          0.556\n 2   2018-01-09          0.624\n 3   2018-01-10          0.467\n 4   2018-01-11          0.678\n 5   2018-01-12          0.504\n 6   2018-01-16          0.591\n 7   2018-01-17          0.516\n 8   2018-01-18          0.419\n 9   2018-01-19          0.610\n 10  2018-01-22          0.675\n 11  2018-01-23          0.562\n 12  2018-01-24          0.550\n 13  2018-01-25          0.563\n 14  2018-01-26          0.722\n 15  2018-01-29          0.592\n 16  2018-01-30          0.525\n 17  2018-01-31          0.753,\n   model  average  std_dev  minimum  maximum\n 0  None    0.593    0.094    0.419    0.768]\n\n\n\nCode Challenge: Search the documentation and find a model hyper-parameter to tune. Then see how the new model performs with that hyper-parameter set to the optimal value that you found."
  },
  {
    "objectID": "chapters/36_mnist_digit_recognition/mnist_digit_recognition.html#reading-in-data",
    "href": "chapters/36_mnist_digit_recognition/mnist_digit_recognition.html#reading-in-data",
    "title": "36  MNIST Digit Recognition",
    "section": "36.1 Reading-In Data",
    "text": "36.1 Reading-In Data\nThe MNIST dataset comes pre-loaded in keras, in the form of a set of four numpy arrays.\n\nfrom keras.datasets import mnist\n\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n\n2023-08-31 15:55:47.729038: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-08-31 15:55:47.777390: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-08-31 15:55:47.779107: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-08-31 15:55:48.608385: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\nLet’s examine the structure of of these arrays.\n\ntrain_images\n\narray([[[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       ...,\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)\n\n\n\ntrain_labels\n\narray([5, 0, 4, ..., 5, 6, 8], dtype=uint8)\n\n\ntrain_images and train_labels form the training set, the data that the model will learn from. The model will then be tested on the “test set”, test_images and test_labels. Our images are encoded as numpy arrays, and the labels are simply an array of digits, ranging from 0 to 9. There is a one-to-one correspondence between the images and the labels.\nLet’s have a look at the training data.\n\ntrain_images.shape\n\n(60000, 28, 28)\n\n\n\ntrain_labels\n\narray([5, 0, 4, ..., 5, 6, 8], dtype=uint8)\n\n\nLet’s have a look at the test data.\n\ntest_images.shape\n\n(10000, 28, 28)\n\n\n\ntest_labels\n\narray([7, 2, 1, ..., 4, 5, 6], dtype=uint8)\n\n\nOur workflow will be as follow: first we will present our neural network with the training data, train_images and train_labels. The network will then learn to associate images and labels. Finally, we will ask the network to produce predictions for test_images, and we will verify if these predictions match the labels from test_labels."
  },
  {
    "objectID": "chapters/36_mnist_digit_recognition/mnist_digit_recognition.html#data-wrangling",
    "href": "chapters/36_mnist_digit_recognition/mnist_digit_recognition.html#data-wrangling",
    "title": "36  MNIST Digit Recognition",
    "section": "36.2 Data Wrangling",
    "text": "36.2 Data Wrangling\nBefore training, we will preprocess our data by reshaping it into the shape that the network expects, and scaling it so that all values are in the [0, 1] interval. Previously, our training images for instance were stored in an array of shape (60000, 28, 28) of type uint8 with values in the [0, 255] interval. We transform it into a float32 array of shape (60000, 28 * 28) with values between 0 and 1.\n\ntrain_images = train_images.reshape((60000, 28 * 28))\ntrain_images = train_images.astype('float32') / 255\n\ntest_images = test_images.reshape((10000, 28 * 28))\ntest_images = test_images.astype('float32') / 255\n\nWe also need to one-hot encode the labels.\n\nfrom keras.utils import to_categorical\n\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)\n\n\ntrain_labels\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)\n\n\n\ntest_labels\n\narray([[0., 0., 0., ..., 1., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
  },
  {
    "objectID": "chapters/36_mnist_digit_recognition/mnist_digit_recognition.html#building-the-network",
    "href": "chapters/36_mnist_digit_recognition/mnist_digit_recognition.html#building-the-network",
    "title": "36  MNIST Digit Recognition",
    "section": "36.3 Building the Network",
    "text": "36.3 Building the Network\nIn keras, a dense neural network is constructed by instantiating a Sequential model and then adding layers to it.\n\nfrom keras import models\nfrom keras import layers\n\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\nnetwork.add(layers.Dense(10, activation='softmax'))\n\nThe core building block of neural networks is the layer, a data-processing module which you can conceive as a “filter” for data. Some data comes in, and comes out in a more useful form. Precisely, layers extract representations out of the data fed into them – hopefully representations that are more meaningful for the problem at hand. Most of deep learning really consists of chaining together simple layers which will implement a form of progressive “data distillation”. A deep learning model is like a sieve for data processing, made of a succession of increasingly refined data filters – the layers.\nHere our network consists of a sequence of two Dense layers, which are densely-connected (also called fully-connected) neural layers. The second (and last) layer is a 10-way softmax layer, which means it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 digit classes.\nTo make our network ready for training, we need to pick three more things, as part of the compilation step:\n\nA loss function: the is how the network will be able to measure how good a job it is doing on its training data, and thus how it will be able to steer itself in the right direction.\nAn optimizer: this is the mechanism through which the network will update itself based on the data it sees and its loss function.\nMetrics to monitor during training and testing. Here we will only care about accuracy (the fraction of the images that were correctly classified).\n\n\nnetwork.compile(loss='categorical_crossentropy',\n                optimizer='rmsprop',\n                metrics=['accuracy'])"
  },
  {
    "objectID": "chapters/36_mnist_digit_recognition/mnist_digit_recognition.html#fitting-and-testing-the-network",
    "href": "chapters/36_mnist_digit_recognition/mnist_digit_recognition.html#fitting-and-testing-the-network",
    "title": "36  MNIST Digit Recognition",
    "section": "36.4 Fitting and Testing the Network",
    "text": "36.4 Fitting and Testing the Network\nWe are now ready to train our network, which in keras is done via a call to the .fit() method of the network: we “fit” the model to its training data.\n\nnetwork.fit(train_images, train_labels, epochs=5, batch_size=128)\n\nEpoch 1/5\n\n\n2023-08-31 15:55:50.006682: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n\n\n469/469 [==============================] - 3s 5ms/step - loss: 0.2671 - accuracy: 0.9230\nEpoch 2/5\n469/469 [==============================] - 2s 4ms/step - loss: 0.1090 - accuracy: 0.9679\nEpoch 3/5\n469/469 [==============================] - 2s 5ms/step - loss: 0.0709 - accuracy: 0.9783\nEpoch 4/5\n469/469 [==============================] - 2s 4ms/step - loss: 0.0515 - accuracy: 0.9843\nEpoch 5/5\n469/469 [==============================] - 2s 4ms/step - loss: 0.0385 - accuracy: 0.9884\n\n\n&lt;keras.src.callbacks.History&gt;\n\n\nTwo quantities are being displayed during training: the loss of the network over the training data, and the accuracy of the network over the training data.\nWe quickly reach an accuracy of 0.989 (i.e. 98.9%) on the training data. Now let’s check that our model performs well on the test set too.\n\ntest_loss, test_acc = network.evaluate(test_images, test_labels)\n\n313/313 [==============================] - 1s 2ms/step - loss: 0.0663 - accuracy: 0.9791\n\n\n\nprint('test_acc:', test_acc)\n\ntest_acc: 0.9790999889373779\n\n\nOur test set accuracy turns out to be 97.8% – that’s quite a bit lower than the training set accuracy. This gap between training accuracy and test accuracy is an example of overfitting, the fact that machine learning models tend to perform worse on new data than on their training data."
  }
]