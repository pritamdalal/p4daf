{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Loan: Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we will use the **keras** package to predict student loan prepayments.  In particular, we will use dense feed-forward neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by importing some initial packages that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading-In Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read-in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/student_loan.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's select our features and organize our lables.  Notice that we are excluding `cosign` and `repay_status` because they are categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_features = \\\n",
    "    ['loan_age','income_annual', 'upb',              \n",
    "    'monthly_payment','fico','origbalance',\n",
    "    'mos_to_repay','mos_to_balln',]    \n",
    "df_X = df_train[lst_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = df_train['paid_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holdout Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to create a holdout set to measure out-of-sample performance.  The following code uses `train_test_split()` to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's perform a Gaussian normalization of our features so that they are all the same order of magnitude and have similar variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = X_train.mean()\n",
    "std = X_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we are scaling both the training set and testing set with the mean and standard deviation of the training set.  It is important not to normalize the test set with it's own standard deviation to avoid information leek into the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = (X_train - mu) / std\n",
    "X_test_scaled = (X_test - mu) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Random Seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting neural networks involves a lot of random number generation.  To ensure that we get reproducible results, let's create a user-defined function that sets a variety of random number generators that get used.  In order to do that we'll need to import a couple of other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 18:58:13.416320: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-31 18:58:13.455995: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-31 18:58:13.456758: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-31 18:58:14.227606: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit our initial neural network.  Let's begin by importing some of the functions we will need from **keras** and **sklearn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set the random seeds with our user-defined function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to construct the model.  We instantiate it with the `Sequential()` constructor and then add two hidden layers with 16 and 8 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=16, input_dim=len(df_X.columns), activation='relu'))\n",
    "model.add(Dense(units=8, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning process is defined in the compilation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the construction and compilation are complete we are ready for the actual learning/fitting to happen.  (I played with the class weights until I got reasonable results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3261/3261 [==============================] - 5s 1ms/step - loss: 0.0971 - accuracy: 0.9838\n",
      "Epoch 2/10\n",
      "3261/3261 [==============================] - 4s 1ms/step - loss: 0.0770 - accuracy: 0.9858\n",
      "Epoch 3/10\n",
      "3261/3261 [==============================] - 4s 1ms/step - loss: 0.0739 - accuracy: 0.9866\n",
      "Epoch 4/10\n",
      "3261/3261 [==============================] - 4s 1ms/step - loss: 0.0726 - accuracy: 0.9869\n",
      "Epoch 5/10\n",
      "3261/3261 [==============================] - 4s 1ms/step - loss: 0.0719 - accuracy: 0.9870\n",
      "Epoch 6/10\n",
      "3261/3261 [==============================] - 4s 1ms/step - loss: 0.0714 - accuracy: 0.9871\n",
      "Epoch 7/10\n",
      "3261/3261 [==============================] - 4s 1ms/step - loss: 0.0711 - accuracy: 0.9872\n",
      "Epoch 8/10\n",
      "3261/3261 [==============================] - 4s 1ms/step - loss: 0.0707 - accuracy: 0.9872\n",
      "Epoch 9/10\n",
      "3261/3261 [==============================] - 4s 1ms/step - loss: 0.0704 - accuracy: 0.9873\n",
      "Epoch 10/10\n",
      "3261/3261 [==============================] - 4s 1ms/step - loss: 0.0701 - accuracy: 0.9873\n",
      "CPU times: user 1min 2s, sys: 3.79 s, total: 1min 5s\n",
      "Wall time: 43.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train_scaled, y_train, epochs=10, verbose=True, batch_size=256, class_weight={0:1, 1:1.25});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `.evaluate()` method of the model to check the out-of-sample accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6521/6521 [==============================] - 6s 857us/step - loss: 0.0586 - accuracy: 0.9874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0585552453994751, 0.9873527884483337]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's check the out-of-sample precision, recall, and f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6521/6521 [==============================] - 4s 618us/step\n",
      "F1:        0.37920489296636084\n",
      "Precision: 0.9361207897793263\n",
      "Recall:    0.2377581120943953\n"
     ]
    }
   ],
   "source": [
    "test_predictions = np.where(model.predict(X_test_scaled) > 0.5, 1, 0)\n",
    "print(\"F1:       \", f1_score(y_test, test_predictions))\n",
    "print(\"Precision:\", precision_score(y_test, test_predictions))\n",
    "print(\"Recall:   \", recall_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the ratio of predicted number of prepayments to the actual number of prepayments.  As you can see, the model only predicts 25% the number of actual prepayments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25398230088495577\n"
     ]
    }
   ],
   "source": [
    "print(test_predictions.sum() / y_test.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check the expected balance ratio: it is about 118%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6521/6521 [==============================] - 4s 652us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1768956596208677"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_test['upb'] * np.ravel(model.predict(X_test_scaled))) / np.sum(X_test['upb'] * y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we implement drop-out regularization.  We begin by first resetting the random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the construction of our network, we add a `Dropout` layer after each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(units=16, input_dim=len(df_X.columns), activation='relu'))\n",
    "model.add(Dropout(rate=0.3, seed=0))\n",
    "model.add(Dense(units=8, activation='relu'))\n",
    "model.add(Dropout(rate=0.3, seed=0))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we fit the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3261/3261 [==============================] - 5s 1ms/step - loss: 0.1176 - accuracy: 0.9826\n",
      "Epoch 2/10\n",
      "3261/3261 [==============================] - 5s 1ms/step - loss: 0.0916 - accuracy: 0.9839\n",
      "Epoch 3/10\n",
      "3261/3261 [==============================] - 5s 1ms/step - loss: 0.0851 - accuracy: 0.9848\n",
      "Epoch 4/10\n",
      "3261/3261 [==============================] - 5s 2ms/step - loss: 0.0823 - accuracy: 0.9853\n",
      "Epoch 5/10\n",
      "3261/3261 [==============================] - 5s 1ms/step - loss: 0.0810 - accuracy: 0.9855\n",
      "Epoch 6/10\n",
      "3261/3261 [==============================] - 5s 1ms/step - loss: 0.0804 - accuracy: 0.9856\n",
      "Epoch 7/10\n",
      "3261/3261 [==============================] - 5s 1ms/step - loss: 0.0802 - accuracy: 0.9856\n",
      "Epoch 8/10\n",
      "3261/3261 [==============================] - 5s 1ms/step - loss: 0.0798 - accuracy: 0.9856\n",
      "Epoch 9/10\n",
      "3261/3261 [==============================] - 5s 2ms/step - loss: 0.0793 - accuracy: 0.9858\n",
      "Epoch 10/10\n",
      "3261/3261 [==============================] - 5s 2ms/step - loss: 0.0790 - accuracy: 0.9859\n",
      "CPU times: user 1min 11s, sys: 4.42 s, total: 1min 16s\n",
      "Wall time: 49.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train_scaled, y_train, epochs=10, verbose=True, batch_size=256, class_weight={0:1, 1:1.25});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the out-of-sample measures of fit.  Dropout regularization doesn't seem to improve these metrics - in fact, the F1 score decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6521/6521 [==============================] - 4s 680us/step\n",
      "F1:        0.31407407407407406\n",
      "Precision: 0.9636363636363636\n",
      "Recall:    0.18761061946902655\n"
     ]
    }
   ],
   "source": [
    "test_predictions = np.where(model.predict(X_test_scaled) > 0.5, 1, 0)\n",
    "print(\"F1:       \", f1_score(y_test, test_predictions))\n",
    "print(\"Precision:\", precision_score(y_test, test_predictions))\n",
    "print(\"Recall:   \", recall_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a further reduction in the absolute number of prepayments that are predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6521/6521 [==============================] - 5s 690us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19469026548672566"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = np.where(model.predict(X_test_scaled) > 0.5, 1, 0)\n",
    "test_predictions.sum() / y_test.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected loan balance ratio increases to 130%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6521/6521 [==============================] - 5s 692us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.302716041226891"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_test['upb'] * np.ravel(model.predict(X_test_scaled))) / np.sum(X_test['upb'] * y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, dropout regularization doesn't seem to improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we implement ridge (`l2`) regularization.  (I tried lasso regularization and it was an epic fail)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement `l2` regularization by populating the `activity_regularizer` argument of the `Dense()` layer constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "model = Sequential()\n",
    "model.add(Dense(units=16, input_dim=len(df_X.columns), activation='relu', activity_regularizer=l2(0.0005)))\n",
    "model.add(Dense(units=8, activation='relu', activity_regularizer=l2(0.0005)))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3261/3261 [==============================] - 5s 1ms/step - loss: 0.1009 - accuracy: 0.9839\n",
      "Epoch 2/10\n",
      "3261/3261 [==============================] - 4s 1ms/step - loss: 0.0783 - accuracy: 0.9862\n",
      "Epoch 3/10\n",
      "3261/3261 [==============================] - 4s 1ms/step - loss: 0.0745 - accuracy: 0.9869\n",
      "Epoch 4/10\n",
      "3261/3261 [==============================] - 4s 1ms/step - loss: 0.0725 - accuracy: 0.9871\n",
      "Epoch 5/10\n",
      "3261/3261 [==============================] - 4s 1ms/step - loss: 0.0712 - accuracy: 0.9872\n",
      "Epoch 6/10\n",
      "3261/3261 [==============================] - 4s 1ms/step - loss: 0.0703 - accuracy: 0.9874\n",
      "Epoch 7/10\n",
      "3261/3261 [==============================] - 4s 1ms/step - loss: 0.0697 - accuracy: 0.9876\n",
      "Epoch 8/10\n",
      "3261/3261 [==============================] - 4s 1ms/step - loss: 0.0693 - accuracy: 0.9877\n",
      "Epoch 9/10\n",
      "3261/3261 [==============================] - 4s 1ms/step - loss: 0.0690 - accuracy: 0.9878\n",
      "Epoch 10/10\n",
      "3261/3261 [==============================] - 4s 1ms/step - loss: 0.0687 - accuracy: 0.9879\n",
      "CPU times: user 1min 6s, sys: 4.6 s, total: 1min 10s\n",
      "Wall time: 44.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train_scaled, y_train, epochs=10, verbose=True, batch_size=256, class_weight={0:1, 1:1.25});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the out-of-sample metrics.  There is a marginal improvement in F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6521/6521 [==============================] - 5s 683us/step\n",
      "F1:        0.43484102104791755\n",
      "Precision: 0.9024163568773235\n",
      "Recall:    0.28643067846607667\n"
     ]
    }
   ],
   "source": [
    "test_predictions = np.where(model.predict(X_test_scaled) > 0.5, 1, 0)\n",
    "print(\"F1:       \", f1_score(y_test, test_predictions))\n",
    "print(\"Precision:\", precision_score(y_test, test_predictions))\n",
    "print(\"Recall:   \", recall_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a marginal improvement in the absolute number of prepayments predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6521/6521 [==============================] - 4s 655us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.31740412979351035"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = np.where(model.predict(X_test_scaled) > 0.5, 1, 0)\n",
    "test_predictions.sum() / y_test.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the expected loan balance ratio is worse: it increases to 121%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6521/6521 [==============================] - 4s 614us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.2114536969706748"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_test['upb'] * np.ravel(model.predict(X_test_scaled))) / np.sum(X_test['upb'] * y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, `l2` regularization doesn't seem to help."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
