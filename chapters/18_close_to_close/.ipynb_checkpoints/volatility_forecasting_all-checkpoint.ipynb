{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volatility Measurement and Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this tutorial is to introduce various estimators for forecasting volatility.  This material is closely related to a the following white papers:\n",
    "\n",
    "1. *Volatility Modeling and Trading* - Artur Sepp, 2016\n",
    "\n",
    "2. *Measuring Historic Volatility* - Colin Bennet & Miguel Gil, 2012\n",
    "\n",
    "Our main objective will be to implement the code for various historical volatility estimators.  To test our work, we will attempt to replicate some of Sepp's results for *weekly* volatility forecasts for SPY (see pp 38-43)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading the packages that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> import pandas as pd\n",
    "##> import pandas_datareader as pdr\n",
    "##> import numpy as np\n",
    "##> import sklearn\n",
    "##> pd.options.display.max_rows = 10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading-In SPY Data From Yahoo Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sepp's analysis covers data starting from 1/1/2005 and ending on 4/2/2016.  Let's grab these SPY prices from Yahoo Finance using `pandas_datareader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_spy = pdr.get_data_yahoo('SPY', start = '2004-12-31', end = '2016-04-02').reset_index()\n",
    "##> df_spy.columns = df_spy.columns.str.lower().str.replace(' ', '_')\n",
    "##> df_spy.rename(columns = {'date':'trade_date'}, inplace = True)\n",
    "##> df_spy.insert(0, 'ticker', 'SPY')\n",
    "##> df_spy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Daily Returns & Realized Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The volatility estimators that we will implement will involve various daily returns.  Let's calculate them in the following block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> # daily (close-to-close)\n",
    "##> df_spy['dly_ret'] = np.log(df_spy['close']).diff()\n",
    "##> # overnight (close-to-open)\n",
    "##> df_spy['overnight'] = np.log(df_spy['open']) - np.log(df_spy['close']).shift(1)\n",
    "##> # intraday (open-to-close)\n",
    "##> df_spy['open_close'] = np.log(df_spy['close']) - np.log(df_spy['open'])\n",
    "##> \n",
    "##> df_spy = df_spy[1:].reset_index(drop = True)\n",
    "##> df_spy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing Dates for Backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organizing dates is an important step in a historical analysis.  \n",
    "\n",
    "We are performing a weekly analysis, which means that in later steps we will performing aggregation calculations of daily calculations grouped into weeks.  Therefore, we will need to add a column to `df_spy` that will allow us to group by weeks.\n",
    "\n",
    "The key to our approach will be to use the `.dt.weekday` attribute of the `trade_date` columns.  In the following code, the variable `weekday` is a `Series` that contains the weekday associated with each date.  Notice that Monday is encoded by `0` and Friday is encoded by `4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> weekday = df_spy['trade_date'].dt.weekday\n",
    "##> weekday\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is a simple `for`-loop that has the effect of creating a week-number for each week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> week_num = []\n",
    "##> ix_week = 0\n",
    "##> week_num.append(ix_week)\n",
    "##> for ix in range(0, len(weekday) - 1):\n",
    "##>     prev_day = weekday[ix]\n",
    "##>     curr_day = weekday[ix + 1]\n",
    "##>     if curr_day < prev_day:\n",
    "##>         ix_week = ix_week + 1\n",
    "##>     week_num.append(ix_week)\n",
    "##> np.array(week_num) # I use the array function simply because it looks better when it prints\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now insert the week numbers into `df_spy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_spy.insert(2, 'week_num', week_num)\n",
    "##> df_spy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question:** The `pandas.Series.dt.week` attribute gives the *week-of-the-year* for a give trade-date.  My initial idea was to use `.dt.week` and `dt.year` for my grouping, but I ran into an issue.  Can you think what the issue was?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use `groupby()` to calculate the starting and ending dates for each week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_start_end = \\\n",
    "##>     (\n",
    "##>     df_spy.groupby(['week_num'], as_index = False)[['trade_date']].agg([min, max])['trade_date']\n",
    "##>     .rename(columns = {'min':'week_start', 'max':'week_end'})\n",
    "##>     .reset_index()\n",
    "##>     .rename(columns = {'index':'week_num'})\n",
    "##>     )\n",
    "##> df_start_end\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge this data into `df_spy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_spy = df_spy.merge(df_start_end)\n",
    "##> df_spy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Weekly Realized Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a `week_num` associated with each `trade_date`, we can use `groupby()` to calculate the realized volatility.\n",
    "\n",
    "These weekly realized volatilities are the labels that we will be predicting later in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_realized = \\\n",
    "##>     (\n",
    "##>     df_spy\n",
    "##>         .groupby(['week_num', 'week_start', 'week_end'], as_index = False)[['dly_ret']].agg(lambda x: np.std(x) * np.sqrt(252))\n",
    "##>         .rename(columns = {'dly_ret':'realized_vol'})\n",
    "##>     )\n",
    "##> df_realized = df_realized[1:]\n",
    "##> df_realized\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close-to-Close Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first estimator that we will implement is the simlple close-to-close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> def close_to_close(r):\n",
    "##>     T = r.shape[0]\n",
    "##>     r_bar = r.mean()\n",
    "##>     vol = np.sqrt((1 / (T - 1)) * ((r - r_bar) ** 2).sum()) * np.sqrt(252)\n",
    "##>     return(vol)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `close_to_close()` is an aggregation function that takes in an array of daily returns and returns back a number.  In order to calculate weekly estimates we use `close_to_close()` as the aggregation function applied to a `.groupby()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_close_to_close = \\\n",
    "##>     (\n",
    "##>     df_spy\n",
    "##>         .groupby(['week_num', 'week_start', 'week_end'], as_index = False)[['dly_ret']]\n",
    "##>         .agg(close_to_close)\n",
    "##>         .rename(columns = {'dly_ret':'close_to_close'})\n",
    "##>     )\n",
    "##> df_close_to_close = df_close_to_close[0:-1]\n",
    "##> df_close_to_close\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question:** Verify that the `.groupby()` above works just fine with out including `week_start` and `week_end`.  If that is the case, then why did I include it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Challenge:** Create an alternative version of our close-to-close function using `np.std()`.  Call the new function `close_to_close_std()`. Verify that your values match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Sepp 2016, the author uses the $R^2$ between the forecasts and the realized labels as a means of assessing the quality of a particular estimator.  Let's utilize `sklearn` to do the same.\n",
    "\n",
    "We being by importing the `LinearRegression` constructor and instantiating a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> from sklearn.linear_model import LinearRegression\n",
    "##> mdl_reg = LinearRegression(fit_intercept = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's organize our features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> X = df_close_to_close[['close_to_close']]\n",
    "##> y = df_realized['realized_vol']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> mdl_reg.fit(X, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.score()` method of a `LinearRegression` model returns the $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> mdl_reg.score(X, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can examine the slope and intercept of our model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> print(mdl_reg.intercept_)\n",
    "##> print(mdl_reg.coef_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question:** How do our results compare to Sepp's?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also measure the bias and efficiency of the the close-to-close estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> # bias\n",
    "##> print(np.mean(df_close_to_close['close_to_close'] - df_realized['realized_vol']))\n",
    "##> \n",
    "##> # efficiency\n",
    "##> print(np.std(df_realized['realized_vol']) / np.std(df_close_to_close['close_to_close']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parkinson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next estimator that we implement is the Parkinson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> def parkinson(hl):\n",
    "##>     T = hl.shape[0]\n",
    "##>     high = hl.high\n",
    "##>     low = hl.low\n",
    "##>     vol = np.sqrt(np.sum((np.log(high / low) ** 2)) * (1 / (4 * np.log(2))) / T) * np.sqrt(252)\n",
    "##>     return(vol)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply our function to a single weeks worth of data in `df_spy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> parkinson(df_spy.query('week_num == 0')[['high', 'low']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a programming standpoint, the Parkinson estimate is a little bit different because it is an aggregation function that takes in two columns (`high` and `low`) and returns a single number.  \n",
    "\n",
    "For this reason, we will need to use `.apply()` rather than `.agg()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_parkinson = \\\n",
    "##>     (\n",
    "##>     df_spy.groupby(['week_num', 'week_start', 'week_end'])[['high', 'low']].apply(parkinson)\n",
    "##>     .to_frame().reset_index()\n",
    "##>     .rename(columns = {0:'parkinson'})\n",
    "##>     )\n",
    "##> df_parkinson = df_parkinson[:-1]\n",
    "##> df_parkinson\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's fit a linear regression to the parkinson forecasts and the realized volatilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> from sklearn.linear_model import LinearRegression\n",
    "##> mdl_reg = LinearRegression(fit_intercept = True)\n",
    "##> X = df_parkinson[['parkinson']]\n",
    "##> y = df_realized['realized_vol']\n",
    "##> mdl_reg.fit(X, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Challenge:** Check the $R^2$ and coefficients and compare them with Sepp's.  How closely do we match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also measure the bias and efficiency of the Parkinson estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> # bias\n",
    "##> print(np.mean(df_parkinson['parkinson'] - df_realized['realized_vol']))\n",
    "##> \n",
    "##> # efficiency\n",
    "##> print(np.std(df_realized['realized_vol']) / np.std(df_parkinson['parkinson']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Garman-Klass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next estimator is the Garman-Klass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> def garman_klass(ohlc):\n",
    "##>     T = ohlc.shape[0]\n",
    "##>     o = ohlc.open\n",
    "##>     h = ohlc.high\n",
    "##>     l = ohlc.low\n",
    "##>     c = ohlc.close\n",
    "##>     vol = np.sqrt(np.sum((0.5 * np.log(h / l) ** 2) - ((2 * np.log(2) - 1) * np.log(c / o) ** 2)) / T) * np.sqrt(252)\n",
    "##>     return(vol)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the function works for a single week of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> garman_klass(df_spy.query('week_num == 0')[['open', 'high', 'low', 'close']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Garman-Klass estimator takes in four different columns to produce a single numeric estimate, thus we have to use `.apply()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_garman_klass = \\\n",
    "##>     (\n",
    "##>     df_spy.groupby(['week_num', 'week_start', 'week_end'])[['open', 'high', 'low', 'close']].apply(garman_klass)\n",
    "##>     .to_frame().reset_index()\n",
    "##>     .rename(columns = {0:'garman_klass'} )\n",
    "##>     )\n",
    "##> df_garman_klass = df_garman_klass[:-1]\n",
    "##> df_garman_klass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's check for the goodness of predictions by fitting a linear regression and calculating the $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> from sklearn.linear_model import LinearRegression\n",
    "##> mdl_reg = LinearRegression(fit_intercept = True)\n",
    "##> X = df_garman_klass[['garman_klass']]\n",
    "##> y = df_realized['realized_vol']\n",
    "##> mdl_reg.fit(X, y)\n",
    "##> mdl_reg.score(X, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also measure the bias and efficiency of the Garman-Klass estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> # bias\n",
    "##> print(np.mean(df_garman_klass['garman_klass'] - df_realized['realized_vol']))\n",
    "##> \n",
    "##> # efficiency\n",
    "##> print(np.std(df_realized['realized_vol']) / np.std(df_garman_klass['garman_klass']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rogers-Satchell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Challenge:** Implement the Rogers-Satchell model, and calculate the $R^2$ between the forecasts and the realized, and also the bias and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yang-Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's repeat thes same steps for the Yang-Zang estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> def yang_zhang(ohlc_on_oc):\n",
    "##>     T = ohlc_on_oc.shape[0]\n",
    "##>     ohlc = ohlc_on_oc[['open', 'high', 'low', 'close']]\n",
    "##>     on = ohlc_on_oc.overnight\n",
    "##>     oc = ohlc_on_oc.open_close\n",
    "##>     \n",
    "##>     var_overnight = (close_to_close(on) / np.sqrt(252)) ** 2\n",
    "##>     var_open_close = (close_to_close(oc) / np.sqrt(252)) ** 2\n",
    "##>     var_rogers_satchell = (rogers_satchell(ohlc) / np.sqrt(252)) ** 2\n",
    "##>     \n",
    "##>     c = 0.34 / (1.34 + (T + 1)/(T - 1))\n",
    "##>     \n",
    "##>     vol = np.sqrt((var_overnight) + (c * var_open_close) + ((1 - c) * (var_rogers_satchell))) * np.sqrt(252)\n",
    "##>     \n",
    "##>     return(vol)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the function on a single week of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> yang_zhang(df_spy.query('week_num == 0')[['open', 'high', 'low', 'close', 'overnight', 'open_close']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating weekly forecasts using `.groupby()` and `.apply()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_yang_zhang = \\\n",
    "##>     (\n",
    "##>     df_spy.groupby(['week_num', 'week_start', 'week_end'])[['open', 'high', 'low', 'close', 'overnight', 'open_close']].apply(yang_zhang)\n",
    "##>     .to_frame().reset_index()\n",
    "##>     .rename(columns = {0:'yang_zhang'} )\n",
    "##>     )\n",
    "##> df_yang_zhang = df_yang_zhang[:-1]\n",
    "##> df_yang_zhang\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the performance of Yang-Zang by checking the $R^2$ of the fitted regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> from sklearn.linear_model import LinearRegression\n",
    "##> mdl_reg = LinearRegression(fit_intercept = True)\n",
    "##> X = df_yang_zhang[['yang_zhang']]\n",
    "##> y = df_realized['realized_vol']\n",
    "##> mdl_reg.fit(X, y)\n",
    "##> mdl_reg.score(X, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also measure the bias and efficiency of the Yang-Zang estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> # bias\n",
    "##> print(np.mean(df_yang_zhang['yang_zhang'] - df_realized['realized_vol']))\n",
    "##> \n",
    "##> # efficiency\n",
    "##> print(np.std(df_realized['realized_vol']) / np.std(df_yang_zhang['yang_zhang']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Challenge:** There is a short-hand identity for $R^2$ that would allow us to not have to bother with `sklearn`.  Google it and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
