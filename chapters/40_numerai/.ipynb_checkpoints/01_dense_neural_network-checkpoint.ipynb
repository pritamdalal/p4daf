{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerai is a hedge fund that crowdsources their market predictions.  They disseminate data that is anonymized so that the data scientists who are working on the forecasting are not even aware of what features they are working with.  The prediction problem is reduced to a classification of predicting a gain or loss.\n",
    "\n",
    "In this tutorial we will fit a variety models to Numerai data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by importing the data and organizing our features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>feature21</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.499664</td>\n",
       "      <td>0.951271</td>\n",
       "      <td>0.127110</td>\n",
       "      <td>0.469706</td>\n",
       "      <td>0.188336</td>\n",
       "      <td>0.113830</td>\n",
       "      <td>0.917618</td>\n",
       "      <td>0.398412</td>\n",
       "      <td>0.418910</td>\n",
       "      <td>0.452983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137192</td>\n",
       "      <td>0.201437</td>\n",
       "      <td>0.507708</td>\n",
       "      <td>0.919475</td>\n",
       "      <td>0.978169</td>\n",
       "      <td>0.177080</td>\n",
       "      <td>0.101372</td>\n",
       "      <td>0.722138</td>\n",
       "      <td>0.832319</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.099515</td>\n",
       "      <td>0.682824</td>\n",
       "      <td>0.867939</td>\n",
       "      <td>0.943828</td>\n",
       "      <td>0.505526</td>\n",
       "      <td>0.886766</td>\n",
       "      <td>0.530862</td>\n",
       "      <td>0.531002</td>\n",
       "      <td>0.980002</td>\n",
       "      <td>0.941859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.642640</td>\n",
       "      <td>0.533367</td>\n",
       "      <td>0.616879</td>\n",
       "      <td>0.697038</td>\n",
       "      <td>0.741461</td>\n",
       "      <td>0.086690</td>\n",
       "      <td>0.109533</td>\n",
       "      <td>0.324666</td>\n",
       "      <td>0.552276</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.671993</td>\n",
       "      <td>0.383901</td>\n",
       "      <td>0.533011</td>\n",
       "      <td>0.690863</td>\n",
       "      <td>0.176539</td>\n",
       "      <td>0.600196</td>\n",
       "      <td>0.381543</td>\n",
       "      <td>0.648849</td>\n",
       "      <td>0.831643</td>\n",
       "      <td>0.861746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520068</td>\n",
       "      <td>0.660924</td>\n",
       "      <td>0.538882</td>\n",
       "      <td>0.160117</td>\n",
       "      <td>0.765317</td>\n",
       "      <td>0.301772</td>\n",
       "      <td>0.352097</td>\n",
       "      <td>0.638205</td>\n",
       "      <td>0.383552</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.578177</td>\n",
       "      <td>0.872357</td>\n",
       "      <td>0.679625</td>\n",
       "      <td>0.108961</td>\n",
       "      <td>0.945910</td>\n",
       "      <td>0.571062</td>\n",
       "      <td>0.891958</td>\n",
       "      <td>0.916592</td>\n",
       "      <td>0.141508</td>\n",
       "      <td>0.258504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037959</td>\n",
       "      <td>0.604539</td>\n",
       "      <td>0.974103</td>\n",
       "      <td>0.187519</td>\n",
       "      <td>0.938254</td>\n",
       "      <td>0.560129</td>\n",
       "      <td>0.136483</td>\n",
       "      <td>0.284507</td>\n",
       "      <td>0.199446</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.474311</td>\n",
       "      <td>0.639613</td>\n",
       "      <td>0.563562</td>\n",
       "      <td>0.169508</td>\n",
       "      <td>0.456858</td>\n",
       "      <td>0.580710</td>\n",
       "      <td>0.969811</td>\n",
       "      <td>0.357417</td>\n",
       "      <td>0.157594</td>\n",
       "      <td>0.251147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038095</td>\n",
       "      <td>0.770200</td>\n",
       "      <td>0.697395</td>\n",
       "      <td>0.792327</td>\n",
       "      <td>0.711650</td>\n",
       "      <td>0.177080</td>\n",
       "      <td>0.247403</td>\n",
       "      <td>0.666598</td>\n",
       "      <td>0.755557</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0  0.499664  0.951271  0.127110  0.469706  0.188336  0.113830  0.917618   \n",
       "1  0.099515  0.682824  0.867939  0.943828  0.505526  0.886766  0.530862   \n",
       "2  0.671993  0.383901  0.533011  0.690863  0.176539  0.600196  0.381543   \n",
       "3  0.578177  0.872357  0.679625  0.108961  0.945910  0.571062  0.891958   \n",
       "4  0.474311  0.639613  0.563562  0.169508  0.456858  0.580710  0.969811   \n",
       "\n",
       "   feature8  feature9  feature10  ...  feature13  feature14  feature15  \\\n",
       "0  0.398412  0.418910   0.452983  ...   0.137192   0.201437   0.507708   \n",
       "1  0.531002  0.980002   0.941859  ...   0.642640   0.533367   0.616879   \n",
       "2  0.648849  0.831643   0.861746  ...   0.520068   0.660924   0.538882   \n",
       "3  0.916592  0.141508   0.258504  ...   0.037959   0.604539   0.974103   \n",
       "4  0.357417  0.157594   0.251147  ...   0.038095   0.770200   0.697395   \n",
       "\n",
       "   feature16  feature17  feature18  feature19  feature20  feature21  target  \n",
       "0   0.919475   0.978169   0.177080   0.101372   0.722138   0.832319       0  \n",
       "1   0.697038   0.741461   0.086690   0.109533   0.324666   0.552276       1  \n",
       "2   0.160117   0.765317   0.301772   0.352097   0.638205   0.383552       0  \n",
       "3   0.187519   0.938254   0.560129   0.136483   0.284507   0.199446       1  \n",
       "4   0.792327   0.711650   0.177080   0.247403   0.666598   0.755557       0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv('numerai_training_data.csv')\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df_data.drop(columns='target').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = df_data['target'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data is Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice thing about working with the Numerai data is that it is clean and normalized.  \n",
    "\n",
    "1. All the features are in the range of $[0, 1]$.\n",
    "1. All the features have a mean of 0.50 and a standard deviation of 0.28.\n",
    "1. The occurrence labels is even at 50% gains and 50% losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     0.511372\n",
       "feature2     0.492770\n",
       "feature3     0.492105\n",
       "feature4     0.499420\n",
       "feature5     0.502291\n",
       "feature6     0.493039\n",
       "feature7     0.480280\n",
       "feature8     0.494526\n",
       "feature9     0.492926\n",
       "feature10    0.489265\n",
       "feature11    0.495725\n",
       "feature12    0.510969\n",
       "feature13    0.489852\n",
       "feature14    0.509350\n",
       "feature15    0.487469\n",
       "feature16    0.509012\n",
       "feature17    0.488944\n",
       "feature18    0.484929\n",
       "feature19    0.491757\n",
       "feature20    0.509223\n",
       "feature21    0.498371\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature1     0.282260\n",
       "feature2     0.287446\n",
       "feature3     0.282481\n",
       "feature4     0.284493\n",
       "feature5     0.289867\n",
       "feature6     0.287061\n",
       "feature7     0.287526\n",
       "feature8     0.288087\n",
       "feature9     0.293945\n",
       "feature10    0.287046\n",
       "feature11    0.290922\n",
       "feature12    0.285451\n",
       "feature13    0.291276\n",
       "feature14    0.290140\n",
       "feature15    0.286997\n",
       "feature16    0.289279\n",
       "feature17    0.284790\n",
       "feature18    0.290445\n",
       "feature19    0.283742\n",
       "feature20    0.291001\n",
       "feature21    0.289637\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a guess of increase for all assets would yield an accuracy of 50.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5051702657807309"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model that we will fit is a simple logistic regression.  We will use a 10-fold cross-validation accuracy as our goodness of fit metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "mdl_logistic_regression = LogisticRegression(C=1.0, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.9 s, sys: 15.8 s, total: 38.7 s\n",
      "Wall time: 4.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(mdl_logistic_regression, df_X, df_y, cv=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a mean accuracy of about 52% which is just slightly higher than guessing a gain for all assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5219476744186047"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's fit a random forest.  We use a 5-fold cross-validation accuracy because these models take time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "mdl_random_forest = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.8 s, sys: 58.9 ms, total: 44.9 s\n",
      "Wall time: 44.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.51858389, 0.51723422, 0.52450166, 0.51650748, 0.52460548])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "scores = cross_val_score(mdl_random_forest, df_X, df_y, cv=5, scoring='accuracy', verbose=0)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to logistic regression we get a mean score of around 52%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5202865448504983"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next model that we will fit is a gradient boosted tree with the `xgboost` package.  We use 5-fold cross-validation to assess model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "mdl_xgboost = XGBClassifier(n_estimators=100, max_depth=10, learning_rate=0.01, use_label_encoder=False, eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pritam/.local/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.51292566, 0.51775332, 0.51977782, 0.51629983, 0.51567691])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(mdl_xgboost, df_X, df_y, cv=5, scoring='accuracy', verbose=0)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our mean accuracy score is slightly lower at 51.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5164867109634551"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the subsequent sections we will try a variety of neural networks.  In order to check out-of-sample accuracy, let's first create a holdout set with `train_test_split()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to fit our first neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set our random seeds to get reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's build up our dense feed-forward neural network.  We use 3 hidden layer with 16, 8, and 4 units, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=16, input_dim=len(df_X.columns), activation='relu'))\n",
    "model.add(Dense(units=8, input_dim=len(df_X.columns), activation='relu'))\n",
    "model.add(Dense(units=4, input_dim=len(df_X.columns), activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 s, sys: 1.93 s, total: 18.9 s\n",
      "Wall time: 8.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "h = model.fit(X_train, y_train, epochs=10, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our model performs about the same as the previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602/602 [==============================] - 0s 406us/step - loss: 0.6914 - accuracy: 0.5213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6913726925849915, 0.5212832093238831]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we implement dropout regularization in our neural network.  We use a slightly different architecture with 2 hidden units of 64 and 32 units, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=64, input_dim=len(df_X.columns), activation='relu'))\n",
    "model.add(Dropout(rate=0.1, seed=100))\n",
    "model.add(Dense(units=32, input_dim=len(df_X.columns), activation='relu'))\n",
    "model.add(Dropout(rate=0.1, seed=100))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.7 s, sys: 3.27 s, total: 25 s\n",
      "Wall time: 10.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa5f06b9280>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train, y_train, epochs=10, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout reglarization doesn't seem to have much of an effect on our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602/602 [==============================] - 0s 449us/step - loss: 0.6916 - accuracy: 0.5227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6915660500526428, 0.5227367281913757]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we perform `l1` regularization on our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=64, input_dim=len(df_X.columns), activation='relu', activity_regularizer=l1(0.0005)))\n",
    "model.add(Dense(units=32, input_dim=len(df_X.columns), activation='relu', activity_regularizer=l1(0.0005)))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.7 s, sys: 3.52 s, total: 25.2 s\n",
      "Wall time: 10.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa5f0598a00>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train, y_train, epochs=10, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, `l1` regularization doesn't seem to perform that well on our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602/602 [==============================] - 0s 449us/step - loss: 0.6923 - accuracy: 0.5175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6923099756240845, 0.5174937844276428]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question:** Based on our results so far, which model would you select?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Challenge:** Try some different neural architectures and see if ou can improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
